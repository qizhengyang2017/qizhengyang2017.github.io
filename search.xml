<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[docear 使用心得]]></title>
    <url>%2F2019%2F03%2F03%2F2019-3-3-docear-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>软件</tag>
        <tag>学术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[需要比对的结果，在snakemake脚本里添加log]]></title>
    <url>%2F2019%2F03%2F02%2F%E5%9C%A8snakemake%E8%84%9A%E6%9C%AC%E9%87%8C%E6%B7%BB%E5%8A%A0log%2F</url>
    <content type="text"><![CDATA[log名要添加{sample}，不然出现如下错误 1234[qizhengyang@node1 gatk_snakemake]$ snakemake -n --quietSyntaxError:Not all output, log and benchmark files of rule star_1pass_align contain the same wildcards. This is crucial though, in order to avoid that two or more jobs write to the same file. File &quot;/home02/qizhengyang/qizhengyang/gatk_snakemake/Snakefile&quot;, line 63, in &lt;module&gt; 开始跑，大概 2019-3-2 21:10123snakemake -n --quietpestatnohup snakemake --cluster &quot;qsub -q low&quot; --jobs 100 &amp;&gt; snakemake.out &amp; 2019-3-2 23:52:22 一切正常 打算关闭终端，查看用： 1ps aux|grep Snakefile|grep -v grep]]></content>
      <tags>
        <tag>snakemake</tag>
        <tag>gatk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群和Linux相关]]></title>
    <url>%2F2019%2F03%2F02%2F2019-3-2-%E9%9B%86%E7%BE%A4%E5%92%8CLinux%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[PBS^usagePBS是公开源代码的作业管理系统，在此环境下运行，用户不需要指定程序在哪些节点上运行，程序所需的硬件资源由PBS管理和分配。 PBS(Portable Batch System)是由NASA开发的灵活的批处理系统。它被用于集群系统、超级计算机和大规模并行系统。PBS主要有如下特征： 易用性：为所有的资源提供统一的接口，易于配置以满足不同系统的需求，灵活的作业调度器允许不同系统采用自己的调度策略。 移植性：符合POSIX 1003.2标准，可以用于shell和批处理等各种环境。 适配性：可以适配与各种管理策略，并提供可扩展的认证和安全模型。支持广域网上的负载的动态分发和建立在多个物理位置不同的实体上的虚拟组织。 灵活性：支持交互和批处理作业。 常用命令Linux之三剑客，awk、sed、grep的用法^editawk1234567891011121314eth0 Link encap:Ethernet HWaddr 00:0C:29:18:4C:35 inet addr:192.168.75.130 Bcast:192.168.75.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe18:4c35/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1322 errors:0 dropped:0 overruns:0 frame:0 TX packets:1093 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:147531 (144.0 KiB) TX bytes:134582 (131.4 KiB)ifconfig eth0 |grep &apos;inet addr&apos; |awk -F &quot;:&quot; &apos;&#123;print $2&#125;&apos; 192.168.75.130 Bcast#可以看到后面多出来了一个Bcarst,可以打印出第一列ifconfig eth0 |grep &apos;inet addr&apos; |awk -F &quot;:&quot; &apos;&#123;print $2&#125;&apos;|awk &apos;&#123;print $1&#125;&apos;192.168.75.130 sedstream editor 流编辑器 sed 从输入读取一行，将之拷贝到一个编辑缓冲区，按指定的sed命令编辑完之后，将其发送到屏幕，然后将这行删除，读取下一行。（注意，输出的是编辑之后的结果，如果是删除，输出中部包含删除的结果。重定向之后，部打印到屏幕上。） 123456789101112131415#语法结构sed [options] ‘[command]’ filename # 输出前三行以外的内容，-n flush output on every line，每行刷新输出，默认sed -n '1,3!p' passwd# 把文件第三行替换成“bbb”sed '3cbbb' b.txt # 删除空行 将匹配的行记录到新的文件中sed '/^$/d' passwd &gt; c.txt# 把passwd中包含FORMAT=的记录（行）写入新的文件中sed '/FORMAT=/w newpasswd' passwd# sed 的-i选项可以直接修改文件中的内容sed -i 's/root/rm/' passwd grepLinux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 要用好grep这个工具，其实就是要写好正则表达式，所以这里不对grep的所有功能进行实例讲解，只列几个例子，讲解一个正则表达式的写法。 pattern 可以是正则表达式（用单引号）、或字符串（双引号）、或一个单词（没有引号） 12345678910 ls -l | grep \'^a\'#通过管道过滤ls -l输出的内容，只显示以a开头的行。 grep \'test\' d* #显示所有以d开头的文件中包含test的行。 grep \'test\' aa bb cc #显示在aa，bb，cc文件中匹配test的行。 grep \'[a-z]&#123;5&#125;\' aa #显示所有包含每个字符串至少有5个连续小写字符的字符串的行。 grep \'w(es)t.*1\' aa #如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符（.*），这些字符后面紧跟着另外一个es（1），找到就显示该行。如果用egrep或grep -E，就不用""号进行转义，直接写成\'w(es)t.*1\'就可以了。 12 ls -thl | grep &apos;snakejob.star_2pass_align.*o.*&apos; | awk &apos;&#123;print $7,$8,$9&#125;&apos; | head -n 36# 一个实例，文件排序 其他pestat 节点状态： Excl 所有CPU资源已被占用； Busy CPU已接近满负荷运行； free 全部或部分CPU空闲； offl 管理员手动指定离线状态； 此外，还可以使用另外一个命令pnodes来查看每个节点被占用的cpu核心数，用户可以根据剩余cpu资源合理地指定自己作业中使用的cpu核心数，以免作业处于长期等待状态。该命令有个缺点，就是运行时间比较长，大概要3s才出结果，其原因是这套系统节点数太多，命令执行时间太长。^keylabwiki 1qsub -I -l nodes=node18 # 以交互方式运行 12345678910#压缩tar -czvf ***.tar.gztar -cjvf ***.tar.bz2#解压缩tar -xzvf ***.tar.gztar -xjvf ***.tar.bz2# du，disk usage# df，disk freedu -sh *** # 某个目录中的所有文件占了多少磁盘空间 1./configure --prefix=/opt/software # 利用软件编译安装软件，指定目录 MPIMPI是一种用于节点间通信的方法。^wikipedia 我觉得「消息传递接口」（Message Passing Interface, MPI）就是这样一项技术，而且学习它确实可以让你的并行编程知识变得更深厚。^tutorials makefile简介Makefile文件 软件程序的管理工具 定义规则，实现自动化编译 处理源代码、目标文件、头文件、库文件等依赖关系 根据规则和依赖关系，结合时间戳实现精细化控制 make命令 make命令执行 Makefile 中的定义的编译流程 make命令默认读取当前目录 Makefile 或 makefile 文件，也可用 -f 参数指定 Makefile 文件]]></content>
      <tags>
        <tag>linux</tag>
        <tag>PBS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用 python生成json格式的配置文件]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%94%A8python%E7%94%9F%E6%88%90json%E6%A0%BC%E5%BC%8F%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[python 基础知识writelines 和 write 区别： write()需要传入一个字符串做为参数，否则会报错 writelines()既可以传入字符串又可以传入一个字符序列，并将该字符序列写入文件 注意必须传入的是字符序列，不能是数字序列(卡在这里搞了半天) lambda函数 lambda作为一个表达式，定义了一个匿名函数 12345678910&gt;&gt;&gt; foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]&gt;&gt;&gt;&gt;&gt;&gt; print filter(lambda x: x % 3 == 0, foo)[18, 9, 24, 12, 27]&gt;&gt;&gt;&gt;&gt;&gt; print map(lambda x: x * 2 + 10, foo)[14, 46, 28, 54, 44, 58, 26, 34, 64]&gt;&gt;&gt;&gt;&gt;&gt; print reduce(lambda x, y: x + y, foo)139 列表生成式 列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;][&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;] 任务用文件名（存在列表当值），生成json配置文件 1234567&#123; &quot;label&quot; : &quot;sample&quot;, &quot;key&quot; : &quot;sample&quot;, &quot;storeClass&quot; : &quot;JBrowse/Store/SeqFeature/VCFTabix&quot;, &quot;urlTemplate&quot; : &quot;sample.vcf.gz&quot;, &quot;type&quot; : &quot;JBrowse/View/Track/HTMLVariants&quot;&#125; json 模块 示例： 123456&gt;&gt;&gt; import json&gt;&gt;&gt; print(json.dumps(&#123;&apos;4&apos;: 5, &apos;6&apos;: 7&#125;, sort_keys=True, indent=4))&#123; &quot;4&quot;: 5, &quot;6&quot;: 7&#125; 原创脚本 1234567891011121314151617181920212223242526#!/usr/bin/env python3'''Make a samples.json file with sample names and file names.'''import jsonfrom glob import glob# match filenamesfastqs = glob('/usr/share/nginx/html/vcf/*.vcf.gz')files = &#123;&#125;# extract a sample name from each filename.samples = [fastq.split('/')[-1].split('_')[0] for fastq in fastqs]samples1 = sorted(samples)for sample in samples1: files['label'] = sample files['key'] = sample files['storeClass'] = 'JBrowse/Store/SeqFeature/VCFTabix' files['urlTemplate'] = 'vcf/' + sample + '_filtered.vcf.gz' files['category'] = 'VCF' files['type'] = 'JBrowse/View/Track/HTMLVariants' js = json.dumps(files, indent=4, sort_keys=True) js1 = js + ',' + '\n' open('mysamples1.json', 'a').writelines(js1) 输出 12345678910111213141516&#123; &quot;category&quot;: &quot;VCF&quot;, &quot;key&quot;: &quot;HB-10-1&quot;, &quot;label&quot;: &quot;HB-10-1&quot;, &quot;storeClass&quot;: &quot;JBrowse/Store/SeqFeature/VCFTabix&quot;, &quot;type&quot;: &quot;JBrowse/View/Track/HTMLVariants&quot;, &quot;urlTemplate&quot;: &quot;vcf/HB-10-1_filtered.vcf.gz&quot;&#125;,&#123; &quot;category&quot;: &quot;VCF&quot;, &quot;key&quot;: &quot;HB-10-2&quot;, &quot;label&quot;: &quot;HB-10-2&quot;, &quot;storeClass&quot;: &quot;JBrowse/Store/SeqFeature/VCFTabix&quot;, &quot;type&quot;: &quot;JBrowse/View/Track/HTMLVariants&quot;, &quot;urlTemplate&quot;: &quot;vcf/HB-10-2_filtered.vcf.gz&quot;&#125;, JBrowse 结果 其他安装制定版本为3.6，为什么python的版本是3.5.5 123(py3.6) [qi@localhost ~]$ python --versionPython 3.5.5(py3.6) [qi@localhost ~]$ 1conda install -c anaconda ipython # su 目录高亮是怎么回事？ 说明这个目录权限太高了，所有人都能够访问、修改、删除、运行。正常情况，应该是Owner可以读写，all可读，root可以修改可执行权限才对。正常情况，对于目录，应该权限设定为744，对于文件默认权限应该设定为644。用户私有文件，权限应该设定为600。 更改文件拥有者 12ls -ld html/chown -R qi:qi html/ py3.6的环境，我安装了samtools，也用不了。自然也就用不了，bgzip和tabix。 123(py3.6) [qi@localhost vcf]$ conda list | grep samtoolsperl-bio-samtools 1.43 pl526h1341992_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/biocondasamtools 1.9 h57cc563_6 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda]]></content>
      <tags>
        <tag>jbrowse</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JBrowse 总结]]></title>
    <url>%2F2019%2F03%2F01%2FJBrowse%2F</url>
    <content type="text"><![CDATA[当没有得到结果的时候，总是战战兢兢。得到结果之后，逻辑一下子就很清晰。 下载JBrowse最新版本，我用的是 1.16.3 web服务器，用Nginx。（用 Apache，建立 VCF track 会出错）^question 安装 JBrowse./setup.sh这一步需要安装很多perl依赖库，如果报错，查看setup.log文件，应该是一些 perl 模块没有安装。如： ! Installing GD failed. See /root/.cpanm/work/1551106022.8560/build.log for details. Retry with –force to force install it.Searching XML::LibXML (0) on cpanmetadb …Already tried XML-LibXML-2.0134. Skipping.! Installing the dependencies failed: Module ‘XML::LibXML’ is not installed, Module ‘XML::LibXML::Reader’ is not installed, Module ‘GD’ is not installed, Module ‘DBI’ is not installed, Module ‘DB_File’ is not installed! Bailing out the installation for BioPerl-1.7.5.Searching JSON::XS (0) on cpanmetadb …Unpacking JSON-XS-4.01.tar.gzFAIL 官方文档说，运行./setup.sh 的时候不要是 root 或使用 sudo。（和安装目录有关？）这可能是之前安装失败的原因。 Run the automated-setup script, ./setup.sh, which will attempt to install all of JBrowse’s (modest) prerequisites for you in the jbrowse/ directory itself. Note that setup.sh should not be run as root or with sudo.^1 安装Nginxyum install nginx 123sudo systemctl stop httpd.service # 因为之前有apache服务器，所以要先停掉sudo yum install nginx # 安装systemctl start nginx.service # 启动 如果你的服务器正在运行防火墙，请运行下列命令以允许它进行 HTTP 和 HTTPS 通信： https://www.jianshu.com/p/a482a0f8adfd 123sudo firewall-cmd --permanent --zone=public --add-service=httpsudo firewall-cmd --permanent --zone=public --add-service=httpssudo firewall-cmd --reload 两个 perl 版本的问题，从报错信息知 JBrowse 用的是 perl 5.16.3，即 /usr/bin/perl。系统默认的 perl 是 conda 安装的 perl。 Can’t locate local/lib.pm in @INC (@INC contains: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3 /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5 /var/www/html/JBrowse-1.16.3/bin/../src/perl5 /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JBlibs.pm line 25.Compilation failed in require at bin/prepare-refseqs.pl line 5.BEGIN failed–compilation aborted at bin/prepare-refseqs.pl line 5 解决上面的问题，可以直接copy 一份。（不知道家目录下的 perl5 文件夹是什么时候生成的？） 1234567locate local/lib.pmcp -r /home/qi/perl5/lib/perl5/local /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3# 运行示例sudo bin/prepare-refseqs.pl --fasta docs/tutorial/data_files/volvox.fa --out sample_data/json/volvoxsudo bin/prepare-refseqs.pl --fasta sample_data/raw/yeast_scaffolds/chr1.fa.gz --fasta sample_data/raw/yeast_scaffolds/chr2.fa.gzip --out sample_data/json/yeast/ 柚子数据准备参考序列和特征数据所有track都生成在默认的data目录中 123456789# 指定perl为/usr/bin/perl # seq trackList.json tracks.conf/usr/bin/perl bin/prepare-refseqs.pl --fasta ~/data2/tan_rna/genome/HWB.chromosome.fa# tracks/usr/bin/perl bin/flatfile-to-json.pl --gff HWB.gene.models.gff3 --trackType CanvasFeatures --trackLabel HWBgff# 建索引。生成 ./data/names/usr/bin/perl bin/generate-names.pl 不指定会出现如下错误： (base) [qi@localhost JBrowse-1.16.3]$ bin/flatfile-to-json.pl –helpperl: symbol lookup error: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi/auto/JSON/XS/XS.so: undefined symbol: Perl_xs_apiversion_bootcheck 查资料 This type of error is almost always indicates you are loading a module that was installed using a different build of Perl.^stackoverflow VCF trackshttp://jbrowse.org/docs/variants.html VCF files used with the VCFTabix must be compressed with bgzip and indexed with tabix, both of which are part of the samtools package. This is usually done with commands like: 12for i in *.vcf; do bgzip $i; donefor i in *.vcf.gz; do tabix -p vcf $i; done 安装好之后，我们将JBrowse-1.12.1下所有文件拷贝到网站根目录 1sudo cp * /usr/share/nginx/html http://10.164.6.154 要添加其他物种，可重命名data文件夹 1$ mv data xxx 下次再把另外一个基因组放进来时，bin目录下的脚本又会默认放到data目录，就不会发生混乱啦！ 另外，如果你把JBrowse部署在内网，却想让外网访问到，而部署JBrowse的服务器又在防火墙后面的话，请一定要记得把jbrowse.conf加入到防火墙白名单，因为防火墙默认会把这种.conf结尾的文件屏蔽。^引自 所谓基因组浏览器就是通过这个工具查看基因组，具体包括参考基因组序列，哪个地方是外显子、那个地方是内含子等等功能。参考基因组就是一个fasta文件，哪个地方是外显子、哪个地方是内含子这些信息称之为特征，一般情况下NCBI、ENSEMBL等数据库都会提供GFF3格式的基因组特征文件。JBrowse支持GFF3, BED, FASTA, Wiggle, BigWig, BAM, VCF (with tabix), REST等众多数据格式，BAM, BigWig，VCF格式的数据可以不需要转换而直接使用。 其他相关的命令一些关于防火墙的命令 1234firewall-cmd --state # 查看状态，runningfirewall-cmd --reload # 重启firewallsystemctl stop firewalld.service # 停止firewallsystemctl disable firewalld.service # 禁止firewall开机启动 查看历史记录 12# awk去掉第一列，sed去掉行首空格history | tail -n 40 | awk '&#123;$1="";print $0&#125;' | sed 's/^ *//' conda 命令 1234567891011121314conda create --name bioinfo python=3.6 # 创建环境conda list # 查看包conda update --all # 更新所有包# 查看环境conda info -econda activate bioinfoconda deactivate# ERROR: This cross-compiler package contains no program # https://github.com/conda/conda/issues/6600conda list | grep gcc# 需要超级用户权限conda remove gcc_linux-64 gcc_impl_linux-64 binutils_linux-64 binutils_impl_linux-64 解决复制终端文本换行的问题 123zcat HB-10-2_filtered.vcf.gz | sed -n '/#CHROM/=' # 输出行号zcat HB-10-2_filtered.vcf.gz | head -n 42 &gt; header.txt# 文本编辑器打开会多一行，因为行末尾的换行符 JBrowse 参考JBrowse Configuration Guide Variant Tracks (VCF) Troubleshooting Browse基因浏览器介绍]]></content>
      <tags>
        <tag>生物信息</tag>
        <tag>jbrowse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git和github使用]]></title>
    <url>%2F2019%2F02%2F27%2Fgit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[对之前使用git和github的整理，重新在网上搜索资料，添加代码注释。 简介 我们一直用GitHub作为免费的远程仓库，如果是个人的开源项目，放到GitHub上是完全没有问题的。其实GitHub还是一个开源协作社区，通过GitHub，既可以让别人参与你的开源项目，也可以参与别人的开源项目。 在GitHub出现以前，开源项目开源容易，但让广大人民群众参与进来比较困难，因为要参与，就要提交代码，而给每个想提交代码的群众都开一个账号那是不现实的，因此，群众也仅限于报个bug，即使能改掉bug，也只能把diff文件用邮件发过去，很不方便。 但是在GitHub上，利用Git极其强大的克隆和分支功能，广大人民群众真正可以第一次自由参与各种开源项目了。 使用过的代码12345678910111213# 创建SSH Key# 在用户目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件ssh-keygen -t rsa -C "qizhengyang17@gmail.com"# 测试添加是否成功ssh -T git@github.com# 配置个人信息git config --global user.name "qizhengyang"git config --global user.email "qizhengyang17@gmail.com"# 通过SSH协议连接远程仓库git remote add origin git@github.com:qizhengyang2017/thesis.git 添加、提交 1234567891011121314# 当前仓库状态git status# 将文件更改添加到暂存区git add prepDE.py# 添加所有更改到暂存区git add .# 提交说明git commit -m "第一个脚本文件"# 推送内容到GitHub的仓库git push -u origin master git命令表格直接复制完全没有问题呀 个人本地使用 行为 命令 备注 初始化 init 在本地的当前目录里初始化git仓库 clone 地址 从网络上某个地址拷贝仓库(repository)到本地 查看当前状态 status 查看当前仓库的状态。碰到问题不知道怎么办的时候，可以通过看它给出的提示来解决问题 查看不同 diff 查看当前状态和最新的commit之间不同的地方 diff 版本号1 版本号2 查看两个指定的版本之间不同的地方。这里的版本号指的是commit的hash值 添加文件 add -A 这算是相当通用的了。在commit之前要先add 撤回stage的东西 checkout – . 这里用小数点表示撤回所有修改，在--的前后都有空格 提交 commit -m “提交信息” 提交信息最好能体现更改了什么 删除未tracked clean -xf 删除当前目录下所有没有track过的文件。不管它是否是.gitignore文件里面指定的文件夹和文件 查看提交记录 log 查看当前版本及之前的commit记录 reflog HEAD的变更记录 版本回退 reset –hard 版本号 回退到指定版本号的版本，该版本之后的修改都被删除。同时也是通过这个命令回到最新版本。需要reflog配合 个人使用远程仓库 行为 命令 备注 设置用户名 config –global user.name “你的用户名” 设置邮箱 config –global user.email “你的邮箱” 生成ssh key ssh-keygen -t rsa -C “你的邮箱” 这条命令前面不用加git 添加远程仓库 remote add origin 你复制的地址 设置origin 上传并指定默认 push -u origin master 指定origin为默认主机，以后push默认上传到origin上 提交到远程仓库 push 将当前分支增加的commit提交到远程仓库 从远程仓库同步 pull 在本地版本低于远程仓库版本的时候，获取远程仓库的commit 可以用一张图直观地看出以上主要的命令对仓库的影响。 工作区和暂存区 参考使用GitHub git和github简单教程 使用git和github管理自己的项目 Git | 一篇文章搞定Git、GitHub的理解和使用（学习笔记）]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JBrowse 安装和使用]]></title>
    <url>%2F2019%2F02%2F26%2FJBrowse%20%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[这件事情弄了我两天。查看处理错误信息占据了大部分时间。好在最后终于成功了。 感悟，对于技术文档不能完全照搬。中文的技术文档，主要的作用是了解，最后的实践一定要充分参考官方的英文文档。 基因浏览器介绍（入门）https://www.plob.org/article/11742.html 在日常数据处理和分析工作中，根据分析项目的不同我们会面对各种各样的文件，比如mapping得到的SAM或者BAM文件，在此基础上转化而来的记录各种（定量）信息的bedgraph文件、Wig文件或者tdf文件，亦或记录variant信息的VCF文件。以至于有一种比较调侃的说法，所谓“生物信息”就是整天和各种格式的文件打交道，转换来转换去。 JBrowse 是今天要介绍的主角。它是GMOD开源项目的一部分，想了解这个开源项目可以查看它的官方网站http://gmod.org/wiki/Main_Page 完全基于HTML5和Javascript构建的JBrowse通过AJAX技术实现了数据的异步加载，所以响应速度非常快。由于Javascript将大量的计算工作在前端完成，服务器端只需要向浏览器客户端发送静态文件，因此也极大程度减轻了服务器端的负担。 完全基于HTML5和Javascript构建的JBrowse通过AJAX技术实现了数据的异步加载，所以响应速度非常快。由于Javascript将大量的计算工作在前端完成，服务器端只需要向浏览器客户端发送静态文件，因此也极大程度减轻了服务器端的负担。 准备参考序列安装成功之后首先要做的是格式化参考序列，支持的格式有fasta， gff，或者是用samtools faidx处理过的indexed fasta 文件。参考序列生成的track 会为后续所有文件提供一个坐标，一直放大后参考序列的碱基也会显示出来。 通常我们使用某一个物种的genome fasta 文件，默认情况下，每一条染色体会独立为一条参考序列。如果想把RNA或者蛋白序列当成参考序列也没有问题。 需要注意的是所有数据默认都会输出在out目录中，如果你想用JBrowse展示不同物种的信息，最好使用--out参数指定单独的目录。 在这里提前说明，后面所有基于命令行设置的配置信息都会自动生成在tarckList.json文件中。 准备参考序列需要用到的是bin目录下的prepare-refseqs.pl脚本 对于VCF文件，需要提前做的准备就是使用bgzip压缩之后再利用tabix -p vcf 建立index。 展示方式和BigWig类似，其中type要定义为 “JBrowse/View/Track/HTMLVariants”；而storeClass定义为”JBrowse/Store/SeqFeature/VCFTabix” 即可。 尝试安装主要参考这篇文献 https://yq.aliyun.com/articles/650480 没有成功，应该是perl模块的安装问题。不知道是否和conda有关。 ! Installing GD failed. See /root/.cpanm/work/1551106022.8560/build.log for details. Retry with –force to force install it.Searching XML::LibXML (0) on cpanmetadb …Already tried XML-LibXML-2.0134. Skipping.! Installing the dependencies failed: Module ‘XML::LibXML’ is not installed, Module ‘XML::LibXML::Reader’ is not installed, Module ‘GD’ is not installed, Module ‘DBI’ is not installed, Module ‘DB_File’ is not installed! Bailing out the installation for BioPerl-1.7.5.Searching JSON::XS (0) on cpanmetadb …Unpacking JSON-XS-4.01.tar.gzFAIL ! Installing JSON::XS failed. See /root/.cpanm/work/1551106022.8560/build.log for details. Retry with –force to force install it.Searching Bio::Root::Version (1.006000) on cpanmetadb … -&gt; FAIL Installing the dependencies failed: Module ‘DBI’ is not installed, Module ‘Heap::Simple::XS’ is not installed, Module ‘Bio::Annotation::SimpleValue’ is not installed, Module ‘Bio::SeqFeature::Annotated’ is not installed, Module ‘DB_File’ is not installed, Module ‘Bio::Index::Fasta’ is not installed, Module ‘Bio::SeqFeature::Lite’ is not installed, Module ‘DBD::SQLite’ is not installed, Module ‘Devel::Size’ is not installed, Module ‘Bio::FeatureIO’ is not installed, Module ‘JSON::XS’ is not installed, Module ‘Bio::Root::Version’ is not installed, Module ‘PerlIO::gzip’ is not installed,Module ‘Bio::OntologyIO’ is not installed-&gt; FAIL Bailing out the installation for JBrowse-. 123456789perl -MCPAN -e shellinstall DBIconda install -c bioconda perl-module-listconda install -c bioconda perl-dbiconda install -c bioconda perl-bioperl instmodshlocate XML/SAX.pm /home/conda/feedstock_root/build_artifacts/perl_1548813468557/_build_env/bin/x86_64-conda_cos6-linux-gnu-gcc: No such file or directorymake: *** [Perl.o] Error 127 docker安装（没有成功）http://www.runoob.com/docker/centos-docker-install.html 安装docker很顺畅 123456sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum makecache fastsudo yum -y install docker-cesudo systemctl start dockerdocker run hello-world 由于本地没有hello-world这个镜像，所以会下载一个hello-world的镜像，并在容器内运行。 jbrowse/gmod-jbrowse 卡住了 12docker run -p 8080:80 jbrowse/jbrowse-1.12.0# 有问题 conda 安装（安装成功了，不知道如何启用）12# 需要超级用户权限，不然会失败conda install -c bioconda jbrowse 123# conda 的卸载命令conda list | grep jbrowseconda remove jbrowse 安装Nginxyum install nginx [root@localhost qi]# netstat -ntplActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:111 0.0.0.0: LISTEN 1/systemdtcp 0 0 0.0.0.0:80 0.0.0.0: LISTEN 1823/nginx: mastertcp 0 0 0.0.0.0:6000 0.0.0.0: LISTEN 6706/Xtcp 0 0 192.168.122.1:53 0.0.0.0: LISTEN 6629/dnsmasqtcp 0 0 0.0.0.0:22 0.0.0.0: LISTEN 6296/sshdtcp 0 0 127.0.0.1:631 0.0.0.0: LISTEN 6299/cupsdtcp 0 0 127.0.0.1:6010 0.0.0.0: LISTEN 13051/sshd: qi@pts/tcp 0 0 127.0.0.1:6011 0.0.0.0: LISTEN 23741/sshd: qi@pts/tcp 0 0 127.0.0.1:6012 0.0.0.0: LISTEN 31798/sshd: qi@pts/tcp6 0 0 :::3306 ::: LISTEN 7024/mysqldtcp6 0 0 :::111 ::: LISTEN 1/systemdtcp6 0 0 :::80 ::: LISTEN 1823/nginx: mastertcp6 0 0 :::6000 ::: LISTEN 6706/Xtcp6 0 0 :::22 ::: LISTEN 6296/sshdtcp6 0 0 ::1:631 ::: LISTEN 6299/cupsdtcp6 0 0 ::1:6010 ::: LISTEN 13051/sshd: qi@pts/tcp6 0 0 ::1:6011 ::: LISTEN 23741/sshd: qi@pts/tcp6 0 0 ::1:6012 ::: LISTEN 31798/sshd: qi@pts/ 12kill 1823service nginx star Centos7 yum配置Nginx /home/qi/miniconda3/pkgs/jbrowse-1.16.2-pl526h6bb024c_6/opt/jbrowse 又卸载yum remove nginx 成功的操作 将web服务器换成了apache，放文件的目录就是 /var/www/html How to Install Apache on CentOS 7 CentOS 7 安装 Apache, MySQL, PHP 指南 Jbrowse下载安装 下面这句话很关键，提到了不用 root 或 sudo 安装。我也不知道为什么，这部分： Installing Perl prerequisites 成功。这是昨天一直没成功的地方 Formatting data 失败。提取报错信息里的命令，运行，获取错误信息。Can’t locate local/lib.pm 3. Run the automated-setup script, ./setup.sh, which will attempt to install all of JBrowse’s (modest) prerequisites for you in the jbrowse/ directory itself. Note that setup.sh should not be run as root or with sudo. (py3.6) [qi@localhost JBrowse-1.16.3]$ ./setup.shGathering system information …done.NOTE: Legacy scripts wig-to-json.pl and bam-to-json.pl have been removed from setup. Their functionality has been superseded by add-bam-track.pl and add-bw-track.pl. If you require the old versions, please use JBrowse 1.12.3 or earlier.Minimal release, skipping node and Webpack buildInstalling Perl prerequisites …done. Formatting Volvox example data … failed. See setup.log file for error messages. Formatting Yeast example data … failed. See setup.log file for error messages. Formatting Volvox example data … rm -rf sample_data/json/volvox bin/prepare-refseqs.pl –fasta docs/tutorial/data_files/volvox.fa –out sample_data/json/volvoxAttempting to create directory /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlibmkdir sample_data/json/volvox: Permission denied at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JsonFileStorage.pm line 64. Formatting Yeast example data … rm -rf sample_data/json/yeast/ bin/prepare-refseqs.pl –fasta sample_data/raw/yeast_scaffolds/chr1.fa.gz –fasta sample_data/raw/yeast_scaffolds/chr2.fa.gzip –out sample_data/json/yeast/mkdir sample_data/json/yeast/: Permission denied at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JsonFileStorage.pm line 64. Can’t locate local/lib.pm in @INC (@INC contains: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3 /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5 /var/www/html/JBrowse-1.16.3/bin/../src/perl5 /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JBlibs.pm line 25.Compilation failed in require at bin/prepare-refseqs.pl line 5.BEGIN failed–compilation aborted at bin/prepare-refseqs.pl line 5 1234567locate local/lib.pmcp -r /home/qi/perl5/lib/perl5/local /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3sudo bin/prepare-refseqs.pl --fasta docs/tutorial/data_files/volvox.fa --out sample_data/json/volvoxsudo bin/prepare-refseqs.pl --fasta sample_data/raw/yeast_scaffolds/chr1.fa.gz --fasta sample_data/raw/yeast_scaffolds/chr2.fa.gzip --out sample_data/json/yeast/ 地址： http://10.164.6.154/JBrowse-1.16.3/index.html?data=sample_data/json/volvox 2019-2-28 19:53:01 今天登入ip的时候，没有显示。 Enable Apache to start at boot, and restart the service for the above changes to take effect: 12sudo systemctl enable httpd.servicesudo systemctl restart httpd.service 1234# 确认http服务是否开启，端口 80sudo systemctl is-enabled httpd.service# 查看历史记录，awk去掉第一列，sed去掉行首空格history | tail -n 40 | awk '&#123;$1="";print $0&#125;' | sed 's/^ *//' 准备参考序列和特征数据JBrowser 用的是 perl 5.16.3 所有track都生成在默认的data目录中 123456# 指定perl为/usr/bin/perl # seq trackList.json tracks.conf/usr/bin/perl bin/prepare-refseqs.pl --fasta ~/data2/tan_rna/genome/HWB.chromosome.fa# tracks/usr/bin/perl bin/flatfile-to-json.pl --gff HWB.gene.models.gff3 --trackType CanvasFeatures --trackLabel HWBgff 不指定会出现如下错误：[^1] (base) [qi@localhost JBrowse-1.16.3]$ bin/flatfile-to-json.pl –helpperl: symbol lookup error: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi/auto/JSON/XS/XS.so: undefined symbol: Perl_xs_apiversion_bootcheck 查资料 This type of error is almost always indicates you are loading a module that was installed using a different build of Perl. https://stackoverflow.com/questions/51375821/perl-xs-apiversion-bootcheck [^1]: 不指定，如果用sudo 也不会出现错误，如：sudo bin/prepare-refseqs.pl --fasta ~/data2/tan_rna/genome/HWB.chromosome.fa --out sample_data/json/HWB 不清楚原因 建立索引方便搜索缺这一步，搜索功能不能用 12# 生成 ./data/names/usr/bin/perl bin/generate-names.pl http://10.164.6.154/JBrowse-1.16.3/index.html?data=data VCF trackshttp://jbrowse.org/docs/variants.html 123for i in *.vcf; do bgzip $i; donefor i in *.vcf.gz; do tabix -p vcf $i; done]]></content>
      <tags>
        <tag>软件</tag>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用GATK和snakemake框架的总结]]></title>
    <url>%2F2019%2F02%2F20%2Fsnakemake%2F</url>
    <content type="text"><![CDATA[[TOC] 标准流程 检查md5值12md5sum -c checksums.md5# 报错,使用dos2unix转化格式 trimmomatic1234567# 2019-2-12 17:16:35 trimfor i in *_1.clean.fq.gzdo trimmomatic PE -threads 8 -phred33 $i $&#123;i/_1/_2&#125; -baseout $&#123;i:0:7&#125;.fq.gz \ ILLUMINACLIP:/home/qi/miniconda3/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 \ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done 2019-2-13 12:55:38 查看结果，只trim了一部分，查看trim.log。HB-9-3终止。 ps -aux | grep trimmomatic 查看任务是否还在运行。没有在运行。程序不知什么原因意外中断，来的时候xshell也没有连上服务器，怀疑是服务器自动重启了。 12345678910nohup mv HB-*.fq.gz trim &amp;rm nohup.outcd trimrm HB-9-3__*fq.gzmv HB-9-3* ..cd ..# trim剩下的fq.gznohup time bash trim.sh &gt; trim.log 2&gt;&amp;1 &amp;jobsps -aux | grep trimmomatic 2019-2-13 17:06:38 查看任务，意外终止。NH-11-2 移动、删除文件，reboot，重新执行脚本。 1nohup time bash trim.sh &gt;&gt; trim.log 2&gt;&amp;1 &amp; 2019-2-13 19:35:51 查看，又意外停止 查看系统最后重启时间 1who -b 显然，服务器会自动重启。 不运行任务的时候，不会重启。是因为CPU温度过高？ 更改脚本试一试，把-threads 8 改成 4 123456for i in *_1.clean.fq.gzdo trimmomatic PE -threads 4 -phred33 $i $&#123;i/_1/_2&#125; -baseout $&#123;i:0:7&#125;.fq.gz \ ILLUMINACLIP:/home/qi/miniconda3/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 \ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done 1nohup time bash trim.sh &gt; trim.log 2&gt;&amp;1 &amp; 没有效果 threads改成16 一小时不到又重启了。。 2019-2-14 16:16:57 质检1fastqc *.fq.gz -o qcDIR/ -t 6 -d qcDIR/ &gt;&gt;fastqc.r.log 2&gt;&gt;fastqc.e.log 1conda install -c bioconda multiqc 做multiqc之前构建一个python3的虚拟环境 1234567891011python --versionconda create --name py3.6 python=3.6source activate py3.6# You'll want to add the source activate py3.6 line to your .bashrc file so that the environment is loaded every time you load the terminal.# conda deactivate# conda activate py3.6# Windows: activate py3.6# 需要重新安装conda install -c bioconda multiqc 123#利用multiqc整合结果，方便批量查看mkdir qcDIR_multiqcmultiqc qcDIR/ -o qcDIR_multiqc/ 用 snakemake 编写任务流程snakemake是一个用来编写任务流程的工具，用python写的，因此其执行的流程脚本也比较通俗易懂，易于理解，可以看做用户友好版的make。（make在安装软件的时候会用到，没有研究过makefile文件，对它的用处不是太了解。） 其实流程控制是复杂任务（在生信领域很常见）必需的关注点。只是snakemake对于代码功力不够的人来说，在写好代码与重复流程的花销的trade off上，还不如一遍遍重复流程。。但是真的是一个写好了就很好用的东西。 snakemake能够使用文件名通配的方式对一类文件进行处理 12# installingconda install -c bioconda snakemake 简单的例子12345678910111213cd $HOME# Create a folder where we will run our commands:mkdir snakemake-examplecd snakemake-example# Make a fake genome:touch genome.fa# Make some fake data:mkdir fastqtouch fastq/Sample1.R1.fastq.gz fastq/Sample1.R2.fastq.gztouch fastq/Sample2.R1.fastq.gz fastq/Sample2.R2.fastq.gz snakemake 脚本 123456789101112131415SAMPLES = ['Sample1', 'Sample2']rule all: input: expand('&#123;sample&#125;.txt', sample=SAMPLES)rule quantify_genes: input: genome = 'genome.fa', r1 = 'fastq/&#123;sample&#125;.R1.fastq.gz', r2 = 'fastq/&#123;sample&#125;.R2.fastq.gz' output: '&#123;sample&#125;.txt' shell: 'echo &#123;input.genome&#125; &#123;input.r1&#125; &#123;input.r2&#125; &gt; &#123;output&#125;' 学院集群上运行 1snakemake --snakefile Snakefile 12# 可视化snakemake --forceall --dag | dot -Tpng &gt; dag1.png snakemake 规则 Snakemake基于规则执行命令，规则一般由input, output,shell三部分组成。除了rule all，其余必须有output Snakemake可以自动确定不同规则的输入输出的依赖关系，根据时间戳来判断文件是否需要重新生成 Snakemake以{sample}.fa形式进行文件名通配，用{wildcards.sample}获取sample的实际文件名 Snakemake用expand()生成多个文件名，本质是Python的列表推导式 Snakemake可以在规则外直接写Python代码，在规则内的run里也可以写Python代码。 Snakefile的第一个规则通常是rule all，根据all里的文件决定执行哪些rule。如上面的例子，注释掉all里的input则不执行第二条rule，（推断未尝试：rule all里定义最终的输出文件，程序也能执行，那么rule里的输出文件在什么时候会被删除，是在所有rule运行完之后，还是在判断出该输出文件不会被用到的时候？） 在output中的结果文件可以是未存在目录中的文件,这时会自动创建不存在的目录（不需要事先建文件夹，这个功能实在是方便） snakemake 命令 wildcards: 用来获取通配符匹配到的部分，例如对于通配符”{dataset}/file.{group}.txt”匹配到文件101/file.A.txt，则{wildcards.dataset}就是101，{wildcards.group}就是A。 temp: 通过temp方法可以在所有rule运行完后删除指定的中间文件，eg.output: temp(“f1.bam”)。 protected: 用来指定某些中间文件是需要保留的，eg.output: protected(“f1.bam”)。 snakemake 执行一般讲所有的参数配置写入Snakefile后直接在Snakefile所在路径执行snakemake命令即可开始执行流程任务，如果只有一个snakefile的话，连文件都不用写。一些常用的参数： 123456789101112131415161718--snakefile, -s 指定Snakefile，否则是当前目录下的Snakefile--dryrun, -n 不真正执行，一般用来查看Snakefile是否有错--printshellcmds, -p 输出要执行的shell命令--reason, -r 输出每条rule执行的原因,默认FALSE--cores, --jobs, -j 指定运行的核数，若不指定，则使用最大的核数--force, -f 重新运行第一条rule或指定的rule--forceall, -F 重新运行所有的rule，不管是否已经有输出结果--forcerun, -R 重新执行Snakefile，当更新了rule时候使用此命令#一些可视化命令$ snakemake --dag | dot -Tpdf &gt; dag.pdf#集群投递snakemake --cluster "qsub -V -cwd -q 节点队列" -j 10# --cluster: 集群运行指令# qusb -V -cwd -q， 表示输出当前环境变量(-V),在当前目录下运行(-cwd), 投递到指定的队列(-q), 如果不指定则使用任何可用队列# --local-cores N: 在每个集群中最多并行N核# --cluster-config/-u FILE: 集群配置文件 snakemake 实践123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187# 2019-2-19 10:28:52 part# GATK snakemake# qizhengyangfrom os.path import joinGENOME = 'genome/HWB.chromosome.fa'GTF = 'genes/HWB.gene.models.gtf'(SAMPLES,) = glob_wildcards('pairedDIR/&#123;sample&#125;_1P.fq.gz')PATTERN_R1 = join('pairedDIR', '&#123;sample&#125;_1P.fq.gz')PATTERN_R2 = join('pairedDIR', '&#123;sample&#125;_2P.fq.gz')rule all: input: 'star_index_2pass/', expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES), 'star_index_2pass/', expand('star_2pass/&#123;sample&#125;Aligned.out.sam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_rg_added_sorted.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup_split.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;.vcf', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_filtered.vcf', sample=SAMPLES)rule star_index: input: genome = GENOME, gtf = GTF output: # 最后加上 directory()，不然在集群上运行会报错 star_index = directory('star_index/') log: 'star_index.log' threads: 8 run: # star 1-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.star_index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbGTFfile &#123;input.gtf&#125;' ' 2&gt; &#123;log&#125;')rule star_1pass_align: input: index = 'star_index/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: index = 'star_1pass/&#123;sample&#125;SJ.out.tab' threads: 8 params: prefix = './star_1pass/&#123;sample&#125;' # 在使用params之前是报错的，NameError,The name 'sample' is unknown in this context run: # star 1-pass align, OK shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule star_2pass_index: input: genome = GENOME, # 这里必需加expand，不然会报错：Wildcards in input files cannot be determined from output files: # 'sample'。 # 报错信息说通配的信息不能从output里推断出来，因为我的output是文件夹。input应该是所有样品的信息，可以用expand函数，这样通配的问题就没有了。 # 然后用--sjdbFileChrStartEnd参数将所有样品的SJ.out.tab文件作为输入的annotated junction进行第二次建index # http://www.bioinfo-scrounger.com/archives/288 # 不能加三个引号（"""或'''注释）进行段落注释 splice_site = expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES) output: index = directory('star_index_2pass/') threads: 8 run: # star 2-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbFileChrStartEnd &#123;input.splice_site&#125;')rule star_2pass_align: input: index = 'star_index_2pass/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' threads: 8 params: prefix = 'star_2pass/&#123;sample&#125;' run: # star 2-pass align shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule picard: input: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' output: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' run: # RGID和RGSM的sample必须是&#123;wildcards.sample&#125;，不然 # The name 'sample' is unknown in this context. Please make sure that you defined that variable. # Also note that braces not used for variable access have to be escaped by repeating them, i.e. &#123;&#123;print $1&#125;&#125; shell('picard AddOrReplaceReadGroups' ' I=&#123;input.sam&#125;' ' O=&#123;output.bam&#125;' ' SO=coordinate' ' RGID=&#123;wildcards.sample&#125;' ' RGLB=rna' ' RGPL=illumina' ' RGPU=hiseq' ' RGSM=&#123;wildcards.sample&#125;')rule picard_markduplicates: input: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' output: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam' params: dedup_metrices = 'star_2pass/&#123;sample&#125;_dedup.metrics' run: shell('picard MarkDuplicates' ' I=&#123;input.bam&#125;' ' O=&#123;output.dedup_bam&#125;' ' CREATE_INDEX=true' ' VALIDATION_STRINGENCY=SILENT' ' M=&#123;params.dedup_metrices&#125;')rule gatk_split: input: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam', genome = GENOME output: split_bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam' run: shell('java -jar $GATK -T SplitNCigarReads' ' -R &#123;input.genome&#125;' ' -I &#123;input.dedup_bam&#125;' ' -o &#123;output.split_bam&#125;' ' -rf ReassignOneMappingQuality' ' -RMQF 255' ' -RMQT 60' ' -U ALLOW_N_CIGAR_READS')rule gatk: input: bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam', genome = GENOME output: vcf = 'star_2pass/&#123;sample&#125;.vcf' run: shell('java -jar $GATK -T HaplotypeCaller' ' -R &#123;input.genome&#125;' ' -I &#123;input.bam&#125;' ' -dontUseSoftClippedBases' ' -stand_call_conf 20.0' ' -o &#123;output.vcf&#125;')rule gatk_filter: input: genome = GENOME, vcf = 'star_2pass/&#123;sample&#125;.vcf' output: 'star_2pass/&#123;sample&#125;_filtered.vcf' run: shell('java -jar $GATK' ' -T VariantFiltration' ' -R &#123;input.genome&#125;' ' -V &#123;input.vcf&#125;' ' -window 35' ' -cluster 3' ' -filterName FS -filter "FS &gt; 30.0"' ' -filterName QD -filter "QD &lt; 2.0"' ' -o &#123;output&#125;') 实验室服务器试运行 By default snakemake executes the first rule in the snakefile. output中会自动创建没有的文件夹 12# cores设置核心数snakemake -s part.py --cores 8 跑samples中的所有样品 存储空间不够，停止 重新运行之后，是从NH-12-1开始 之前的结果只有NH-12-1Log.out，时间是2019/2/20 3:35 现在是： 说明这个样本从头开始跑了。 集群上运行12345678910111213141516# 查看节点状态pestat# 测试 dry runsnakemake -n# 检测某条rulesnakemake -n -r star_index# 可视化# snakemake --dag | dot -Tpdf &gt; dag.pdfsnakemake --forceall --dag | dot -Tpdf &gt; dag.pdf# # 投递任务 -cwd不能用 [qsub: illegal -c value]# snakemake --cluster "qsub -V -cwd -q high" -j 20snakemake --cluster "qsub -V -q high" -j 20# 报错# Complete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-20T152054.823542.snakemake.log 12# 查看错误信息 输出目录要用 directory() ，在实验室服务器上没有遇上这种情况less snakejob.star_index.254.sh.e980069 ImproperOutputException in line 30 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Outputs of incorrect type (directories when expecting files or vice versa). Output directories must be flagged with directory(). for rule star_index:star_index/Removing output files of failed job star_index since they might be corrupted:star_index/Skipped removing non-empty directory star_index/Shutting down, this might take some time. 12345snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20# -j 必须指定，否则Error: you need to specify the maximum number of jobs to be queued or executed at the same time with --jobs.# Here, -j denotes the number of jobs submitted being submitted to the cluster at the same time# -j 代表可以同时提交的任务数# 没有重新运行star_index这个任务 2019-2-20 20:22:01 错误： 1less snakejob.star_1pass_align.28.sh.e980101 Possible cause 1: not enough RAM. Check if you have enough RAM 6975932175 bytes 123# 剩下的三个重新跑 snakemake -nsnakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 没有安装picard。。这种低级错误千万不能再犯了 12conda install -c bioconda picard snakemake -n 终端意外退出，输出到屏幕上的信息没有了 123456789ls -acd .snakemake/log# 按修改时间查看文件# l长格式显示，t按时间排序（-r升序）ls -lth# snakemake程序停止# 已经提交的任务会运行完snakemake -n --quiet 1snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 Error: Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory 123# 解决办法 https://zhuanlan.zhihu.com/p/47575136snakemake --unlocksnakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 报错 1less snakejob.star_2pass_align.68.sh.e980306 libgcc_s.so.1 must be installed for pthread_cancel to work 123less snakejob.star_2pass_align.68.sh.e980306 # 会自动删除失败任务的输出文件# Removing output files of failed job star_2pass_align since they might be corrupted: 短暂的断网，与服务器连接中断 12snakemake -n# 报错，因为已经提交的任务没有运行完 nohup将程序放入后台12whatis nohupman 1 nohup If standard input is a terminal, redirect it from /dev/null. If standard output is a terminal, append output to ‘nohup.out’ if possible, ‘$HOME/nohup.out’ otherwise. If standard error is a terminal, redirect it to standard output. To save output to FILE, use ‘nohup COMMAND &gt; FILE’. 重定向与后台运行的知识 12345678nohup snakemake -n &amp;# [qizhengyang@node1 gatk_rna]$ nohup: 忽略输入并把输出追加到"nohup.out"nohup snakemake -n &gt; test_nohup.out 2&gt;&amp;1 &amp;# 标准输出和标准错误输出都重定向到文件snakemake -n &gt; test_nohup.out 2&gt;&amp;1 &amp;# 会产生进程号 12# nohup方式后台运行(&amp;) 忽略所有发送给子命令的挂断（SIGHUP）信号 重定向子命令的标准输出(stdout)和标准错误(stderr)nohup snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 &gt; snakemake.out 2&gt;&amp;1 &amp; IP 意外变化2019-2-21 16:45:01 实验室的服务器连不上，我查看了ssh server是否开启，查了IP，发现是IP变了。 重新开始2019-2-21 21:33:58查看了实验室服务器生的Index文件夹，比较了大小，之前第一步需要重做。。很悲惨 gatk加了 -Xmx10g -jar # Decrease Java heap size (-Xmx/-Xms) 123snakemake -n -s Snakefile2 --quiet# 调了最大并行数，因为会有内存不够的错误nohup snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 10 &amp;&gt; snakemake.out &amp; 1snakemake --unlock 之后再运行命令 12345678nohup snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 10 &amp;&gt; snakemake.out &amp;# snakemake.out内容重新写入# 查看任务jobs# 或ps aux|grep Snakefile2|grep -v grep# https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/ps.html 2019-2-22 14:56:03 查看任务 1ps aux|grep Snakefile2|grep -v grep 这个任务在内存中，但 qstat没有任务在运行 12345less snakemake.outcd .snakemake/log/ls -thlless 2019-02-21T224633.856137.snakemake.log # 与snakemake.out里的信息一样，没有错误提示，snakemake -n -s Snakefile2 --quiet 这个进程kill不掉 kill 7550 但是，好像起作用了，log文件时间改了，似乎多了一句话（下次log文件应该copy一份下来，或者将最后的内容粘贴下来） Will exit after finishing currently running jobs. 重新运行 123snakemake --unlock# 不放入后台，-j 20，实时监测snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 20 kill -9 7550 有效果 两个进程信息比较 院里集群上的时间快18分钟 报错先查看snakemake.log，然后查看job的std err star 2pass align 出错 2019-2-22 20:06:58 12# 查看错误信息less snakejob.star_2pass_align.52.sh.e980754 Building DAG of jobs…Using shell: /bin/bashProvided cores: 20Rules claiming more threads will be scaled down.Job counts: count jobs 1 star_2pass_align 1 [Fri Feb 22 16:44:54 2019]rule star_2pass_align: input: star_index_2pass/, pairedDIR/OV-9-2_1P.fq.gz, pairedDIR/OV-9-2_2P.fq.gz output: star_2pass/OV-9-2Aligned.out.sam jobid: 0 wildcards: sample=OV-9-2 threads: 20 libgcc_s.so.1 must be installed for pthread_cancel to workterminate called after throwing an instance of ‘St9bad_alloc’libgcc_s.so.1 must be installed for pthread_cancel to work what(): std::bad_alloc[Fri Feb 22 16:46:26 2019]Error in rule star_2pass_align: jobid: 0 output: star_2pass/OV-9-2Aligned.out.sam RuleException:CalledProcessError in line 101 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile2:Command ‘set -euo pipefail; STAR –runThreadN 20 –genomeDir star_index_2pass/ –readFilesIn pairedDIR/OV-9-2_1P.fq.gz pairedDIR/OV-9-2_2P.fq.gz –readFilesCommand zcat –outFileNamePrefix star_2pass/OV-9-2’ returned non-zero exit status 141.snakejob.star_2pass_align.52.sh.e980754 123# 查看任务qstatpestat 12345pbsnodes -a | lesssnakemake --cluster --rerun-incomplete qsub --jobs 10# qstat -a (for summary)# qstat -n (You will see where the nodes the job lands) [qizhengyang@node1 gatk_rna]$ snakemake -nBuilding DAG of jobs…IncompleteFilesException:The files below seem to be incomplete. If you are sure that certain files are not incomplete, mark them as complete with snakemake --cleanup-metadata &lt;filenames&gt; To re-generate the files rerun your command with the –rerun-incomplete flag.Incomplete files:star_2pass/OV-10-1Aligned.out.samstar_2pass/NH-10-3Aligned.out.samstar_2pass/OV-10-2Aligned.out.samstar_2pass/NH-11-2Aligned.out.samstar_2pass/HB-9-3Aligned.out.samstar_2pass/HB-10-3Aligned.out.sam 这样是在当前节点运行的 [qizhengyang@node1 gatk_rna]$ snakemake --rerun-incompleteBuilding DAG of jobs…Using shell: /bin/bashProvided cores: 1Rules claiming more threads will be scaled down.Job counts: count jobs 1 all 36 gatk 36 gatk_filter 36 gatk_split 36 picard 36 picard_markduplicates 25 star_2pass_align 206 [Fri Feb 22 23:06:04 2019]rule picard: input: star_2pass/OV-12-3Aligned.out.sam output: star_2pass/OV-12-3_rg_added_sorted.bam jobid: 101 wildcards: sample=OV-12-3 Job counts: count jobs 1 picard 1INFO 2019-02-22 23:06:07 AddOrReplaceReadGroups 12# 只有胖节点有用，将任务提交到胖节点上，比需指定 -q low。默认是 high，会处于排队状态snakemake --rerun-incomplete --cluster "qsub -q low" --jobs 10 [qizhengyang@node1 gatk_rna]$ snakemake –rerun-incomplete –cluster “qsub -q low” –jobs 10Building DAG of jobs…Using shell: /bin/bashProvided cluster nodes: 10Job counts: count jobs 1 all 36 gatk 36 gatk_filter 36 gatk_split 36 picard 36 picard_markduplicates 25 star_2pass_align 206 [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/HB-12-1Aligned.out.sam output: star_2pass/HB-12-1_rg_added_sorted.bam jobid: 106 wildcards: sample=HB-12-1 Submitted job 106 with external jobid ‘980770.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/NH-9-2Aligned.out.sam output: star_2pass/NH-9-2_rg_added_sorted.bam jobid: 108 wildcards: sample=NH-9-2 Submitted job 108 with external jobid ‘980771.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/OV-12-3Aligned.out.sam output: star_2pass/OV-12-3_rg_added_sorted.bam jobid: 101 wildcards: sample=OV-12-3 Submitted job 101 with external jobid ‘980772.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/HB-10-1Aligned.out.sam output: star_2pass/HB-10-1_rg_added_sorted.bam jobid: 85 wildcards: sample=HB-10-1 Submitted job 85 with external jobid ‘980773.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/NH-9-3Aligned.out.sam output: star_2pass/NH-9-3_rg_added_sorted.bam jobid: 74 wildcards: sample=NH-9-3 Submitted job 74 with external jobid ‘980774.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/OV-10-3Aligned.out.sam output: star_2pass/OV-10-3_rg_added_sorted.bam jobid: 98 wildcards: sample=OV-10-3 Submitted job 98 with external jobid ‘980775.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/HB-11-2Aligned.out.sam output: star_2pass/HB-11-2_rg_added_sorted.bam jobid: 77 wildcards: sample=HB-11-2 Submitted job 77 with external jobid ‘980776.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/OV-11-2Aligned.out.sam output: star_2pass/OV-11-2_rg_added_sorted.bam jobid: 90 wildcards: sample=OV-11-2 Submitted job 90 with external jobid ‘980777.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/NH-12-1Aligned.out.sam output: star_2pass/NH-12-1_rg_added_sorted.bam jobid: 107 wildcards: sample=NH-12-1 Submitted job 107 with external jobid ‘980778.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/OV-12-1Aligned.out.sam output: star_2pass/OV-12-1_rg_added_sorted.bam jobid: 102 wildcards: sample=OV-12-1 Submitted job 102 with external jobid ‘980779.node1’. 运行时间 picard 半小时，CPU时间是4小时。但是picard我并没有设置使用多少线程。 qstat qstat -a qstat -n 2019-2-23 11:02:59 Shutting down, this might take some time.Exiting because a job execution failed. Look above for error messageComplete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-22T232454.376651.snakemake.log 出错的任务 [Sat Feb 23 01:21:32 2019]Error in rule picard: jobid: 88 output: star_2pass/OV-9-2_rg_added_sorted.bam cluster_jobid: 980797.node1 1less snakejob.picard.88.sh.e980797 Building DAG of jobs…Using shell: /bin/bashProvided cores: 96Rules claiming more threads will be scaled down.Job counts: count jobs 1 picard 1 [Sat Feb 23 00:58:20 2019]rule picard: input: star_2pass/OV-9-2Aligned.out.sam output: star_2pass/OV-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=OV-9-2 INFO 2019-02-23 00:58:23 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/OV-9-2Aligned.out.sam -O star_2pass/OV-9-2_rg_added_sorted.bam -SO coordinate -RGID OV-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM OV-9-2 00:58:23.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 00:58:23 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/OV-9-2Aligned.out.sam OUTPUT=star_2pass/OV-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 00:58:23 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 00:58:27 AddOrReplaceReadGroups Created read-group ID=OV-9-2 PL=illumina LB=rna SM=OV-9-2 INFO 2019-02-23 00:59:00 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 33s. Last read position: chr9:26,425,698INFO 2019-02-23 00:59:34 AddOrReplaceReadGroups Processed 2,000,000 records. Elapsed time: 00:01:07s. Time for last 1,000,000: 33s. Last read position: chr5:38,200,263INFO 2019-02-23 01:00:29 AddOrReplaceReadGroups Processed 3,000,000 records. Elapsed time: 00:02:02s. Time for last 1,000,000: 55s. Last read position: chr4:1,886,332INFO 2019-02-23 01:00:56 AddOrReplaceReadGroups Processed 4,000,000 records. Elapsed time: 00:02:28s. Time for last 1,000,000: 26s. Last read position: chr5:6,379,073INFO 2019-02-23 01:01:34 AddOrReplaceReadGroups Processed 5,000,000 records. Elapsed time: 00:03:07s. Time for last 1,000,000: 38s. Last read position: chr6:18,567,116INFO 2019-02-23 01:02:13 AddOrReplaceReadGroups Processed 6,000,000 records. Elapsed time: 00:03:46s. Time for last 1,000,000: 38s. Last read position: chr9:39,573,973INFO 2019-02-23 01:02:40 AddOrReplaceReadGroups Processed 7,000,000 records. Elapsed time: 00:04:13s. Time for last 1,000,000: 27s. Last read position: chr5:22,938,417INFO 2019-02-23 01:03:19 AddOrReplaceReadGroups Processed 8,000,000 records. Elapsed time: 00:04:51s. Time for last 1,000,000: 38s. Last read position: chr4:23,545,960INFO 2019-02-23 01:03:55 AddOrReplaceReadGroups Processed 9,000,000 records. Elapsed time: 00:05:28s. Time for last 1,000,000: 36s. Last read position: chr4:17,586,047INFO 2019-02-23 01:04:24 AddOrReplaceReadGroups Processed 10,000,000 records. Elapsed time: 00:05:57s. Time for last 1,000,000: 28s. Last read position: chr7:21,260,263INFO 2019-02-23 01:04:58 AddOrReplaceReadGroups Processed 11,000,000 records. Elapsed time: 00:06:31s. Time for last 1,000,000: 33s. Last read position: chr4:25,569,847INFO 2019-02-23 01:05:36 AddOrReplaceReadGroups Processed 12,000,000 records. Elapsed time: 00:07:08s. Time for last 1,000,000: 37s. Last read position: chr4:25,075,124INFO 2019-02-23 01:06:04 AddOrReplaceReadGroups Processed 13,000,000 records. Elapsed time: 00:07:36s. Time for last 1,000,000: 28s. Last read position: chr1:5,055,820INFO 2019-02-23 01:06:38 AddOrReplaceReadGroups Processed 14,000,000 records. Elapsed time: 00:08:11s. Time for last 1,000,000: 34s. Last read position: chr5:40,818,516INFO 2019-02-23 01:07:13 AddOrReplaceReadGroups Processed 15,000,000 records. Elapsed time: 00:08:46s. Time for last 1,000,000: 34s. Last read position: chr3:28,597,426INFO 2019-02-23 01:07:40 AddOrReplaceReadGroups Processed 16,000,000 records. Elapsed time: 00:09:12s. Time for last 1,000,000: 26s. Last read position: chr2:52,066,083INFO 2019-02-23 01:08:24 AddOrReplaceReadGroups Processed 17,000,000 records. Elapsed time: 00:09:54s. Time for last 1,000,000: 41s. Last read position: chr5:8,214,376INFO 2019-02-23 01:08:53 AddOrReplaceReadGroups Processed 18,000,000 records. Elapsed time: 00:10:25s. Time for last 1,000,000: 31s. Last read position: chr2:42,040,926INFO 2019-02-23 01:09:19 AddOrReplaceReadGroups Processed 19,000,000 records. Elapsed time: 00:10:52s. Time for last 1,000,000: 26s. Last read position: chr8:3,160,150INFO 2019-02-23 01:09:59 AddOrReplaceReadGroups Processed 20,000,000 records. Elapsed time: 00:11:32s. Time for last 1,000,000: 39s. Last read position: chr9:2,765,258INFO 2019-02-23 01:10:36 AddOrReplaceReadGroups Processed 21,000,000 records. Elapsed time: 00:12:08s. Time for last 1,000,000: 36s. Last read position: chr7:872,502INFO 2019-02-23 01:11:13 AddOrReplaceReadGroups Processed 22,000,000 records. Elapsed time: 00:12:46s. Time for last 1,000,000: 37s. Last read position: chr5:38,922,782INFO 2019-02-23 01:11:43 AddOrReplaceReadGroups Processed 23,000,000 records. Elapsed time: 00:13:15s. Time for last 1,000,000: 29s. Last read position: chr2:43,156,703INFO 2019-02-23 01:12:10 AddOrReplaceReadGroups Processed 24,000,000 records. Elapsed time: 00:13:42s. Time for last 1,000,000: 26s. Last read position: chr2:42,168,429INFO 2019-02-23 01:12:37 AddOrReplaceReadGroups Processed 25,000,000 records. Elapsed time: 00:14:10s. Time for last 1,000,000: 27s. Last read position: chr5:47,894,574INFO 2019-02-23 01:13:05 AddOrReplaceReadGroups Processed 26,000,000 records. Elapsed time: 00:14:37s. Time for last 1,000,000: 27s. Last read position: chr5:47,836,828INFO 2019-02-23 01:13:33 AddOrReplaceReadGroups Processed 27,000,000 records. Elapsed time: 00:15:05s. Time for last 1,000,000: 27s. Last read position: chr8:19,290,720INFO 2019-02-23 01:14:01 AddOrReplaceReadGroups Processed 28,000,000 records. Elapsed time: 00:15:33s. Time for last 1,000,000: 27s. Last read position: chr5:17,045,282INFO 2019-02-23 01:14:29 AddOrReplaceReadGroups Processed 29,000,000 records. Elapsed time: 00:16:02s. Time for last 1,000,000: 28s. Last read position: chr4:16,374,034INFO 2019-02-23 01:14:56 AddOrReplaceReadGroups Processed 30,000,000 records. Elapsed time: 00:16:28s. Time for last 1,000,000: 26s. Last read position: chr2:51,915,818INFO 2019-02-23 01:15:24 AddOrReplaceReadGroups Processed 31,000,000 records. Elapsed time: 00:16:57s. Time for last 1,000,000: 28s. Last read position: chr3:24,910,383INFO 2019-02-23 01:15:55 AddOrReplaceReadGroups Processed 32,000,000 records. Elapsed time: 00:17:27s. Time for last 1,000,000: 30s. Last read position: chr7:12,459,410INFO 2019-02-23 01:16:22 AddOrReplaceReadGroups Processed 33,000,000 records. Elapsed time: 00:17:54s. Time for last 1,000,000: 26s. Last read position: chr9:39,689,303[Sat Feb 23 01:20:04 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 21.69 minutes.Runtime.totalMemory()=1004011520To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.lang.String.substring(String.java:1969) at htsjdk.samtools.util.StringUtil.split(StringUtil.java:89) at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:229) at htsjdk.samtools.SAMTextReader$RecordIterator.parseLine(SAMTextReader.java:268) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:255) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113) 1 picard 1 [Sat Feb 23 00:58:20 2019]rule picard: input: star_2pass/OV-9-2Aligned.out.sam output: star_2pass/OV-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=OV-9-2 INFO 2019-02-23 00:58:23 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/OV-9-2Aligned.out.sam -O star_2pass/OV-9-2_rg_added_sorted.bam -SO coordinate -RGID OV-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM OV-9-2 00:58:23.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 00:58:23 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/OV-9-2Aligned.out.sam OUTPUT=star_2pass/OV-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 00:58:23 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 00:58:27 AddOrReplaceReadGroups Created read-group ID=OV-9-2 PL=illumina LB=rna SM=OV-9-2 INFO 2019-02-23 00:59:00 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 33s. Last read position: chr9:26,425,698INFO 2019-02-23 00:59:34 AddOrReplaceReadGroups Processed 2,000,000 records. Elapsed time: 00:01:07s. Time for last 1,000,000: 33s. Last read position: chr5:38,200,263INFO 2019-02-23 01:00:29 AddOrReplaceReadGroups Processed 3,000,000 records. Elapsed time: 00:02:02s. Time for last 1,000,000: 55s. Last read position: chr4:1,886,332INFO 2019-02-23 01:00:56 AddOrReplaceReadGroups Processed 4,000,000 records. Elapsed time: 00:02:28s. Time for last 1,000,000: 26s. Last read position: chr5:6,379,073INFO 2019-02-23 01:01:34 AddOrReplaceReadGroups Processed 5,000,000 records. Elapsed time: 00:03:07s. Time for last 1,000,000: 38s. Last read position: chr6:18,567,116INFO 2019-02-23 01:02:13 AddOrReplaceReadGroups Processed 6,000,000 records. Elapsed time: 00:03:46s. Time for last 1,000,000: 38s. Last read position: chr9:39,573,973INFO 2019-02-23 01:02:40 AddOrReplaceReadGroups Processed 7,000,000 records. Elapsed time: 00:04:13s. Time for last 1,000,000: 27s. Last read position: chr5:22,938,417INFO 2019-02-23 01:03:19 AddOrReplaceReadGroups Processed 8,000,000 records. Elapsed time: 00:04:51s. Time for last 1,000,000: 38s. Last read position: chr4:23,545,960INFO 2019-02-23 01:03:55 AddOrReplaceReadGroups Processed 9,000,000 records. Elapsed time: 00:05:28s. Time for last 1,000,000: 36s. Last read position: chr4:17,586,047INFO 2019-02-23 01:04:24 AddOrReplaceReadGroups Processed 10,000,000 records. Elapsed time: 00:05:57s. Time for last 1,000,000: 28s. Last read position: chr7:21,260,263INFO 2019-02-23 01:04:58 AddOrReplaceReadGroups Processed 11,000,000 records. Elapsed time: 00:06:31s. Time for last 1,000,000: 33s. Last read position: chr4:25,569,847INFO 2019-02-23 01:05:36 AddOrReplaceReadGroups Processed 12,000,000 records. Elapsed time: 00:07:08s. Time for last 1,000,000: 37s. Last read position: chr4:25,075,124INFO 2019-02-23 01:06:04 AddOrReplaceReadGroups Processed 13,000,000 records. Elapsed time: 00:07:36s. Time for last 1,000,000: 28s. Last read position: chr1:5,055,820INFO 2019-02-23 01:06:38 AddOrReplaceReadGroups Processed 14,000,000 records. Elapsed time: 00:08:11s. Time for last 1,000,000: 34s. Last read position: chr5:40,818,516INFO 2019-02-23 01:07:13 AddOrReplaceReadGroups Processed 15,000,000 records. Elapsed time: 00:08:46s. Time for last 1,000,000: 34s. Last read position: chr3:28,597,426INFO 2019-02-23 01:07:40 AddOrReplaceReadGroups Processed 16,000,000 records. Elapsed time: 00:09:12s. Time for last 1,000,000: 26s. Last read position: chr2:52,066,083INFO 2019-02-23 01:08:24 AddOrReplaceReadGroups Processed 17,000,000 records. Elapsed time: 00:09:54s. Time for last 1,000,000: 41s. Last read position: chr5:8,214,376INFO 2019-02-23 01:08:53 AddOrReplaceReadGroups Processed 18,000,000 records. Elapsed time: 00:10:25s. Time for last 1,000,000: 31s. Last read position: chr2:42,040,926INFO 2019-02-23 01:09:19 AddOrReplaceReadGroups Processed 19,000,000 records. Elapsed time: 00:10:52s. Time for last 1,000,000: 26s. Last read position: chr8:3,160,150INFO 2019-02-23 01:09:59 AddOrReplaceReadGroups Processed 20,000,000 records. Elapsed time: 00:11:32s. Time for last 1,000,000: 39s. Last read position: chr9:2,765,258INFO 2019-02-23 01:10:36 AddOrReplaceReadGroups Processed 21,000,000 records. Elapsed time: 00:12:08s. Time for last 1,000,000: 36s. Last read position: chr7:872,502INFO 2019-02-23 01:11:13 AddOrReplaceReadGroups Processed 22,000,000 records. Elapsed time: 00:12:46s. Time for last 1,000,000: 37s. Last read position: chr5:38,922,782INFO 2019-02-23 01:11:43 AddOrReplaceReadGroups Processed 23,000,000 records. Elapsed time: 00:13:15s. Time for last 1,000,000: 29s. Last read position: chr2:43,156,703INFO 2019-02-23 01:12:10 AddOrReplaceReadGroups Processed 24,000,000 records. Elapsed time: 00:13:42s. Time for last 1,000,000: 26s. Last read position: chr2:42,168,429INFO 2019-02-23 01:12:37 AddOrReplaceReadGroups Processed 25,000,000 records. Elapsed time: 00:14:10s. Time for last 1,000,000: 27s. Last read position: chr5:47,894,574INFO 2019-02-23 01:13:05 AddOrReplaceReadGroups Processed 26,000,000 records. Elapsed time: 00:14:37s. Time for last 1,000,000: 27s. Last read position: chr5:47,836,828INFO 2019-02-23 01:13:33 AddOrReplaceReadGroups Processed 27,000,000 records. Elapsed time: 00:15:05s. Time for last 1,000,000: 27s. Last read position: chr8:19,290,720INFO 2019-02-23 01:14:01 AddOrReplaceReadGroups Processed 28,000,000 records. Elapsed time: 00:15:33s. Time for last 1,000,000: 27s. Last read position: chr5:17,045,282INFO 2019-02-23 01:14:29 AddOrReplaceReadGroups Processed 29,000,000 records. Elapsed time: 00:16:02s. Time for last 1,000,000: 28s. Last read position: chr4:16,374,034INFO 2019-02-23 01:14:56 AddOrReplaceReadGroups Processed 30,000,000 records. Elapsed time: 00:16:28s. Time for last 1,000,000: 26s. Last read position: chr2:51,915,818INFO 2019-02-23 01:15:24 AddOrReplaceReadGroups Processed 31,000,000 records. Elapsed time: 00:16:57s. Time for last 1,000,000: 28s. Last read position: chr3:24,910,383INFO 2019-02-23 01:15:55 AddOrReplaceReadGroups Processed 32,000,000 records. Elapsed time: 00:17:27s. Time for last 1,000,000: 30s. Last read position: chr7:12,459,410INFO 2019-02-23 01:16:22 AddOrReplaceReadGroups Processed 33,000,000 records. Elapsed time: 00:17:54s. Time for last 1,000,000: 26s. Last read position: chr9:39,689,303[Sat Feb 23 01:20:04 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 21.69 minutes.Runtime.totalMemory()=1004011520To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.lang.String.substring(String.java:1969) at htsjdk.samtools.util.StringUtil.split(StringUtil.java:89) at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:229) at htsjdk.samtools.SAMTextReader$RecordIterator.parseLine(SAMTextReader.java:268) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:255) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)Exception in thread “Thread-0” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Hashtable.(Hashtable.java:190) at java.util.Hashtable.(Hashtable.java:211) at java.util.Properties.(Properties.java:148) at java.util.Properties.(Properties.java:140) at java.util.logging.LogManager.reset(LogManager.java:1321) at java.util.logging.LogManager$Cleaner.run(LogManager.java:239)Exception in thread “Thread-1” java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:68) at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:227) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at htsjdk.samtools.util.nio.DeleteOnExitPathHook.runHooks(DeleteOnExitPathHook.java:57) at htsjdk.samtools.util.nio.DeleteOnExitPathHook$$Lambda$34/780934299.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)[Sat Feb 23 01:20:54 2019]Error in rule picard: jobid: 0 output: star_2pass/OV-9-2_rg_added_sorted.bam RuleException:CalledProcessError in line 115 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Command ‘set -euo pipefail; picard AddOrReplaceReadGroups I=star_2pass/OV-9-2Aligned.out.sam O=star_2pass/OV-9-2_rg_added_sorted.bam SO=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2’ returned non-zero exit status 1. File “/home02/qizhengyang/qizhengyang/gatk_rna/Snakefile”, line 115, in __rule_picard File “/home02/qizhengyang/anaconda3/lib/python3.6/concurrent/futures/thread.py”, line 56, in runRemoving output files of failed job picard since they might be corrupted:star_2pass/OV-9-2_rg_added_sorted.bamShutting down, this might take some time.Exiting because a job execution failed. Look above for error message(END) 123456snakemake -n --quietsnakemake --cluster "qsub -q low" --jobs 100pestat# 36 jobs, busyqstat -f | less 一个终端与服务器连接终端 Socket error Event: 32 Error: 10053.Connection closing…Socket close. Connection closed by foreign host. Disconnected from remote host(qizhengyang) at 11:29:56. Type `help’ to learn how to use Xshell prompt.[C:~]$ last 显示用户最近登录信息。top用于实时显示 process 的动态。 123456789snakemake -n -s Snakefile2 --quietmv Snakefile Snakefile_temp_1mv Snakefile1 Snakefile_temp_2mv Snakefile2 Snakefilesnakemake --unlocknohup snakemake --cluster "qsub -q low" --jobs 100 &amp;# 输出追加到"nohup.out"，nohup.out文件保留上次的信息 [qizhengyang@node1 gatk_rna]$ jobs[1]+ Running nohup snakemake –cluster “qsub -q low” –jobs 100 &amp;[qizhengyang@node1 gatk_rna]$ ps aux | grep snakemake | grep -v grep528 39349 0.5 0.0 289360 26440 pts/5 Sl 14:00 0:01 /home02/qizhengyang/anaconda3/bin/python3.6 /home02/qizhengyang/anaconda3/bin/snakemake –cluster qsub -q low –jobs 100 1ll -h nohup.out 一个 picard job 出现错误 Exception in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded Moreover, now i am running command like /usr/bin/java -Xmx40g -jar /usr/local/picard-tools-1.129/picard.jar …., and it’s working. Regards, Ravi. ava刚刚出现的年代，有一个相比于其他语言的优势就是，内存回收机制。不需要明确的调用释放内存的API，java就自动完成，这个过程就是Garbage Collection，简称GC。 其实还是有一个终极方法的，而且是治标治本的方法，就是找到占用内存大的地方，把代码优化了，就不会出现这个问题了。 Building DAG of jobs…Using shell: /bin/bashProvided cores: 96Rules claiming more threads will be scaled down.Job counts: count jobs 1 picard 1 [Sat Feb 23 14:00:10 2019]rule picard: input: star_2pass/HB-9-2Aligned.out.sam output: star_2pass/HB-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=HB-9-2 INFO 2019-02-23 14:00:12 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/HB-9-2Aligned.out.sam -O star_2pass/HB-9-2_rg_added_sorted.bam -SO coordinate -RGID HB-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM HB-9-2 14:00:12.937 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 14:00:13 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/HB-9-2Aligned.out.sam OUTPUT=star_2pass/HB-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=HB-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=HB-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 14:00:13 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 14:00:20 AddOrReplaceReadGroups Created read-group ID=HB-9-2 PL=illumina LB=rna SM=HB-9-2 INFO 2019-02-23 14:00:58 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:37s. Time for last 1,000,000: 37s. Last read position: chr1:10,583,000[Sat Feb 23 14:07:47 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 7.58 minutes.Runtime.totalMemory()=999292928To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.io.BufferedReader.readLine(BufferedReader.java:356) at java.io.LineNumberReader.readLine(LineNumberReader.java:201) at htsjdk.samtools.util.BufferedLineReader.readLine(BufferedLineReader.java:68) at htsjdk.samtools.SAMTextReader.advanceLine(SAMTextReader.java:221) at htsjdk.samtools.SAMTextReader.access$800(SAMTextReader.java:37) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:257) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)Exception in thread “Thread-1” java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:68) at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:227) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at htsjdk.samtools.util.nio.DeleteOnExitPathHook.runHooks(DeleteOnExitPathHook.java:57) at htsjdk.samtools.util.nio.DeleteOnExitPathHook$$Lambda$34/780934299.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)[Sat Feb 23 14:09:07 2019]Error in rule picard: jobid: 0 output: star_2pass/HB-9-2_rg_added_sorted.bam RuleException:CalledProcessError in line 115 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Command ‘set -euo pipefail; picard AddOrReplaceReadGroups I=star_2pass/HB-9-2Aligned.out.sam O=star_2pass/HB-9-2_rg_added_sorted.bam SO=coordinate RGID=HB-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=HB-9-2’ returned non-zero exit status 1. File “/home02/qizhengyang/qizhengyang/gatk_rna/Snakefile”, line 115, in __rule_picard File “/home02/qizhengyang/anaconda3/lib/python3.6/concurrent/futures/thread.py”, line 56, in runRemoving output files of failed job picard since they might be corrupted:star_2pass/HB-9-2_rg_added_sorted.bamShutting down, this might take some time.Exiting because a job execution failed. Look above for error message(END) [qizhengyang@node1 gatk_rna]$ snakemake -n –quietJob counts:count jobs1 all36 gatk36 gatk_filter16 gatk_split1 picard13 picard_markduplicat 12snakemake --unlocknohup snakemake --cluster &quot;qsub -q low&quot; --jobs 100 &amp; 解决办法 查看 picard.jar 的路径 设置环境变量 export picard=/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar， 写入 ~/.bash_profile，source 设置JVM最大可用内存为40g，java -Xmx40g -jar $picard。默认是1G [qizhengyang@node1 gatk_rna]$ find ~ -name picard.jar/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar/home02/qizhengyang/anaconda3/pkgs/picard-2.18.26-0/share/picard-2.18.26-0/picard.jar 查看当前环境变量 export | less 980879.node1 …b.gatk.202.sh qizhengyang 01:31:52 R low980880.node1 …b.gatk.182.sh qizhengyang 01:31:40 R low980881.node1 …b.gatk.204.sh qizhengyang 01:31:11 R low980882.node1 …b.gatk.209.sh qizhengyang 01:31:20 R low980884.node1 …b.gatk.214.sh qizhengyang 01:31:59 R low980888.node1 …b.gatk.187.sh qizhengyang 01:30:53 R low980889.node1 …b.gatk.210.sh qizhengyang 01:30:29 R low980891.node1 …b.gatk.186.sh qizhengyang 01:31:41 R low980892.node1 …b.gatk.216.sh qizhengyang 01:30:55 R low980893.node1 …b.gatk.206.sh qizhengyang 01:31:36 R low980894.node1 …b.gatk.192.sh qizhengyang 01:30:59 R low980896.node1 …b.gatk.188.sh qizhengyang 01:31:45 R low980898.node1 …b.gatk.215.sh qizhengyang 01:31:05 R low980899.node1 …_split.167.sh qizhengyang 06:39:52 R low980900.node1 …b.gatk.200.sh qizhengyang 01:31:05 R low980901.node1 …_split.153.sh qizhengyang 05:35:11 R low980902.node1 …b.gatk.201.sh qizhengyang 01:31:57 R low980903.node1 …b.gatk.195.sh qizhengyang 01:29:26 R low980904.node1 …b.gatk.197.sh qizhengyang 01:29:35 R low980905.node1 …b.gatk.185.sh qizhengyang 01:30:31 R low980909.node1 …_split.160.sh qizhengyang 05:36:38 R low980910.node1 …b.gatk.198.sh qizhengyang 01:29:47 R low980912.node1 …b.gatk.193.sh qizhengyang 01:30:22 R low980913.node1 …_split.155.sh qizhengyang 04:15:22 R low980914.node1 …_split.175.sh qizhengyang 04:04:21 R low980915.node1 …_split.172.sh qizhengyang 03:43:49 R low980916.node1 …_split.177.sh qizhengyang 03:22:15 R low980917.node1 …_split.147.sh qizhengyang 05:08:37 R low980918.node1 …_split.158.sh qizhengyang 05:14:32 R low980919.node1 …_split.176.sh qizhengyang 04:49:57 R low980920.node1 …_split.163.sh qizhengyang 04:37:11 R low980921.node1 …_split.171.sh qizhengyang 04:17:19 R low980922.node1 …_split.154.sh qizhengyang 04:05:50 R low980923.node1 …_split.148.sh qizhengyang 03:46:37 R low980925.node1 …_split.181.sh qizhengyang 03:25:10 R low980926.node1 …_split.169.sh qizhengyang 02:20:52 R low 任务完成 [Sun Feb 24 23:55:12 2019]Finished job 0.103 of 103 steps (100%) doneComplete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-23T214006.459737.snakemake.log 最终的Snakefile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186# 2019-2-19 10:28:52 part# GATK snakemake# qizhengyangfrom os.path import joinGENOME = 'genome/HWB.chromosome.fa'GTF = 'genes/HWB.gene.models.gtf'(SAMPLES,) = glob_wildcards('pairedDIR/&#123;sample&#125;_1P.fq.gz')PATTERN_R1 = join('pairedDIR', '&#123;sample&#125;_1P.fq.gz')PATTERN_R2 = join('pairedDIR', '&#123;sample&#125;_2P.fq.gz')rule all: input: 'star_index_2pass/', expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES), 'star_index_2pass/', expand('star_2pass/&#123;sample&#125;Aligned.out.sam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_rg_added_sorted.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup_split.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;.vcf', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_filtered.vcf', sample=SAMPLES)rule star_index: input: genome = GENOME, gtf = GTF output: star_index = directory('star_index/') log: 'star_index.log' threads: 20 run: # star 1-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.star_index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbGTFfile &#123;input.gtf&#125;' ' 2&gt; &#123;log&#125;')rule star_1pass_align: input: index = 'star_index/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: index = 'star_1pass/&#123;sample&#125;SJ.out.tab' threads: 20 params: prefix = './star_1pass/&#123;sample&#125;' # 在使用params之前是报错的，NameError,The name 'sample' is unknown in this context run: # star 1-pass align, OK shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule star_2pass_index: input: genome = GENOME, # 这里必需加expand，不然会报错：Wildcards in input files cannot be determined from output files: # 'sample'。 # 很奇怪，确实是需要所有样本的剪接位点信息，我之前没有注意到。。感谢报错 # 然后用--sjdbFileChrStartEnd参数将所有样品的SJ.out.tab文件作为输入的annotated junction进行第二次建index # http://www.bioinfo-scrounger.com/archives/288 # 这里不能加三个引号（"""或'''注释） splice_site = expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES) output: index = directory('star_index_2pass/') threads: 20 run: # star 2-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbFileChrStartEnd &#123;input.splice_site&#125;')rule star_2pass_align: input: index = 'star_index_2pass/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' threads: 20 params: prefix = 'star_2pass/&#123;sample&#125;' run: # star 2-pass align shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule picard: input: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' output: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' run: # RGID和RGSM的sample必须是&#123;wildcards.sample&#125;，不然 # The name 'sample' is unknown in this context. Please make sure that you defined that variable. # Also note that braces not used for variable access have to be escaped by repeating them, i.e. &#123;&#123;print $1&#125;&#125; shell('java -Xmx40g -jar $picard AddOrReplaceReadGroups' ' I=&#123;input.sam&#125;' ' O=&#123;output.bam&#125;' ' SO=coordinate' ' RGID=&#123;wildcards.sample&#125;' ' RGLB=rna' ' RGPL=illumina' ' RGPU=hiseq' ' RGSM=&#123;wildcards.sample&#125;')rule picard_markduplicates: input: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' output: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam' params: dedup_metrices = 'star_2pass/&#123;sample&#125;_dedup.metrics' run: shell('java -Xmx40g -jar $picard MarkDuplicates' ' I=&#123;input.bam&#125;' ' O=&#123;output.dedup_bam&#125;' ' CREATE_INDEX=true' ' VALIDATION_STRINGENCY=SILENT' ' M=&#123;params.dedup_metrices&#125;')rule gatk_split: input: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam', genome = GENOME output: split_bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam' run: shell('java -Xmx10g -jar $GATK -T SplitNCigarReads' ' -R &#123;input.genome&#125;' ' -I &#123;input.dedup_bam&#125;' ' -o &#123;output.split_bam&#125;' ' -rf ReassignOneMappingQuality' ' -RMQF 255' ' -RMQT 60' ' -U ALLOW_N_CIGAR_READS')rule gatk: input: bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam', genome = GENOME output: vcf = 'star_2pass/&#123;sample&#125;.vcf' run: shell('java -Xmx10g -jar $GATK -T HaplotypeCaller' ' -R &#123;input.genome&#125;' ' -I &#123;input.bam&#125;' ' -dontUseSoftClippedBases' ' -stand_call_conf 20.0' ' -o &#123;output.vcf&#125;')rule gatk_filter: input: genome = GENOME, vcf = 'star_2pass/&#123;sample&#125;.vcf' output: 'star_2pass/&#123;sample&#125;_filtered.vcf' run: shell('java -Xmx10g -jar $GATK ' ' -T VariantFiltration' ' -R &#123;input.genome&#125;' ' -V &#123;input.vcf&#125;' ' -window 35' ' -cluster 3' ' -filterName FS -filter "FS &gt; 30.0"' ' -filterName QD -filter "QD &lt; 2.0"' ' -o &#123;output&#125;') 参考资料 利用snakemake搭建流程简明教程 snakemake docs RNAseq variant calling pipeline Build bioinformatics pipelines with Snakemake kallisto-snakefiles Writing a RNA-Seq workflow with snakemake snakemake初步 RNA-seq 检测变异之 GATK 最佳实践流程 snakemake-example 我最喜欢的流程管理工具]]></content>
      <tags>
        <tag>生物信息</tag>
        <tag>snakemake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[the machine stops]]></title>
    <url>%2F2019%2F02%2F11%2Fthe-machine-stops%2F</url>
    <content type="text"><![CDATA[从工业革命开始，城市化，信息化，智能化，人们的生活发生了不可逆转的剧变。有些东西消失后就不会再出现，比如蒸汽机车，有些东西即使外界再变也不会消失，比如文明。 they have given up, to a great extent, the amenities and achievements of civilization: solitude and leisure, the sanction to be oneself, truly absorbed, whether in contemplating a work of art, a scientific theory, a sunset, or the face of one’s beloved.]]></content>
  </entry>
  <entry>
    <title><![CDATA[甲基化数据分析]]></title>
    <url>%2F2019%2F01%2F30%2F%E7%94%B2%E5%9F%BA%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[DNA甲基化数据分析流程 用Trimmomatic去除低质量序列（q&lt;20），接头。 用BSMAP比对，允许0.8的错配率。 用methratio.py提取甲基化比例，选项-r去除PCR重复。 获得DMR 为了得到可靠的DMR区域，合并两个生物学重复，仅考虑所有文库中深度至少为4的胞嘧啶。 使用200bp窗口（50bp步长）识别DMR。 对每个窗口内的甲基化和未甲基化胞嘧啶进行Fisher精确检验。使用Benjamini-Hochberg对p值进行调整，估计错误发生率（FDR）。 FDR&lt;0.01，甲基化水平变化大于1.5倍且至少含有5个差异甲基化胞嘧啶（DMCs：Fisher精确检验中p&lt;0.01）的窗口用于进一步分析，窗口在100bp内合并为更大的区域。 RNA-seq 数据分析 Trimmomatic去除低质量序列和接头 用STAR进行比对，–sjdbGTFfile 用于提供基因组注释文件 htseq-count计算每个基因map上的片段数 DESeq2计算差异表达基因 DMR-associated基因分析 DMR相关基因定义为2kb启动子区域内具有DMR的基因 仅用DMR-associated genes进行基因聚类 差异基因定义，FPKM &gt;= 1, FDR &lt;= 0.01, fc &gt;= 1.5 GO注释 使用拟南芥（TAIR10）、番茄（ITAG3）、草莓（PhytozomeV12）的蛋白序列和GO注释文件 blast 用GOATOOLs进行GO富集 小RNA分析 使用BWA比对 计算DMR的24 nt 小RNA丰度（normalized to per million per one hundred base pair）]]></content>
      <tags>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql使用过程中遇到的一些问题]]></title>
    <url>%2F2019%2F01%2F29%2FMysql%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%2F</url>
    <content type="text"><![CDATA[blast2go本地化需要用到mysql，操作系统Centos 7。 1. 卸载之前的mysql之前的版本是mysql5.6，想要更新到mysql5.7。 查看mysql安装了哪些东西。 1rpm -qa |grep -i mysql 开始卸载 1yum remove XXX 查看卸载是否完成 查找mysql相关目录 1find / -name mysql 删除相关目录 123rm -rf XXXrm -rf /etc/my.cnfrm -rf rm -rf /var/log/mysqld.log 2. 安装Mysql5.7 安装mysql源 1234wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpmyum localinstall mysql57-community-release-el7-11.noarch.rpm# 检查是否安装成功yum repolist enabled | grep &quot;mysql.*-community.*&quot; 启动mysql服务 123systemctl start mysqld# 查看状态systemctl status mysqld 这里可能会遇到 “Another mysqld server running on port 3306 error”，可以采用netstat -lp | grep 3306 查找占用这个端口号的进程，kill 这个PID。（我发现占用这个3306的是mysql??）。 修改root密码 我用root身份导入数据的时候，提示我要修改密码。生成的默认密码在 /var/log/mysqld.log 文件中。使用 grep 命令找到日志中的密码。 1grep &apos;temporary password&apos; /var/log/mysqld.log 1mysql&gt;ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123,c%vPl9ek&apos;; 或者： 123mysql&gt; use mysql;mysql&gt; update mysql.user set authentication_string=password(&apos;123,c%vPl9ek&apos;) where user=&apos;root&apos;;mysql&gt; flush privileges; 3. 导入数据1mysql -u root -p &lt; b2gdb.sql 会出现莫名其妙的错误，“ERROR 1819 (HY000) at line 4: Your password does not satisfy the current policy requirements”，但是我的密码是符合它的规则的。最后采取的办法是把检验密码的插件删了。 1mysql&gt;uninstall plugin validate_password; 有试着导入数据，这次的错误是： “ERROR 1101 (42000) at line 9: BLOB, TEXT, GEOMETRY or JSON column ‘description’ can’t have a default value” 先查看了sql_mode 12mysql&gt; select @@session.sql_mode;mysql&gt; select @@global.sql_mode; 之后重新设置sql_mode 12set sql_mode=&apos;&apos;;set global sql_mode=&apos;&apos;; 重新打开一个终端，进入mysql，查看sql_mode 1select @@global.sql_mode; 在尝试导入数据，成功。 4. blast2go123456cd data1/data/blast2go/mysql -u root -p &lt; b2gdb.sqlmysql -u root -p -e &quot;GRANT ALL ON b2gdb.* TO &apos;blast2go&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;blast4it&apos;;&quot;mysql -u root -p -e &quot;FLUSH PRIVILEGES;&quot;# 这一步很耗时nohup time mysql -s -u root -p b2gdb &lt; go_monthly-assocdb-data &gt; mysql.out 2&gt;&amp;1 &amp; 5. 参考 MySQL sql_mode 说明 blast2go本地化教程 your password does not satisfy the current policy requirements another mysqld server running on the port 3306 Centos7 完全卸载mysql CentOS 7 下 MySQL 5.7 的安装与配置]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器安装 CentOS 7 并开启ssh服务之后]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85-CentOS-7-%E5%B9%B6%E5%BC%80%E5%90%AFssh%E6%9C%8D%E5%8A%A1%E4%B9%8B%E5%90%8E%2F</url>
    <content type="text"><![CDATA[挂载服务器里的另外两块硬盘硬盘 查看没有挂载的硬盘 fdisk -l 格式化硬盘，mkfs.ext4 /dev/sdb.报错，/dev/sdb is apparently in use by the system; will not make a 文件系统 here!。用这个命令：dmsetup remove_all mkfs.ext4 /dev/sdb 挂载 mount /dev/sdb /data mount挂载ntfs 格式的移动硬盘 1234成功执行mount -t ntfs-3g /dev/sdd1 /run/media/qi/Zhangmiao_15623277907未成功执行mount -t ntfs-3g /dev/sdb5 /run/media/qi/软件 自动挂载设置 Ps:在使用gnome桌面挂载U盘的时候发现，系统可以识别NTFS 分区的存在，但是通过桌面无法自动挂载，系统会提示： Error mounting /dev/sdb1 at /run/media/lenovo/v220w: Command-line `mount -t “ntfs” -o “uhelper=udisks2,nodev,nosuid,uid=1000,gid=1000,dmask=0077,fmask=0177” “/dev/sdb1” “/run/media/lenovo/v220w”‘ exited with non-zero exit status 32: mount: unknown filesystem type ‘ntfs’ mount 提示未知的文件系统类型 ‘ntfs’ 解决办法： $ mount[Tab][Tab] #连续按两次 Tab 键作命令补齐 mount mount.glusterfs mount.nfs4 mountstatsmount.cifs mount.lowntfs-3g mount.ntfs-3gmount.fuse mount.nfs mountpoint 可以看到只有mount.ntfs-3g，在使用 mount -t 挂载ntfs 时 mount 会调用 mount.ntfs-3g 而非默认的 mount.ntfs $ locate mount.ntfs-3g #查找有关文件所在位置/usr/local/share/man/man8/mount.ntfs-3g.8/usr/sbin/mount.ntfs-3g$ sudo ln -s /usr/sbin/mount.ntfs-3g /usr/sbin/mount.ntfs #创建软链接 之后就可以自动识别U 盘而不会出现如上报错了。。。 centOS7 将“桌面”、“图片”等替换成英文 export LANG=en_US xdg-user-dirs-gtk-update 弹出配置界面，选择替换 export LANG=zh_CN.UTF-8 重启，不替换。 IP变化 wlp0s26f7u3:inet 192.168.1.102/24发现只有连同一个wifi(Magic-home)的电脑才能登入。 原来的IP：4: wlp0s26f7u3:inet 10.164.11.166]]></content>
      <tags>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R输出图形]]></title>
    <url>%2F2018%2F12%2F12%2Fexporting-nice-plots-in-r%2F</url>
    <content type="text"><![CDATA[这篇博文写的很好 https://www.r-bloggers.com/exporting-nice-plots-in-r/介绍的 Inkscape 也很好用，解决了之前IGV输出SVG图片的问题。导入PPT只显示一半。用Ai cc 打开，报错。“往返Tiny时剪贴将丢失”，只有框架没有图 There are two main problems when exporting graphics from R: Anti-aliasing is not activated in Windows R (this does not apply to Linux or Mac) – windows下没有反锯齿设置，解决：library(Cairo) When increasing the resolution the labels automatically decrease and become unreadable –分辨率升高，标签会自动缩小 If we want to increase the resolution of the plot we can’t just change the resolution parameter:We also have to change the point size, the formula is size * new resolution DPI / 72 DPI: If we double the image size we also need to double the point size: 123456789101112131415161718192021222324# https://www.r-graph-gallery.com/265-grouped-boxplot-with-ggplot2/# exporting nice plot https://www.r-bloggers.com/exporting-nice-plots-in-r/# librarylibrary(ggplot2) # create a data framevariety=rep(LETTERS[1:7], each=40)treatment=rep(c("high","low"),each=20)note=seq(1:280)+sample(1:150, 280, replace=T)data=data.frame(variety, treatment , note) # grouped boxplotggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot()# One box per treatmentggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot() + facet_wrap(~treatment)# one box per varietyggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot() + facet_wrap(~variety, scale="free")]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google news "epigenetics plants"]]></title>
    <url>%2F2018%2F12%2F10%2FGoogle-News%2F</url>
    <content type="text"><![CDATA[University of Nebraska-Lincoln researchers have found revolutionary evidence that an evolutionary phenomenon at work in complex organisms is at play in their single-celled, extreme-loving counterparts, too.]]></content>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群上安装DESeq2遇到的困难及解决办法]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85DESeq2%E9%81%87%E5%88%B0%E7%9A%84%E5%9B%B0%E9%9A%BE%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[BiocManager::install(“DESeq2”, version = “3.8”)报错，错误内容：configure: WARNING: Only g++ version 4.7.2 or greater can be used with RcppArmadillo.configure: error: Please use a different compiler.ERROR: configuration failed for package ‘RcppArmadillo’ 源码安装gcc参考../configure –prefix=/home02/qizhengyang/packages -enable-checking=release -enable-languages=c,c++ -disable-multilib报错，错误内容：/usr/bin/ld: cannot find -lgfortran 重新安装gcc，增加fortran../configure –prefix=/home02/qizhengyang/packages -enable-checking=release -enable-languages=c,c++,fortran -disable-multilib &amp;&amp; make -j4 &amp;&amp; make install 添加两个libexport LD_LIBRARY_PATH=$HOME/packages/lib64:$LD_LIBRARY_PATHexport LD_LIBRARY_PATH=$HOME/packages/lib:$LD_LIBRARY_PATH BiocManager::install(“DESeq2”, version = “3.8”)成功 备注：configure时间超级长。过程艰辛。还尝试过手动安装RcppArmadillo，失败。.libPaths()“/home02/qizhengyang/packages/R/lib64/R/library”R CMD INSTALL -l /home02/qizhengyang/packages/R/lib64/R/library RcppArmadillo_0.3.6.3.tar.gz]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群上安装的R包]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85%E7%9A%84R%E5%8C%85%2F</url>
    <content type="text"><![CDATA[写了一个R script批量安装。1234567install.packages("devtools",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")install.packages("dplyr",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")install.packages("BiocManager",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")BiocManager::install("ballgown", version = "3.8")BiocManager::install(genefilter", version = "3.8")devtools::install_github('alyssafrazee/RSkittleBrewer')BiocManager::install("methylKit", version = "3.8")]]></content>
  </entry>
  <entry>
    <title><![CDATA[集群上更新R并安装methylKit]]></title>
    <url>%2F2018%2F10%2F31%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85R%2F</url>
    <content type="text"><![CDATA[普通用户在集群上安装R主要的教程里面有一些需要注意的，安装bzip，要加入一个 -fPIC 到 CFLAG 。（在 Makefile中CFLAG=-fPIC -Wall -Winline -O2 -g…）在R安装configure之前需要注意设置环境变量123LD_LIBRARY_PATH=/home02/qizhengyang/packages/libexport CFLAGS="-I/home02/qizhengyang/packages/include"export LDFLAGS="-L/home02/qizhengyang/packages/lib" R安装完成之后123export R_HOME=$HOME/packages/Rexport R_LIBS=$HOME/packages/R/lib64/libraryexport PATH="$R_HOME/bin":$PATH 最后 source ~/.bashrc 装methyKit也是坑一堆。我下了autoconf-2.69，libxml2-2.7.2。源码安装。./configure –prefix=/home02/qizhengyang/packages]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《局外人》的主旨]]></title>
    <url>%2F2018%2F08%2F17%2F%E5%B1%80%E5%A4%96%E4%BA%BA%E4%B8%BB%E6%97%A8%2F</url>
    <content type="text"><![CDATA[1937年8月一个男人，在人们通常视为人生大事的地方（婚姻、社会地位等等）寻找人生，然后某天在翻阅一本时装目录的时候，突然了解到他对自己人生亦即时装目录上鼓吹的那种人生是何其无所谓。]]></content>
  </entry>
  <entry>
    <title><![CDATA[《非洲的青山》]]></title>
    <url>%2F2018%2F07%2F14%2F%E9%9D%9E%E6%B4%B2%E7%9A%84%E9%9D%92%E5%B1%B1%2F</url>
    <content type="text"><![CDATA[第四章，“乔伊斯中等身高，他把眼睛用坏了。在那最后一晚，喝醉了，和乔伊斯在一起，他不断引用埃德加·基内的话：‘思维清晰、生命绚丽如战争时一样。’我知道我没有把这句话彻底弄清楚。等你见到他，他会提到三年前被打断的谈话。能见到我们这个时代伟大的作家真让人高兴。”这段话越发让我觉得我看的不是一部小说。 查了维基百科 Green Hills of Africa is a 1935 work of nonfiction by American writer Ernest Hemingway.]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2018%2F07%2F13%2Fhexo%2F</url>
    <content type="text"><![CDATA[NexT教程12345hexo clean hexo g hexo s hexo d # 后面两步可合并 hexo d -g 连接Github与本地]]></content>
  </entry>
</search>
