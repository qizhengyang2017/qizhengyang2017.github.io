<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F07%2F18%2Ftest%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[DMR注释]]></title>
    <url>%2F2019%2F03%2F20%2FDMR%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[bed：chromStart从0开始 gff3：chromStart从1开始 https://cloud.tencent.com/developer/article/1078324 https://www.plob.org/article/3748.html A.bed 123456789chr1 69200 70000 TCGA-3C-AAAU-10A-01D-A41E-01 53225 0.0055chr1 95676511 95676518 TCGA-3C-AAAU-10A-01D-A41E-01 2 -1.6636chr1 95680124 167057183 TCGA-3C-AAAU-10A-01D-A41E-01 24886 0.0053chr1 167057495 167059336 TCGA-3C-AAAU-10A-01D-A41E-01 3 -1.0999chr1 167059760 181602002 TCGA-3C-AAAU-10A-01D-A41E-01 9213 -8.00E-04chr1 181603120 181609567 TCGA-3C-AAAU-10A-01D-A41E-01 6 -1.2009chr1 879511 894690 TCGA-3C-AAAU-10A-01D-A41E-01 12002 0.0055chr1 201474400 201474544 TCGA-3C-AAAU-10A-01D-A41E-01 2 -1.4235chr1 201475220 247813706 TCGA-3C-AAAU-10A-01D-A41E-01 29781 -4.00E-04 B.bed 12345678910chr1 69091 70008 OR4F5chr1 367640 368634 OR4F29chr1 621096 622034 OR4F16chr1 859308 879961 SAMD11chr1 879584 894689 NOC2Lchr1 895967 901095 KLHL17chr1 901877 911245 PLEKHN1chr1 910584 917473 PERM1chr1 934342 935552 HES4chr1 936518 949921 ISG15 1bedtools intersect -a A.bed -b B.bed -wa -wb | bedtools groupby -i - -g 1-4 -c 10 -o collapse 12chr1 69200 70000 TCGA-3C-AAAU-10A-01D-A41E-01 OR4F5chr1 879511 894690 TCGA-3C-AAAU-10A-01D-A41E-01 SAMD11,NOC2L 需要这样的一个文件 12345chr1 69091 70008 exonchr1 367640 368634 intronchr1 621096 622034 promoterchr1 859308 879961 intergenic.... 1bedtools intersect -a CpG.hyper.bed -b csi.bed -wa -wb | bedtools groupby -i - -g 1-4 -c 10 -o collapse &gt; result.bed 实践 用Galaxy 工具 gff-to-bed将gff3 文件转成bed，然后提取前四列 123456789101112[qizhengyang@node1 bedtest]$ cat Galaxy3-\[GFF-to-BED_on_data_1\].bed | cut -f1,2,3,4 &gt; csi.bed[qizhengyang@node1 bedtest]$ head csi.bedchr4 17943 22804 genechr4 17943 22804 mRNAchr4 17943 18472 five_prime_UTRchr4 17943 18544 exonchr4 18472 18544 CDSchr4 18917 19169 exonchr4 18917 19169 CDSchr4 19540 19817 exonchr4 19540 19817 CDSchr4 21013 21168 exon 处理CpG_regions_myDiff25p.hyper.txt 1234567891011qizhengyang@node1 bedtest]$ cat CpG_regions_myDiff25p.hyper.txt |cut -f1,2,3chr start endchr1 36001 36100chr1 101501 101600chr1 162601 162700chr1 386101 386200chr1 425401 425500chr1 435301 435400chr1 440601 440700chr1 440701 440800chr1 440801 440900 用bedtools工具进行注释 123456789101112[qizhengyang@node1 bedtest]$ bedtools intersect -a CpG.hyper.bed -b csi.bed -wa -wb | bedtools groupby -i - -g 1-3 -c 7 -o collapse &gt; result1.bed[qizhengyang@node1 bedtest]$ head result1.bedchr1 386101 386200 gene,mRNA,exon,three_prime_UTR,gene,mRNA,exon,three_prime_UTR,mRNA,exon,three_prime_UTR,mRNA,exon,three_prime_UTRchr1 501101 501200 gene,mRNAchr1 501201 501300 gene,mRNAchr1 501401 501500 gene,mRNAchr1 691601 691700 gene,mRNAchr1 723301 723400 gene,mRNA,exon,three_prime_UTRchr1 736701 736800 gene,mRNA,exon,three_prime_UTRchr1 758501 758600 gene,mRNA,mRNA,mRNAchr1 1643701 1643800 exon,three_prime_UTR,gene,mRNAchr1 2044401 2044500 gene,mRNA,mRNA,mRNA,mRNA bedtools merge案例三：-d 两个独立区域间距小于（等于）该值时将被合并为一个区域；-o collapse显示合并了哪些标签 https://anjingwd.github.io/AnJingwd.github.io/2017/08/19/bedtools%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E8%AF%A6%E8%A7%A3/ 1234chr1 36001 36100chr1 36101 36108chr1 162601 162700chr1 386101 386200 1bedtools merge -i test.merge -d 5 -c 1 -o count,collapse 123chr1 36001 36108 2 chr1,chr1chr1 162601 162700 1 chr1chr1 386101 386200 1 chr1 12345678# 只处理前三列，不会处理第一行bedtools merge -i CpG_regions_myDiff25p.hyper.txt -d 5 -c 1 -o count,collapse &gt; CpG.merge.hyper.txtcat CpG.merge.hyper.txt |cut -f1,2,3 &gt; CpG.merge.hyper.bed# -b不需要考虑顺序，-c 7 代表 基因特征的这一列（应该是这个程序会把a、b文件合并）， -g 应该是a文件的列数bedtools intersect -a CpG.merge.hyper.bed -b Galaxy3-[GFF-to-BED_on_data_1].bed -wa -wb | bedtools groupby -i - -g 1-3 -c 10 -o collapse | lessbedtools intersect -a CpG.merge.hyper.bed -b Galaxy4-\[GFF-to-BED_on_data_3\].bed -wa -wb | bedtools groupby -i - -g 1-3 -c 7 -o collapse &gt; result3.bed CpG_regions_myDiff25p.hyper.txt 123456chr start end strand pvalue qvalue meth.diffchr1 36001 36100 * 7.64041300115147e-07 2.95924501404506e-05 26.027397260274chr1 101501 101600 * 7.9298876929699e-11 7.72182883208429e-09 33.7349397590361chr1 162601 162700 * 4.35900104458979e-16 1.09149763553095e-13 46.6709760827408chr1 386101 386200 * 1.168443361622e-08 7.19208543362641e-07 37.7489177489177chr1 425401 425500 * 6.76995294656392e-07 2.66176753139068e-05 39.3233082706767 Galaxy3-[GFF-to-BED_on_data_1].bed 1234567891011121314151617chr4 17943 22804 gene 0 +chr4 17943 22804 mRNA 0 +chr4 17943 18472 five_prime_UTR 0 +chr4 17943 18544 exon 0 +chr4 18472 18544 CDS 0 +chr4 18917 19169 exon 0 +chr4 18917 19169 CDS 0 +chr4 19540 19817 exon 0 +chr4 19540 19817 CDS 0 +chr4 21013 21168 exon 0 +chr4 21013 21168 CDS 0 +chr4 22041 22804 exon 0 +chr4 22041 22182 CDS 0 +chr4 22182 22804 three_prime_UTR 0 +chr4 17943 22804 mRNA 0 +chr4 17943 18472 five_prime_UTR 0 +chr4 17943 18544 exon 0 + 流程：1234bedtools merge -i CpG_regions_myDiff25p.hyper.txt -d 5 -c 1 -o count,collapse &gt; CpG.merge.hyper.txt# 可以用CpG.merge.hyper.txt这个merge之后的文件，-c 9bedtools intersect -a CpG.merge.hyper.txt -b Galaxy4-\[GFF-to-BED_on_data_3\].bed -wa -wb | bedtools groupby -i - -g 1-3 -c 9 -o collapse | less 1234567891011121314dos2unix csi.promoter.txtcut csi.promoter.txt -f1,3,4 &gt; csi.promoter.bedsed -i &apos;s/$/\tpromoter/g&apos; csi.promoter.bedcut Galaxy4-\[GFF-to-BED_on_data_3\].bed -f1,2,3,4 &gt; te.bed# -v OFS 指定输出分隔符cat Galaxy3-\[GFF-to-BED_on_data_1\].bed | awk -F&quot;\t&quot; -v OFS=&quot;\t&quot; &apos;$4==&quot;gene&quot;&#123;print $1,$2,$3,$4&#125;&apos; &gt; gene.bedcat te.bed gene.bed csi.promoter.bed &gt; csi.merge.bedbedtools intersect -a CpG.merge.hyper.txt -b csi.merge.bed -wa -wb | bedtools groupby -i - -g 1-3 -c 9 -o collapse &gt; CpG.annotation.txt 然后写python脚本清理信息 123456789101112with open('DMR_annotation.txt','w') as a: with open('CpG.annotation.txt', 'r') as f: for line in f: line = line.strip().split() if 'promoter' in line[3]: line = '\t'.join(line[0:3]) + '\t' + 'promoter' + '\n' elif 'gene' in line[3]: line = '\t'.join(line[0:3]) + '\t' + 'gene' + '\n' else: line = '\t'.join(line[0:3]) + '\t' + 'TE' + '\n' print(line) a.write(line) 1234567891011chr1 36001 36100 TEchr1 101501 101600 promoterchr1 162601 162700 TEchr1 386101 386200 genechr1 435301 435400 TEchr1 440601 440900 TE]]></content>
      <categories>
        <category>毕业倒计时</category>
      </categories>
      <tags>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地化galaxy]]></title>
    <url>%2F2019%2F03%2F18%2F2019-3-18-%E6%9C%AC%E5%9C%B0%E5%8C%96galaxy%2F</url>
    <content type="text"><![CDATA[安装的原因 https://galaxyproject.github.io/training-material/topics/epigenetics/tutorials/methylation-seq/tutorial.html 安装最新的版本，第一次 sh run.sh出错。运行第二次，可以了。 centos7 本地化galaxy 要关闭防火墙 https://www.cnblogs.com/moxiaoan/p/5683743.html 服务器上localhost:8080，还是10.164.10.186:8080都能访问。我一开始想的是web服务器的问题。 移除http服务，在自己电脑上访问不了Jbrowse(10.174.10.186)，服务器上能正常访问。所以想到了可能是防火墙的问题 123# 调试 killps -ef |grep uwsgikill &lt;第二列&gt; https://docs.galaxyproject.org/en/master/admin/framework_dependencies.html#conda Conda and virtualenv are incompatible, unless an adapted virtualenv_ from the conda-forge_ channel is used. Galaxy can create a virtualenv using the adapted virtualenv package. Once a valid .venvenvironment exists it will be used. https://galaxyproject.github.io/training-material/topics/introduction/slides/introduction.html#2 https://galaxyproject.org/admin/get-galaxy/ https://github.com/galaxyproject/galaxy https://docs.galaxyproject.org/en/release_19.01/admin/config.html https://www.jianshu.com/p/a1f297eb4859 123456ps -ef | grep uwsginohup sh run.sh &amp;&gt; run.log ## 不是永久的# systemctl stop firewalld firewall-cmd --zone=public --add-port=8080/tcp --permanent firewall-cmd --reload 即使我把程序放到后台，但只要我关掉终端，网页就打不开 1nohup sh ~/software/galaxy/run.sh &amp;&gt; run.log &amp; 下次用的时候 1sh ~/software/galaxy/run.sh 用法类似于 jypyter notebook 2019-3-27 23:50:53：exit退出终端没有问题]]></content>
      <categories>
        <category>毕业倒计时</category>
      </categories>
      <tags>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dplyr的使用以及clusterProfiler使用]]></title>
    <url>%2F2019%2F03%2F17%2F2019-3-17-dplyr%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8AclusterProfiler%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[以后每周整理一次一些技术上的内容。dplyr包的使用用于整理数据用到的函数： mutate() select() gather() 将宽数据格式变为长数据格 123456789101112131415161718# 处理CpG region 文件library(dplyr)library(tidyr)setwd("C:/Users/Administrator/Desktop/")df&lt;-read.table("CpG_regions.txt",header = T)df1 &lt;- df %&gt;% mutate(W = (numCs1 + numCs2 + numCs3 + numCs4) / (coverage1 + coverage2 + coverage3 + coverage4), S = (numCs5 + numCs6 + numCs7 + numCs8) / (coverage5 + coverage6 + coverage7 + coverage8)) %&gt;% select(-(strand:numTs8))df1_long &lt;- gather(df1,samples,level,W:S)ggplot(df1_long, aes(samples,level))+geom_violin(aes(color=samples))+geom_boxplot(width=0.2,aes(color=samples))write.csv(df1,"cpg.csv",row.names = F, quote=F)# 用配对双尾t检验检测是否有差异# gather# https://www.cnblogs.com/shangfr/p/6110614.html clusterProfiler使用 12345678910111213141516171819setwd("C:/Users/Administrator/Desktop/")library(org.Atair10.eg.db)library(clusterProfiler)DEG_GENES&lt;-read.table("up_yin.txt",header = T)DEG_GENES&lt;-as.character(DEG_GENES[,1])org &lt;- org.Atair10.eg.db# OrgDb要改，keyType改成"GID"ego_up &lt;-enrichGO(gene = DEG_GENES, OrgDb = org, keyType = "GID", ont = "BP") dotplot(ego_up, showCategory=20)write.csv(ego_up, "ego_up.csv", row.names = F)# 这个结果和原来的一样，organism http://www.genome.jp/kegg/catalog/org_list.htmlkegg_up = enrichKEGG(DEG_GENES, organism="ath", pvalueCutoff=.1)write.csv(kegg_up,"kegg_up.csv",row.names = F)]]></content>
      <tags>
        <tag>代码</tag>
        <tag>毕业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何发表论文]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%A6%82%E4%BD%95%E5%8F%91%E8%A1%A8%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[表达方式，应当「SHOULD」遵循《The Element of Style》： 使段落成为文章的单元：一个段落只表达一个主题 通常在每一段落开始要点题，在段落结尾要扣题 使用主动语态 陈述句中使用肯定说法 删除不必要的词 避免连续使用松散的句子 使用相同的结构表达并列的意思 将相关的词放在一起 在总结中，要用同一种时态（这里指英文中的时态，中文不适用，所以可以不理会） 将强调的词放在句末 https://github.com/fex-team/styleguide/blob/master/markdown.md 看文献的方法：下面我想说，希望你们养成这样一个习惯，每周阅读CNS，你们知道CNS是什么意思，有人说是中枢神经系统（课堂上引起哄笑）。CNS是Cell、Nature、Science，CNS每周出一期，要养成一个习惯浏览一下目录。我经常会问自己，为什么这篇文章会发在CNS上，如果能坚持一个学期下来，你必定会有很大的收获。我这个习惯是在读研究生是养成的，每周我与几位同学一起网上浏览，这周CNS上发表了哪些文章，当然很多东西你不懂，偶尔一个东西你懂，你就会问它为什么可以发表在Nature、Science上，久而久之，你就养成两个习惯，一个是批判精神，很多已经发表在Nature、Science上的文章都有漏洞，所以要学会批判。过去我念书的时候，老师讲要是能找到论文的错误就及格了。有一次生化考试，内容是对一篇发表在Scientific American杂志上的一篇文章找错误，Scientific American的杂志往往是一个非常成熟的工作，一位非常有名的科学家写在这个杂志上的文章，我们的考试是对发表在这个杂志上的一篇综述找错，这篇文章写错了，叫大家找。结果大家傻了，这有名的大杂志怎么会有错误，所以就是要有批判精神。另一种你要养成欣赏别人做的好的工作，如到音乐厅去听音乐或到展览馆去欣赏艺术作品一样。其实做科学是很有趣的，政府出钱给你玩，你如同每天在听音乐，每天在看电影一样，哪有什么不开心的。 在座的各位都是研究生，通过这堂课，要逐渐地培养自己对科学的欣赏能力。现在谈一谈怎样阅读科学文章。有谁能告诉我，你们是怎么读文章的（课堂上，有的说，先读题目和摘要；有的说先看材料和方法，再看结果；有的说先看结果），我在读研究生一年级的时候，一周上两堂分子生物学、两堂生化课。老师在每次讲课时都给我们开一个十几篇文章的目录，这些文章并不是短文章，每堂课前，我们先把文章拷贝好，然后用荧光笔把重要的内容画好。给我们上课的教授，教我们一个办法，是哈佛大学Walter Gilbert发明的，Gilbert因发明DNA序列测试法，得过诺贝尔奖。他在给本科生讲课时说，你们是怎样读Paper的，会读Paper是你一辈子受用的事情，这个Paper不能够先看序言、方法，然后结果，既花时间又给它牵着鼻子走，这是很可怕的。写文章的人就是要牵着你的鼻子看他的文章，让你去相信他写的东西，但你先不要去相信他，拿到文章后，先看题目、摘要，然后就去看图1，因为结果都在图里，不要借助任何帮助的情况下，先给图1下一个你自己的结论，然后再看图2，也下个结论，等到你把几个图都看完以后，在下个结论，然后去看文章，结果会发现你的结论经常与作者的结论不一样，两个不一样的结论，那总有一个是错的，这时再去看文章，是你的错，还是他的错，也许你没有看清楚，那是你的错，也许是他的解释错。这样的话，你的收获是很大的，而且速度也很快，不会被他牵着鼻子走。我评审文章一般不看讨论，因为有误导现象，专门领着你去上他的圈套，所以看Paper，首先了解他要探讨怎样一个问题，这是一个大框架，以及问题的重要性，假如是个吹毛求疵的小问题，Who cares。然后用实验的方法能否回答这个问题，如一个分子在细胞里是怎么动的，用光学显微镜是没法看的，这肯定是胡说八道。一篇文章一定有它的闪光的思想，如在一篇文章中有出人意料的新观点，而且新观点是否有实验结果来支持，如果实验结果没有支持新的观点，那样的文章要被拒绝。但是很多这样的文章也照样发表，所以你自己不一定要相信作者的观点，自己要有自己的想法。 我在斯坦福大学有位熟人，我知道我的很多文章在他手里败下阵来。我问他，你是怎样审文章的，他告诉我，他评审有一个基本原则，文章到手，第一反应是拒绝，然后再到文章中去找，找到足够的证据来改变我的拒绝，我才考虑去接受，所以你的实验结果一定要用多种方法、从多种角度，反反复复来证明。这是对得起别人，也对得起你自己。不要过两天人家认为你的文章是错的，你会没面子的，所以结果一定要非常可靠。 —— 来自微信]]></content>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[做海报也不容易]]></title>
    <url>%2F2019%2F03%2F11%2F2019-3-11-%E5%81%9A%E6%B5%B7%E6%8A%A5%E4%B9%9F%E4%B8%8D%E5%AE%B9%E6%98%93%2F</url>
    <content type="text"><![CDATA[第一天日期不对，”plant”莫名其妙变成”pant” 第二天干吗在后面加了个“博士”，人家只是在读博士 要跟审核的人讲清楚改了哪些地方]]></content>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[set up a BAM file]]></title>
    <url>%2F2019%2F03%2F05%2Fset-up-a-BAM-file%2F</url>
    <content type="text"><![CDATA[Jbrowse：diagnostic message object response error 这种错误应该就是bam文件没有排序，或者没有建立索引。 貌似samtools sort 输出文件不能和输入文件一样，不然就没排序？这导致了后面的麻烦 2019-3-5 23:45:23：排完序之后会有输出 xx merge 受了下面内容的影响，以为是web 服务器问题 Apache Configuration NoteIf you are using the Apache web server, please be aware that the module mime_magic can cause BAM files to be served incorrectly. Usually, the error in the web developer console will be something like “Not a BAM file”. Some packaged versions of Apache, particularly on Red Hat or CentOS-based systems, are configured with this module turned on by default. We recommend you deactivate this Apache module for the server or directory used to serve JBrowse files. If you do not want to deactivate this module for the entire server, try adding this line to your HTTPD config or .htaccess file: 1AddType application/octet-stream .bam .bami .bai 123456# 停止、开启服务，systemctl stop nginx.servicesystemctl start httpd.servicenetstat -ntpl/usr/bin/perl ../bin/add-bam-track.pl --label jhhbam --bam_url jhh.bam --in trackList.json http://10.164.6.4/JBrowse-1.16.3 正确的操作How do I set up a BAM file?有bam、bigwig的perl脚本生成json格式的配置文件 When you set up a BAM file in jbrowse, the best way to do it is as follows Put the BAM file and the BAM index (.bai) in your data directory (e.g. you downloaded jbrowse, and your did prepare-refseqs.pl, which created a data subfolder. Put your BAM file in there) Then use add-bam-track.pl like this: add-bam-track.pl –label mybam –bam_url mybam.bam –in data/trackList.json This will only create a simple section of configuration in your data directory’s trackList.json for the BAM file that will look like 1234567&#123; &quot;storeClass&quot; : &quot;JBrowse/Store/SeqFeature/BAM&quot;, &quot;urlTemplate&quot; : &quot;mybam.bam&quot;, &quot;label&quot; : &quot;mybam&quot;, &quot;type&quot; : &quot;JBrowse/View/Track/Alignments2&quot;, &quot;key&quot; : &quot;mybam&quot;&#125; This basically just instructs JBrowse to fetch “mybam.bam” using the class that understands BAM data, “JBrowse/Store/SeqFeature/BAM”, and display it with the “Alignments2” tracktype. If this does not work please post to github or gmod-ajax@lists.sourceforge.net Other notes Don’t use bam-to-json.pl, it is old and converting your probably humongous next-gen-sequencing BAM into text json is unwieldy Your bam index should just be named the same as your BAM with .bai on the end The add-bam-track.pl does NOT copy the bam to your data directory for you, you put it there yourself, and specify the –bam_url appropriately baiUrlTemplate is unnecessary if your bai file is just your bam file with .bai added onto the end, in that case it is found automatically 排序 samtools使用 123samtools view KL_3d_clean_filter.xRNAs.bam | lesssamtools sort KL_3d_clean_filter.xRNAs.bam -o KL_3d_clean_filter.xRNAs.sorted.bam samtools index KL_3d_clean_filter.xRNAs.sorted.bam bam生成配置文件12# 对样品生成 json 文件/usr/bin/perl ../bin/add-bam-track.pl --label mybam --bam_url HB-12-2_dedup.bam --in trackList.json 1234567# 查看帮助，有各项参数的意思/usr/bin/perl bin/prepare-refseqs.pl --help# 添加key,即track显示的名称/usr/bin/perl bin/prepare-refseqs.pl --fasta /home/qi/JHH/siRNA/Cclementina_182_v1.fa --out Ccle --key Ccle.fasta/usr/bin/perl bin/flatfile-to-json.pl --gff /home/qi/JHH/siRNA/Cclementina_v1.0_gene_no_Cc_182.gff3 --trackType CanvasFeatures --trackLabel Ccle.gff3 --out Ccle 发现了 Cclementina Jbrowse download fasta –out Data directory to process. Default ‘data/‘. 123# 建立索引，搜索/usr/bin/perl bin/generate-names.pl --help/usr/bin/perl bin/generate-names.pl --out Ccle http://10.164.6.4/index.html?data=Ccle 12# 生成bam的配置文件。配置文件中的路径只能写相对路径。要把文件移到相应的文件夹下for i in *.sorted.bam;do t=`echo $i | cut -d _ -f 1,2`;/usr/bin/perl ../bin/add-bam-track.pl --label $t --bam_url $i --in trackList.json; done wig to bigwig 1conda install -c bioconda ucsc-wigtobigwig 现在我用conda安装软件，总是会碰到perssion denied的问题。明天再搞了。 (base) [qi@localhost Ccle]$ ls ../binadd-bam-track.pl add-json.pl biodb-to-json.pl draw-basepair-track.pl generate-names.pl json2conf.pl new-plugin.pl remove-track.pladd-bw-track.pl add-track-json.pl cpanm flatfile-to-json.pl jbdoc maker2jbrowse prepare-refseqs.pl ucsc-to-json.pl]]></content>
      <tags>
        <tag>Jbrowse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo next 主题 seo]]></title>
    <url>%2F2019%2F03%2F05%2F2019-3-5-hexo-next-%E4%B8%BB%E9%A2%98-seo%2F</url>
    <content type="text"><![CDATA[小结 seo 确实起作用 抓取用的旧版 robots.txt这个文件，经过测试，有无决定了seo是否起作用（2019-3-5 17:20:21：我也不确定，也许是写入索引需要一点时间）。（根目录下必须放，但第一次提交会报错，此时，将内容复制进去，点击第二个选项，再点击第三个。如果事先没有放，会报错，错误404） hexo clean 会把robots.txt清理掉 hexo d 不会重新提交robots.txt 抓取此网址及其直接链接？（前者可以月提交500次，后者月之能提交10次，区别就是点后者提交能一并提交他的子目录。） Google抓取 添加属性并输入你的网站 下载HTML文件并放入网站文件中使其可以访问 robots.txt测试工具。首先我们需要将网站地图robots.txt加入网站的根目录文件中。它会告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。 站点地图，我们进入站点地图，全选页面然后进行提交。 *这里需要注意的是，我们在修改网站信息后，站点地图的内容不会自动进行更新。所以养成良好的习惯，定期在网站更新后对站点地图进行重新提交，有助于你的网站快速被收录。 google抓取工具。该工具可以帮助你了解谷歌的网页抓取机器人是否有访问你网站中的网页，呈现出来的样子，及是否有被禁止的网页。 输入链接后点击抓取预览，请求编入索引 选择第二个抓取此网址及其直接链接 不明白的地方：搜索界面的时间。可能是编入索引这一步的问题。暂时这样吧。 第一次抓取 第二次抓取 抓取此网址及其直接链接起作用了 一开始，搜索结果又没有我的信息了。 我听完报告回来，一切都正常了。连我的Twitter都有了，seo真实博大精深，没有精力深究了（可能是之前我在站点或主题的配置文件添加了社交网站，搜了一下并没有），我也很奇怪。时间是对的。 听报告送书啦《作物功能基因组学》 听报告的时候感觉ipad拍照像素很低 2019-3-5 17:36:19 又试一次，twitter没有了 怪了！ 参考 https://www.cmwonderland.com/2018/05/05/search-engine/ https://hoxis.github.io/Hexo+Next%20SEO%E4%BC%98%E5%8C%96.html https://www.wordpresshy.com/20029 https://www.inkwp.com/crawl-or-remove-website-content-immediately/]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[盖茨思考的东西]]></title>
    <url>%2F2019%2F03%2F05%2F2019-3-5-%E7%9B%96%E8%8C%A8%E6%80%9D%E8%80%83%E7%9A%84%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[《麻省理工科技评论》 盖茨列出 10 项改变世界的技术名单，大部分你也猜得到 但它和 2018 年的名单有些不一样，因为盖茨也在成长。 This year’s 10 Breakthrough Technologies fall into roughly three groups: climate change mitigation, healthcare, and AI. They are: Robot dexterity—robot hands that can learn to manipulate unfamiliar objects on their own New-wave nuclear power—both fission and fusion reactor designs that could help bring down carbon emissions Predicting preemies—a simple blood test to warn of a preterm birth, potentially saving many children’s lives Gut probe in a pill—a swallowable device that can image the digestive tract and even perform biopsies Custom cancer vaccines—a treatment that uses the body’s own immune system to target only tumor cells The cow-free burger—both plant-based and lab-grown meat alternatives that could drastically cut emissions from the food industry Carbon dioxide catcher—techniques for absorbing CO2 from the air and locking it away that may finally become economic An ECG on your wrist—the ability for people with heart conditions to continuously monitor their health and get early warnings of problems Sanitation without sewers—a self-contained toilet that could tackle disease and unpleasant living conditions in much of the developing world Smooth-talking AI assistants—new advances in natural language processing that make digital assistants capable of greater autonomy 2017 年，盖茨投资了美国人造肉食品初创公司 Memphis Meats，更早前还投资过同类公司 Impossible Foods 。人造肉的意义在于，与传统畜牧业相比，它的肉类生产过程能够使用了传统畜牧业所需大约 1% 的土地和 10% 的水，减少对生态的损耗。^1]]></content>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览了好奇心日报]]></title>
    <url>%2F2019%2F03%2F04%2F2019-3-4-%E6%B5%8F%E8%A7%88%E4%BA%86%E5%A5%BD%E5%A5%87%E5%BF%83%E6%97%A5%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[在一轮招聘热潮之前，WeWork裁减了大约300名员工 诞生 100 年后，塞林格还在守望我们对青春与童年的认知 1919 年元旦，塞林格出生于曼哈顿，是一个四口之家的小儿子。从一所中学退学后，他被送往福吉谷军事学院，也就是小说中赶走霍尔顿的学校的原型。1937 年至 1941 年，塞林格没有接受过什么正规教育。他随父亲在奥地利和波兰学着做生意，在德军占领前一周回到美国，上过夜校的写作课，试着给文艺界富有盛名的《纽约客》投稿。 混乱于 1941 年结束，塞林格应征入伍，前往欧洲战场从事情报工作。1945 年，他因战争带来的精神问题入院。战后，他搬回纽约的父母家，继续写作。 《麦田里的守望者》塑造了一个可爱的废物，但没有人会否认，这部小说为塞林格本人在文学界赢得了一席之地。这个地位之重要超出了当时人们的预期。包括厄普代克（John Updike）等文学偶像在内，同时代和晚辈的美国作家都深受塞林格的影响，至今仍活在“霍尔顿”不散的阴魂之下。]]></content>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gmail 使用]]></title>
    <url>%2F2019%2F03%2F04%2F2019-3-4-Gmail-%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[诺禾致源2018-12-1的邮件在gmail中消失了。是因为我将它转发到了outlook? gmail 搜索 Gmail 不会将“垃圾邮件”或“已删除邮件”中的邮件显示在搜索结果中。 搜索“垃圾邮件”和“已删除邮件” 如果要搜索，点击所有邮件下拉菜单，然后选择邮件、垃圾邮件和已删除邮件。 在 Gmail 中进行搜索 Gmail 现在已经有搜索附件的功能了]]></content>
      <tags>
        <tag>gmail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网页不能登入检查]]></title>
    <url>%2F2019%2F03%2F03%2F2019-3-3-%E7%BD%91%E9%A1%B5%E4%B8%8D%E8%83%BD%E7%99%BB%E5%85%A5%E6%A3%80%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[停电，IP又变了。http://10.164.6.4/ 下面的步骤，全要做一遍，才能登入网页。 12345678910111213# 用root查看，才会显示进程信息。查看80端口是否被监听。netstat -ntpl# 停止apache服务sudo systemctl stop httpd.service# 启动nginx服务systemctl start nginx.service# 必须做sudo firewall-cmd --permanent --zone=public --add-service=httpsudo firewall-cmd --permanent --zone=public --add-service=httpssudo firewall-cmd --reload 如何查看防火墙是否允许http 1234firewall-cmd --query-service http ##查看http服务是否支持，返回yes或者nofirewall-cmd --add-service=http ##临时开放http服务firewall-cmd --add-service=http --permanent ##永久开放http服务firewall-cmd --reload ##重启防火墙生效 Centos7 yum配置Nginx CentOS 7.4 firewall 开启http/https 服务]]></content>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docear 使用心得]]></title>
    <url>%2F2019%2F03%2F02%2F2019-3-3-docear-%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>学术</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[需要比对的结果，在snakemake脚本里添加log]]></title>
    <url>%2F2019%2F03%2F02%2F%E5%9C%A8snakemake%E8%84%9A%E6%9C%AC%E9%87%8C%E6%B7%BB%E5%8A%A0log%2F</url>
    <content type="text"><![CDATA[log名要添加{sample}，不然出现如下错误 1234[qizhengyang@node1 gatk_snakemake]$ snakemake -n --quietSyntaxError:Not all output, log and benchmark files of rule star_1pass_align contain the same wildcards. This is crucial though, in order to avoid that two or more jobs write to the same file. File &quot;/home02/qizhengyang/qizhengyang/gatk_snakemake/Snakefile&quot;, line 63, in &lt;module&gt; 开始跑，大概 2019-3-2 21:10123snakemake -n --quietpestatnohup snakemake --cluster &quot;qsub -q low&quot; --jobs 100 &amp;&gt; snakemake.out &amp; 2019-3-2 23:52:22 一切正常 打算关闭终端，查看用： 1ps aux|grep Snakefile|grep -v grep 2019-3-3 15:34:06 1234150 of 255 steps (59%) doneShutting down, this might take some time.Exiting because a job execution failed. Look above for error messageComplete log: /home02/qizhengyang/qizhengyang/gatk_snakemake/.snakemake/log/2019-03-02T210938.398049.snakemake.log error picard_markduplicates 123456OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00002b6141200000, 1882193920, 0) failed; error=&apos;无法分配内存&apos; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 1882193920 bytes for committing reserved memory.# An error report file with more information is saved as:# /home02/qizhengyang/qizhengyang/gatk_snakemake/hs_err_pid14847.log 12snakemake -n --quietnohup snakemake --cluster &quot;qsub -q low&quot; --jobs 100 &amp;&gt;&gt; snakemake.out &amp; 2019-3-5 11:37:02 用时： 开始：[Sat Mar 2 21:09:38 2019] 结束：[Sun Mar 3 08:21:52 2019] 中间出错过一次，java -Xmx40g -jar $picard MarkDuplicates，下次设置JVM最大可用内存为80g 开始：[Sun Mar 3 16:11:13 2019] 结束：[Mon Mar 4 22:12:01 2019] 总用时 ~41 小时 附运行代码 添加log没有用，比对信息有输出 HB-10-1Log.final.out]]></content>
      <tags>
        <tag>snakemake</tag>
        <tag>gatk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群和Linux相关]]></title>
    <url>%2F2019%2F03%2F02%2F2019-3-2-%E9%9B%86%E7%BE%A4%E5%92%8CLinux%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[PBS^usagePBS是公开源代码的作业管理系统，在此环境下运行，用户不需要指定程序在哪些节点上运行，程序所需的硬件资源由PBS管理和分配。 PBS(Portable Batch System)是由NASA开发的灵活的批处理系统。它被用于集群系统、超级计算机和大规模并行系统。PBS主要有如下特征： 易用性：为所有的资源提供统一的接口，易于配置以满足不同系统的需求，灵活的作业调度器允许不同系统采用自己的调度策略。 移植性：符合POSIX 1003.2标准，可以用于shell和批处理等各种环境。 适配性：可以适配与各种管理策略，并提供可扩展的认证和安全模型。支持广域网上的负载的动态分发和建立在多个物理位置不同的实体上的虚拟组织。 灵活性：支持交互和批处理作业。 常用命令Linux之三剑客，awk、sed、grep的用法^editawk1234567891011121314eth0 Link encap:Ethernet HWaddr 00:0C:29:18:4C:35 inet addr:192.168.75.130 Bcast:192.168.75.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe18:4c35/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:1322 errors:0 dropped:0 overruns:0 frame:0 TX packets:1093 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:147531 (144.0 KiB) TX bytes:134582 (131.4 KiB)ifconfig eth0 |grep &apos;inet addr&apos; |awk -F &quot;:&quot; &apos;&#123;print $2&#125;&apos; 192.168.75.130 Bcast#可以看到后面多出来了一个Bcarst,可以打印出第一列ifconfig eth0 |grep &apos;inet addr&apos; |awk -F &quot;:&quot; &apos;&#123;print $2&#125;&apos;|awk &apos;&#123;print $1&#125;&apos;192.168.75.130 sedstream editor 流编辑器 sed 从输入读取一行，将之拷贝到一个编辑缓冲区，按指定的sed命令编辑完之后，将其发送到屏幕，然后将这行删除，读取下一行。（注意，输出的是编辑之后的结果，如果是删除，输出中部包含删除的结果。重定向之后，部打印到屏幕上。） 123456789101112131415#语法结构sed [options] ‘[command]’ filename # 输出前三行以外的内容，-n flush output on every line，每行刷新输出，默认sed -n '1,3!p' passwd# 把文件第三行替换成“bbb”sed '3cbbb' b.txt # 删除空行 将匹配的行记录到新的文件中sed '/^$/d' passwd &gt; c.txt# 把passwd中包含FORMAT=的记录（行）写入新的文件中sed '/FORMAT=/w newpasswd' passwd# sed 的-i选项可以直接修改文件中的内容sed -i 's/root/rm/' passwd grepLinux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 要用好grep这个工具，其实就是要写好正则表达式，所以这里不对grep的所有功能进行实例讲解，只列几个例子，讲解一个正则表达式的写法。 pattern 可以是正则表达式（用单引号）、或字符串（双引号）、或一个单词（没有引号） 12345678910 ls -l | grep \'^a\'#通过管道过滤ls -l输出的内容，只显示以a开头的行。 grep \'test\' d* #显示所有以d开头的文件中包含test的行。 grep \'test\' aa bb cc #显示在aa，bb，cc文件中匹配test的行。 grep \'[a-z]&#123;5&#125;\' aa #显示所有包含每个字符串至少有5个连续小写字符的字符串的行。 grep \'w(es)t.*1\' aa #如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符（.*），这些字符后面紧跟着另外一个es（1），找到就显示该行。如果用egrep或grep -E，就不用""号进行转义，直接写成\'w(es)t.*1\'就可以了。 12 ls -thl | grep &apos;snakejob.star_2pass_align.*o.*&apos; | awk &apos;&#123;print $7,$8,$9&#125;&apos; | head -n 36# 一个实例，文件排序 其他pestat 节点状态： Excl 所有CPU资源已被占用； Busy CPU已接近满负荷运行； free 全部或部分CPU空闲； offl 管理员手动指定离线状态； 此外，还可以使用另外一个命令pnodes来查看每个节点被占用的cpu核心数，用户可以根据剩余cpu资源合理地指定自己作业中使用的cpu核心数，以免作业处于长期等待状态。该命令有个缺点，就是运行时间比较长，大概要3s才出结果，其原因是这套系统节点数太多，命令执行时间太长。^keylabwiki 1qsub -I -l nodes=node18 # 以交互方式运行 12345678910#压缩tar -czvf ***.tar.gztar -cjvf ***.tar.bz2#解压缩tar -xzvf ***.tar.gztar -xjvf ***.tar.bz2# du，disk usage# df，disk freedu -sh *** # 某个目录中的所有文件占了多少磁盘空间 1./configure --prefix=/opt/software # 利用软件编译安装软件，指定目录 MPIMPI是一种用于节点间通信的方法。^wikipedia 我觉得「消息传递接口」（Message Passing Interface, MPI）就是这样一项技术，而且学习它确实可以让你的并行编程知识变得更深厚。^tutorials makefile简介Makefile文件 软件程序的管理工具 定义规则，实现自动化编译 处理源代码、目标文件、头文件、库文件等依赖关系 根据规则和依赖关系，结合时间戳实现精细化控制 make命令 make命令执行 Makefile 中的定义的编译流程 make命令默认读取当前目录 Makefile 或 makefile 文件，也可用 -f 参数指定 Makefile 文件]]></content>
      <tags>
        <tag>linux</tag>
        <tag>PBS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用 python生成json格式的配置文件]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%94%A8python%E7%94%9F%E6%88%90json%E6%A0%BC%E5%BC%8F%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[python 基础知识writelines 和 write 区别： write()需要传入一个字符串做为参数，否则会报错 writelines()既可以传入字符串又可以传入一个字符序列，并将该字符序列写入文件 注意必须传入的是字符序列，不能是数字序列(卡在这里搞了半天) lambda函数 lambda作为一个表达式，定义了一个匿名函数 12345678910&gt;&gt;&gt; foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]&gt;&gt;&gt;&gt;&gt;&gt; print filter(lambda x: x % 3 == 0, foo)[18, 9, 24, 12, 27]&gt;&gt;&gt;&gt;&gt;&gt; print map(lambda x: x * 2 + 10, foo)[14, 46, 28, 54, 44, 58, 26, 34, 64]&gt;&gt;&gt;&gt;&gt;&gt; print reduce(lambda x, y: x + y, foo)139 列表生成式 列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in &apos;ABC&apos; for n in &apos;XYZ&apos;][&apos;AX&apos;, &apos;AY&apos;, &apos;AZ&apos;, &apos;BX&apos;, &apos;BY&apos;, &apos;BZ&apos;, &apos;CX&apos;, &apos;CY&apos;, &apos;CZ&apos;] 任务用文件名（存在列表当值），生成json配置文件 1234567&#123; &quot;label&quot; : &quot;sample&quot;, &quot;key&quot; : &quot;sample&quot;, &quot;storeClass&quot; : &quot;JBrowse/Store/SeqFeature/VCFTabix&quot;, &quot;urlTemplate&quot; : &quot;sample.vcf.gz&quot;, &quot;type&quot; : &quot;JBrowse/View/Track/HTMLVariants&quot;&#125; json 模块 示例： 123456&gt;&gt;&gt; import json&gt;&gt;&gt; print(json.dumps(&#123;&apos;4&apos;: 5, &apos;6&apos;: 7&#125;, sort_keys=True, indent=4))&#123; &quot;4&quot;: 5, &quot;6&quot;: 7&#125; 原创脚本 1234567891011121314151617181920212223242526#!/usr/bin/env python3'''Make a samples.json file with sample names and file names.'''import jsonfrom glob import glob# match filenamesfastqs = glob('/usr/share/nginx/html/vcf/*.vcf.gz')files = &#123;&#125;# extract a sample name from each filename.samples = [fastq.split('/')[-1].split('_')[0] for fastq in fastqs]samples1 = sorted(samples)for sample in samples1: files['label'] = sample files['key'] = sample files['storeClass'] = 'JBrowse/Store/SeqFeature/VCFTabix' files['urlTemplate'] = 'vcf/' + sample + '_filtered.vcf.gz' files['category'] = 'VCF' files['type'] = 'JBrowse/View/Track/HTMLVariants' js = json.dumps(files, indent=4, sort_keys=True) js1 = js + ',' + '\n' open('mysamples1.json', 'a').writelines(js1) 输出 12345678910111213141516&#123; &quot;category&quot;: &quot;VCF&quot;, &quot;key&quot;: &quot;HB-10-1&quot;, &quot;label&quot;: &quot;HB-10-1&quot;, &quot;storeClass&quot;: &quot;JBrowse/Store/SeqFeature/VCFTabix&quot;, &quot;type&quot;: &quot;JBrowse/View/Track/HTMLVariants&quot;, &quot;urlTemplate&quot;: &quot;vcf/HB-10-1_filtered.vcf.gz&quot;&#125;,&#123; &quot;category&quot;: &quot;VCF&quot;, &quot;key&quot;: &quot;HB-10-2&quot;, &quot;label&quot;: &quot;HB-10-2&quot;, &quot;storeClass&quot;: &quot;JBrowse/Store/SeqFeature/VCFTabix&quot;, &quot;type&quot;: &quot;JBrowse/View/Track/HTMLVariants&quot;, &quot;urlTemplate&quot;: &quot;vcf/HB-10-2_filtered.vcf.gz&quot;&#125;, JBrowse 结果 其他安装制定版本为3.6，为什么python的版本是3.5.5 123(py3.6) [qi@localhost ~]$ python --versionPython 3.5.5(py3.6) [qi@localhost ~]$ 1conda install -c anaconda ipython # su 目录高亮是怎么回事？ 说明这个目录权限太高了，所有人都能够访问、修改、删除、运行。正常情况，应该是Owner可以读写，all可读，root可以修改可执行权限才对。正常情况，对于目录，应该权限设定为744，对于文件默认权限应该设定为644。用户私有文件，权限应该设定为600。 更改文件拥有者 12ls -ld html/chown -R qi:qi html/ py3.6的环境，我安装了samtools，也用不了。自然也就用不了，bgzip和tabix。 123(py3.6) [qi@localhost vcf]$ conda list | grep samtoolsperl-bio-samtools 1.43 pl526h1341992_1 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/biocondasamtools 1.9 h57cc563_6 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda]]></content>
      <tags>
        <tag>jbrowse</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JBrowse 总结]]></title>
    <url>%2F2019%2F03%2F01%2FJBrowse%2F</url>
    <content type="text"><![CDATA[当没有得到结果的时候，总是战战兢兢。得到结果之后，逻辑一下子就很清晰。 下载JBrowse最新版本，我用的是 1.16.3 web服务器，用Nginx。（用 Apache，建立 VCF track 会出错）^question 安装 JBrowse./setup.sh这一步需要安装很多perl依赖库，如果报错，查看setup.log文件，应该是一些 perl 模块没有安装。如： ! Installing GD failed. See /root/.cpanm/work/1551106022.8560/build.log for details. Retry with –force to force install it.Searching XML::LibXML (0) on cpanmetadb …Already tried XML-LibXML-2.0134. Skipping.! Installing the dependencies failed: Module ‘XML::LibXML’ is not installed, Module ‘XML::LibXML::Reader’ is not installed, Module ‘GD’ is not installed, Module ‘DBI’ is not installed, Module ‘DB_File’ is not installed! Bailing out the installation for BioPerl-1.7.5.Searching JSON::XS (0) on cpanmetadb …Unpacking JSON-XS-4.01.tar.gzFAIL 官方文档说，运行./setup.sh 的时候不要是 root 或使用 sudo。（和安装目录有关？）这可能是之前安装失败的原因。 Run the automated-setup script, ./setup.sh, which will attempt to install all of JBrowse’s (modest) prerequisites for you in the jbrowse/ directory itself. Note that setup.sh should not be run as root or with sudo.^1 安装Nginxyum install nginx 123sudo systemctl stop httpd.service # 因为之前有apache服务器，所以要先停掉sudo yum install nginx # 安装systemctl start nginx.service # 启动 如果你的服务器正在运行防火墙，请运行下列命令以允许它进行 HTTP 和 HTTPS 通信： https://www.jianshu.com/p/a482a0f8adfd 123sudo firewall-cmd --permanent --zone=public --add-service=httpsudo firewall-cmd --permanent --zone=public --add-service=httpssudo firewall-cmd --reload 两个 perl 版本的问题，从报错信息知 JBrowse 用的是 perl 5.16.3，即 /usr/bin/perl。系统默认的 perl 是 conda 安装的 perl。 Can’t locate local/lib.pm in @INC (@INC contains: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3 /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5 /var/www/html/JBrowse-1.16.3/bin/../src/perl5 /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JBlibs.pm line 25.Compilation failed in require at bin/prepare-refseqs.pl line 5.BEGIN failed–compilation aborted at bin/prepare-refseqs.pl line 5 解决上面的问题，可以直接copy 一份。（不知道家目录下的 perl5 文件夹是什么时候生成的？） 1234567locate local/lib.pmcp -r /home/qi/perl5/lib/perl5/local /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3# 运行示例sudo bin/prepare-refseqs.pl --fasta docs/tutorial/data_files/volvox.fa --out sample_data/json/volvoxsudo bin/prepare-refseqs.pl --fasta sample_data/raw/yeast_scaffolds/chr1.fa.gz --fasta sample_data/raw/yeast_scaffolds/chr2.fa.gzip --out sample_data/json/yeast/ 柚子数据准备参考序列和特征数据所有track都生成在默认的data目录中 123456789# 指定perl为/usr/bin/perl # seq trackList.json tracks.conf/usr/bin/perl bin/prepare-refseqs.pl --fasta ~/data2/tan_rna/genome/HWB.chromosome.fa# tracks/usr/bin/perl bin/flatfile-to-json.pl --gff HWB.gene.models.gff3 --trackType CanvasFeatures --trackLabel HWBgff# 建索引。生成 ./data/names/usr/bin/perl bin/generate-names.pl 不指定会出现如下错误： (base) [qi@localhost JBrowse-1.16.3]$ bin/flatfile-to-json.pl –helpperl: symbol lookup error: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi/auto/JSON/XS/XS.so: undefined symbol: Perl_xs_apiversion_bootcheck 查资料 This type of error is almost always indicates you are loading a module that was installed using a different build of Perl.^stackoverflow VCF trackshttp://jbrowse.org/docs/variants.html VCF files used with the VCFTabix must be compressed with bgzip and indexed with tabix, both of which are part of the samtools package. This is usually done with commands like: 12for i in *.vcf; do bgzip $i; donefor i in *.vcf.gz; do tabix -p vcf $i; done 安装好之后，我们将JBrowse-1.12.1下所有文件拷贝到网站根目录 1sudo cp * /usr/share/nginx/html http://10.164.6.154 要添加其他物种，可重命名data文件夹 1$ mv data xxx 下次再把另外一个基因组放进来时，bin目录下的脚本又会默认放到data目录，就不会发生混乱啦！ 另外，如果你把JBrowse部署在内网，却想让外网访问到，而部署JBrowse的服务器又在防火墙后面的话，请一定要记得把jbrowse.conf加入到防火墙白名单，因为防火墙默认会把这种.conf结尾的文件屏蔽。^引自 所谓基因组浏览器就是通过这个工具查看基因组，具体包括参考基因组序列，哪个地方是外显子、那个地方是内含子等等功能。参考基因组就是一个fasta文件，哪个地方是外显子、哪个地方是内含子这些信息称之为特征，一般情况下NCBI、ENSEMBL等数据库都会提供GFF3格式的基因组特征文件。JBrowse支持GFF3, BED, FASTA, Wiggle, BigWig, BAM, VCF (with tabix), REST等众多数据格式，BAM, BigWig，VCF格式的数据可以不需要转换而直接使用。 其他相关的命令一些关于防火墙的命令 1234firewall-cmd --state # 查看状态，runningfirewall-cmd --reload # 重启firewallsystemctl stop firewalld.service # 停止firewallsystemctl disable firewalld.service # 禁止firewall开机启动 查看历史记录 12# awk去掉第一列，sed去掉行首空格history | tail -n 40 | awk '&#123;$1="";print $0&#125;' | sed 's/^ *//' conda 命令 1234567891011121314conda create --name bioinfo python=3.6 # 创建环境conda list # 查看包conda update --all # 更新所有包# 查看环境conda info -econda activate bioinfoconda deactivate# ERROR: This cross-compiler package contains no program # https://github.com/conda/conda/issues/6600conda list | grep gcc# 需要超级用户权限conda remove gcc_linux-64 gcc_impl_linux-64 binutils_linux-64 binutils_impl_linux-64 解决复制终端文本换行的问题 123zcat HB-10-2_filtered.vcf.gz | sed -n '/#CHROM/=' # 输出行号zcat HB-10-2_filtered.vcf.gz | head -n 42 &gt; header.txt# 文本编辑器打开会多一行，因为行末尾的换行符 JBrowse 参考JBrowse Configuration Guide Variant Tracks (VCF) Troubleshooting Browse基因浏览器介绍]]></content>
      <tags>
        <tag>生物信息</tag>
        <tag>jbrowse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git和github使用]]></title>
    <url>%2F2019%2F02%2F27%2Fgit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[对之前使用git和github的整理，重新在网上搜索资料，添加代码注释。 简介 我们一直用GitHub作为免费的远程仓库，如果是个人的开源项目，放到GitHub上是完全没有问题的。其实GitHub还是一个开源协作社区，通过GitHub，既可以让别人参与你的开源项目，也可以参与别人的开源项目。 在GitHub出现以前，开源项目开源容易，但让广大人民群众参与进来比较困难，因为要参与，就要提交代码，而给每个想提交代码的群众都开一个账号那是不现实的，因此，群众也仅限于报个bug，即使能改掉bug，也只能把diff文件用邮件发过去，很不方便。 但是在GitHub上，利用Git极其强大的克隆和分支功能，广大人民群众真正可以第一次自由参与各种开源项目了。 使用过的代码12345678910111213# 创建SSH Key# 在用户目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件ssh-keygen -t rsa -C "qizhengyang17@gmail.com"# 测试添加是否成功ssh -T git@github.com# 配置个人信息git config --global user.name "qizhengyang"git config --global user.email "qizhengyang17@gmail.com"# 通过SSH协议连接远程仓库git remote add origin git@github.com:qizhengyang2017/thesis.git 添加、提交 1234567891011121314# 当前仓库状态git status# 将文件更改添加到暂存区git add prepDE.py# 添加所有更改到暂存区git add .# 提交说明git commit -m "第一个脚本文件"# 推送内容到GitHub的仓库git push -u origin master git命令表格直接复制完全没有问题呀 个人本地使用 行为 命令 备注 初始化 init 在本地的当前目录里初始化git仓库 clone 地址 从网络上某个地址拷贝仓库(repository)到本地 查看当前状态 status 查看当前仓库的状态。碰到问题不知道怎么办的时候，可以通过看它给出的提示来解决问题 查看不同 diff 查看当前状态和最新的commit之间不同的地方 diff 版本号1 版本号2 查看两个指定的版本之间不同的地方。这里的版本号指的是commit的hash值 添加文件 add -A 这算是相当通用的了。在commit之前要先add 撤回stage的东西 checkout – . 这里用小数点表示撤回所有修改，在--的前后都有空格 提交 commit -m “提交信息” 提交信息最好能体现更改了什么 删除未tracked clean -xf 删除当前目录下所有没有track过的文件。不管它是否是.gitignore文件里面指定的文件夹和文件 查看提交记录 log 查看当前版本及之前的commit记录 reflog HEAD的变更记录 版本回退 reset –hard 版本号 回退到指定版本号的版本，该版本之后的修改都被删除。同时也是通过这个命令回到最新版本。需要reflog配合 个人使用远程仓库 行为 命令 备注 设置用户名 config –global user.name “你的用户名” 设置邮箱 config –global user.email “你的邮箱” 生成ssh key ssh-keygen -t rsa -C “你的邮箱” 这条命令前面不用加git 添加远程仓库 remote add origin 你复制的地址 设置origin 上传并指定默认 push -u origin master 指定origin为默认主机，以后push默认上传到origin上 提交到远程仓库 push 将当前分支增加的commit提交到远程仓库 从远程仓库同步 pull 在本地版本低于远程仓库版本的时候，获取远程仓库的commit 可以用一张图直观地看出以上主要的命令对仓库的影响。 工作区和暂存区 参考使用GitHub git和github简单教程 使用git和github管理自己的项目 Git | 一篇文章搞定Git、GitHub的理解和使用（学习笔记）]]></content>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JBrowse 安装和使用]]></title>
    <url>%2F2019%2F02%2F26%2FJBrowse%20%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[这件事情弄了我两天。查看处理错误信息占据了大部分时间。好在最后终于成功了。 感悟，对于技术文档不能完全照搬。中文的技术文档，主要的作用是了解，最后的实践一定要充分参考官方的英文文档。 基因浏览器介绍（入门）https://www.plob.org/article/11742.html 在日常数据处理和分析工作中，根据分析项目的不同我们会面对各种各样的文件，比如mapping得到的SAM或者BAM文件，在此基础上转化而来的记录各种（定量）信息的bedgraph文件、Wig文件或者tdf文件，亦或记录variant信息的VCF文件。以至于有一种比较调侃的说法，所谓“生物信息”就是整天和各种格式的文件打交道，转换来转换去。 JBrowse 是今天要介绍的主角。它是GMOD开源项目的一部分，想了解这个开源项目可以查看它的官方网站http://gmod.org/wiki/Main_Page 完全基于HTML5和Javascript构建的JBrowse通过AJAX技术实现了数据的异步加载，所以响应速度非常快。由于Javascript将大量的计算工作在前端完成，服务器端只需要向浏览器客户端发送静态文件，因此也极大程度减轻了服务器端的负担。 完全基于HTML5和Javascript构建的JBrowse通过AJAX技术实现了数据的异步加载，所以响应速度非常快。由于Javascript将大量的计算工作在前端完成，服务器端只需要向浏览器客户端发送静态文件，因此也极大程度减轻了服务器端的负担。 准备参考序列安装成功之后首先要做的是格式化参考序列，支持的格式有fasta， gff，或者是用samtools faidx处理过的indexed fasta 文件。参考序列生成的track 会为后续所有文件提供一个坐标，一直放大后参考序列的碱基也会显示出来。 通常我们使用某一个物种的genome fasta 文件，默认情况下，每一条染色体会独立为一条参考序列。如果想把RNA或者蛋白序列当成参考序列也没有问题。 需要注意的是所有数据默认都会输出在out目录中，如果你想用JBrowse展示不同物种的信息，最好使用--out参数指定单独的目录。 在这里提前说明，后面所有基于命令行设置的配置信息都会自动生成在tarckList.json文件中。 准备参考序列需要用到的是bin目录下的prepare-refseqs.pl脚本 对于VCF文件，需要提前做的准备就是使用bgzip压缩之后再利用tabix -p vcf 建立index。 展示方式和BigWig类似，其中type要定义为 “JBrowse/View/Track/HTMLVariants”；而storeClass定义为”JBrowse/Store/SeqFeature/VCFTabix” 即可。 尝试安装主要参考这篇文献 https://yq.aliyun.com/articles/650480 没有成功，应该是perl模块的安装问题。不知道是否和conda有关。 ! Installing GD failed. See /root/.cpanm/work/1551106022.8560/build.log for details. Retry with –force to force install it.Searching XML::LibXML (0) on cpanmetadb …Already tried XML-LibXML-2.0134. Skipping.! Installing the dependencies failed: Module ‘XML::LibXML’ is not installed, Module ‘XML::LibXML::Reader’ is not installed, Module ‘GD’ is not installed, Module ‘DBI’ is not installed, Module ‘DB_File’ is not installed! Bailing out the installation for BioPerl-1.7.5.Searching JSON::XS (0) on cpanmetadb …Unpacking JSON-XS-4.01.tar.gzFAIL ! Installing JSON::XS failed. See /root/.cpanm/work/1551106022.8560/build.log for details. Retry with –force to force install it.Searching Bio::Root::Version (1.006000) on cpanmetadb … -&gt; FAIL Installing the dependencies failed: Module ‘DBI’ is not installed, Module ‘Heap::Simple::XS’ is not installed, Module ‘Bio::Annotation::SimpleValue’ is not installed, Module ‘Bio::SeqFeature::Annotated’ is not installed, Module ‘DB_File’ is not installed, Module ‘Bio::Index::Fasta’ is not installed, Module ‘Bio::SeqFeature::Lite’ is not installed, Module ‘DBD::SQLite’ is not installed, Module ‘Devel::Size’ is not installed, Module ‘Bio::FeatureIO’ is not installed, Module ‘JSON::XS’ is not installed, Module ‘Bio::Root::Version’ is not installed, Module ‘PerlIO::gzip’ is not installed,Module ‘Bio::OntologyIO’ is not installed-&gt; FAIL Bailing out the installation for JBrowse-. 123456789perl -MCPAN -e shellinstall DBIconda install -c bioconda perl-module-listconda install -c bioconda perl-dbiconda install -c bioconda perl-bioperl instmodshlocate XML/SAX.pm /home/conda/feedstock_root/build_artifacts/perl_1548813468557/_build_env/bin/x86_64-conda_cos6-linux-gnu-gcc: No such file or directorymake: *** [Perl.o] Error 127 docker安装（没有成功）http://www.runoob.com/docker/centos-docker-install.html 安装docker很顺畅 123456sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum makecache fastsudo yum -y install docker-cesudo systemctl start dockerdocker run hello-world 由于本地没有hello-world这个镜像，所以会下载一个hello-world的镜像，并在容器内运行。 jbrowse/gmod-jbrowse 卡住了 12docker run -p 8080:80 jbrowse/jbrowse-1.12.0# 有问题 conda 安装（安装成功了，不知道如何启用）12# 需要超级用户权限，不然会失败conda install -c bioconda jbrowse 123# conda 的卸载命令conda list | grep jbrowseconda remove jbrowse 安装Nginxyum install nginx [root@localhost qi]# netstat -ntplActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:111 0.0.0.0: LISTEN 1/systemdtcp 0 0 0.0.0.0:80 0.0.0.0: LISTEN 1823/nginx: mastertcp 0 0 0.0.0.0:6000 0.0.0.0: LISTEN 6706/Xtcp 0 0 192.168.122.1:53 0.0.0.0: LISTEN 6629/dnsmasqtcp 0 0 0.0.0.0:22 0.0.0.0: LISTEN 6296/sshdtcp 0 0 127.0.0.1:631 0.0.0.0: LISTEN 6299/cupsdtcp 0 0 127.0.0.1:6010 0.0.0.0: LISTEN 13051/sshd: qi@pts/tcp 0 0 127.0.0.1:6011 0.0.0.0: LISTEN 23741/sshd: qi@pts/tcp 0 0 127.0.0.1:6012 0.0.0.0: LISTEN 31798/sshd: qi@pts/tcp6 0 0 :::3306 ::: LISTEN 7024/mysqldtcp6 0 0 :::111 ::: LISTEN 1/systemdtcp6 0 0 :::80 ::: LISTEN 1823/nginx: mastertcp6 0 0 :::6000 ::: LISTEN 6706/Xtcp6 0 0 :::22 ::: LISTEN 6296/sshdtcp6 0 0 ::1:631 ::: LISTEN 6299/cupsdtcp6 0 0 ::1:6010 ::: LISTEN 13051/sshd: qi@pts/tcp6 0 0 ::1:6011 ::: LISTEN 23741/sshd: qi@pts/tcp6 0 0 ::1:6012 ::: LISTEN 31798/sshd: qi@pts/ 12kill 1823service nginx star Centos7 yum配置Nginx /home/qi/miniconda3/pkgs/jbrowse-1.16.2-pl526h6bb024c_6/opt/jbrowse 又卸载yum remove nginx 成功的操作 将web服务器换成了apache，放文件的目录就是 /var/www/html How to Install Apache on CentOS 7 CentOS 7 安装 Apache, MySQL, PHP 指南 Jbrowse下载安装 下面这句话很关键，提到了不用 root 或 sudo 安装。我也不知道为什么，这部分： Installing Perl prerequisites 成功。这是昨天一直没成功的地方 Formatting data 失败。提取报错信息里的命令，运行，获取错误信息。Can’t locate local/lib.pm 3. Run the automated-setup script, ./setup.sh, which will attempt to install all of JBrowse’s (modest) prerequisites for you in the jbrowse/ directory itself. Note that setup.sh should not be run as root or with sudo. (py3.6) [qi@localhost JBrowse-1.16.3]$ ./setup.shGathering system information …done.NOTE: Legacy scripts wig-to-json.pl and bam-to-json.pl have been removed from setup. Their functionality has been superseded by add-bam-track.pl and add-bw-track.pl. If you require the old versions, please use JBrowse 1.12.3 or earlier.Minimal release, skipping node and Webpack buildInstalling Perl prerequisites …done. Formatting Volvox example data … failed. See setup.log file for error messages. Formatting Yeast example data … failed. See setup.log file for error messages. Formatting Volvox example data … rm -rf sample_data/json/volvox bin/prepare-refseqs.pl –fasta docs/tutorial/data_files/volvox.fa –out sample_data/json/volvoxAttempting to create directory /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlibmkdir sample_data/json/volvox: Permission denied at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JsonFileStorage.pm line 64. Formatting Yeast example data … rm -rf sample_data/json/yeast/ bin/prepare-refseqs.pl –fasta sample_data/raw/yeast_scaffolds/chr1.fa.gz –fasta sample_data/raw/yeast_scaffolds/chr2.fa.gzip –out sample_data/json/yeast/mkdir sample_data/json/yeast/: Permission denied at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JsonFileStorage.pm line 64. Can’t locate local/lib.pm in @INC (@INC contains: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3 /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5 /var/www/html/JBrowse-1.16.3/bin/../src/perl5 /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /var/www/html/JBrowse-1.16.3/bin/../src/perl5/JBlibs.pm line 25.Compilation failed in require at bin/prepare-refseqs.pl line 5.BEGIN failed–compilation aborted at bin/prepare-refseqs.pl line 5 1234567locate local/lib.pmcp -r /home/qi/perl5/lib/perl5/local /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/5.16.3sudo bin/prepare-refseqs.pl --fasta docs/tutorial/data_files/volvox.fa --out sample_data/json/volvoxsudo bin/prepare-refseqs.pl --fasta sample_data/raw/yeast_scaffolds/chr1.fa.gz --fasta sample_data/raw/yeast_scaffolds/chr2.fa.gzip --out sample_data/json/yeast/ 地址： http://10.164.6.154/JBrowse-1.16.3/index.html?data=sample_data/json/volvox 2019-2-28 19:53:01 今天登入ip的时候，没有显示。 Enable Apache to start at boot, and restart the service for the above changes to take effect: 12sudo systemctl enable httpd.servicesudo systemctl restart httpd.service 1234# 确认http服务是否开启，端口 80sudo systemctl is-enabled httpd.service# 查看历史记录，awk去掉第一列，sed去掉行首空格history | tail -n 40 | awk '&#123;$1="";print $0&#125;' | sed 's/^ *//' 准备参考序列和特征数据JBrowser 用的是 perl 5.16.3 所有track都生成在默认的data目录中 123456# 指定perl为/usr/bin/perl # seq trackList.json tracks.conf/usr/bin/perl bin/prepare-refseqs.pl --fasta ~/data2/tan_rna/genome/HWB.chromosome.fa# tracks/usr/bin/perl bin/flatfile-to-json.pl --gff HWB.gene.models.gff3 --trackType CanvasFeatures --trackLabel HWBgff 不指定会出现如下错误：[^1] (base) [qi@localhost JBrowse-1.16.3]$ bin/flatfile-to-json.pl –helpperl: symbol lookup error: /var/www/html/JBrowse-1.16.3/bin/../src/perl5/../../extlib/lib/perl5/x86_64-linux-thread-multi/auto/JSON/XS/XS.so: undefined symbol: Perl_xs_apiversion_bootcheck 查资料 This type of error is almost always indicates you are loading a module that was installed using a different build of Perl. https://stackoverflow.com/questions/51375821/perl-xs-apiversion-bootcheck [^1]: 不指定，如果用sudo 也不会出现错误，如：sudo bin/prepare-refseqs.pl --fasta ~/data2/tan_rna/genome/HWB.chromosome.fa --out sample_data/json/HWB 不清楚原因 建立索引方便搜索缺这一步，搜索功能不能用 12# 生成 ./data/names/usr/bin/perl bin/generate-names.pl http://10.164.6.154/JBrowse-1.16.3/index.html?data=data VCF trackshttp://jbrowse.org/docs/variants.html 123for i in *.vcf; do bgzip $i; donefor i in *.vcf.gz; do tabix -p vcf $i; done]]></content>
      <tags>
        <tag>生物信息</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用GATK和snakemake框架的总结]]></title>
    <url>%2F2019%2F02%2F20%2Fsnakemake%2F</url>
    <content type="text"><![CDATA[[TOC] 标准流程 检查md5值12md5sum -c checksums.md5# 报错,使用dos2unix转化格式 trimmomatic1234567# 2019-2-12 17:16:35 trimfor i in *_1.clean.fq.gzdo trimmomatic PE -threads 8 -phred33 $i $&#123;i/_1/_2&#125; -baseout $&#123;i:0:7&#125;.fq.gz \ ILLUMINACLIP:/home/qi/miniconda3/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 \ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done 2019-2-13 12:55:38 查看结果，只trim了一部分，查看trim.log。HB-9-3终止。 ps -aux | grep trimmomatic 查看任务是否还在运行。没有在运行。程序不知什么原因意外中断，来的时候xshell也没有连上服务器，怀疑是服务器自动重启了。 12345678910nohup mv HB-*.fq.gz trim &amp;rm nohup.outcd trimrm HB-9-3__*fq.gzmv HB-9-3* ..cd ..# trim剩下的fq.gznohup time bash trim.sh &gt; trim.log 2&gt;&amp;1 &amp;jobsps -aux | grep trimmomatic 2019-2-13 17:06:38 查看任务，意外终止。NH-11-2 移动、删除文件，reboot，重新执行脚本。 1nohup time bash trim.sh &gt;&gt; trim.log 2&gt;&amp;1 &amp; 2019-2-13 19:35:51 查看，又意外停止 查看系统最后重启时间 1who -b 显然，服务器会自动重启。 不运行任务的时候，不会重启。是因为CPU温度过高？ 更改脚本试一试，把-threads 8 改成 4 123456for i in *_1.clean.fq.gzdo trimmomatic PE -threads 4 -phred33 $i $&#123;i/_1/_2&#125; -baseout $&#123;i:0:7&#125;.fq.gz \ ILLUMINACLIP:/home/qi/miniconda3/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 \ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done 1nohup time bash trim.sh &gt; trim.log 2&gt;&amp;1 &amp; 没有效果 threads改成16 一小时不到又重启了。。 2019-2-14 16:16:57 质检1fastqc *.fq.gz -o qcDIR/ -t 6 -d qcDIR/ &gt;&gt;fastqc.r.log 2&gt;&gt;fastqc.e.log 1conda install -c bioconda multiqc 做multiqc之前构建一个python3的虚拟环境 1234567891011python --versionconda create --name py3.6 python=3.6source activate py3.6# You'll want to add the source activate py3.6 line to your .bashrc file so that the environment is loaded every time you load the terminal.# conda deactivate# conda activate py3.6# Windows: activate py3.6# 需要重新安装conda install -c bioconda multiqc 123#利用multiqc整合结果，方便批量查看mkdir qcDIR_multiqcmultiqc qcDIR/ -o qcDIR_multiqc/ 用 snakemake 编写任务流程snakemake是一个用来编写任务流程的工具，用python写的，因此其执行的流程脚本也比较通俗易懂，易于理解，可以看做用户友好版的make。（make在安装软件的时候会用到，没有研究过makefile文件，对它的用处不是太了解。） 其实流程控制是复杂任务（在生信领域很常见）必需的关注点。只是snakemake对于代码功力不够的人来说，在写好代码与重复流程的花销的trade off上，还不如一遍遍重复流程。。但是真的是一个写好了就很好用的东西。 snakemake能够使用文件名通配的方式对一类文件进行处理 12# installingconda install -c bioconda snakemake 简单的例子12345678910111213cd $HOME# Create a folder where we will run our commands:mkdir snakemake-examplecd snakemake-example# Make a fake genome:touch genome.fa# Make some fake data:mkdir fastqtouch fastq/Sample1.R1.fastq.gz fastq/Sample1.R2.fastq.gztouch fastq/Sample2.R1.fastq.gz fastq/Sample2.R2.fastq.gz snakemake 脚本 123456789101112131415SAMPLES = ['Sample1', 'Sample2']rule all: input: expand('&#123;sample&#125;.txt', sample=SAMPLES)rule quantify_genes: input: genome = 'genome.fa', r1 = 'fastq/&#123;sample&#125;.R1.fastq.gz', r2 = 'fastq/&#123;sample&#125;.R2.fastq.gz' output: '&#123;sample&#125;.txt' shell: 'echo &#123;input.genome&#125; &#123;input.r1&#125; &#123;input.r2&#125; &gt; &#123;output&#125;' 学院集群上运行 1snakemake --snakefile Snakefile 12# 可视化snakemake --forceall --dag | dot -Tpng &gt; dag1.png snakemake 规则 Snakemake基于规则执行命令，规则一般由input, output,shell三部分组成。除了rule all，其余必须有output Snakemake可以自动确定不同规则的输入输出的依赖关系，根据时间戳来判断文件是否需要重新生成 Snakemake以{sample}.fa形式进行文件名通配，用{wildcards.sample}获取sample的实际文件名 Snakemake用expand()生成多个文件名，本质是Python的列表推导式 Snakemake可以在规则外直接写Python代码，在规则内的run里也可以写Python代码。 Snakefile的第一个规则通常是rule all，根据all里的文件决定执行哪些rule。如上面的例子，注释掉all里的input则不执行第二条rule，（推断未尝试：rule all里定义最终的输出文件，程序也能执行，那么rule里的输出文件在什么时候会被删除，是在所有rule运行完之后，还是在判断出该输出文件不会被用到的时候？） 在output中的结果文件可以是未存在目录中的文件,这时会自动创建不存在的目录（不需要事先建文件夹，这个功能实在是方便） snakemake 命令 wildcards: 用来获取通配符匹配到的部分，例如对于通配符”{dataset}/file.{group}.txt”匹配到文件101/file.A.txt，则{wildcards.dataset}就是101，{wildcards.group}就是A。 temp: 通过temp方法可以在所有rule运行完后删除指定的中间文件，eg.output: temp(“f1.bam”)。 protected: 用来指定某些中间文件是需要保留的，eg.output: protected(“f1.bam”)。 snakemake 执行一般讲所有的参数配置写入Snakefile后直接在Snakefile所在路径执行snakemake命令即可开始执行流程任务，如果只有一个snakefile的话，连文件都不用写。一些常用的参数： 123456789101112131415161718--snakefile, -s 指定Snakefile，否则是当前目录下的Snakefile--dryrun, -n 不真正执行，一般用来查看Snakefile是否有错--printshellcmds, -p 输出要执行的shell命令--reason, -r 输出每条rule执行的原因,默认FALSE--cores, --jobs, -j 指定运行的核数，若不指定，则使用最大的核数--force, -f 重新运行第一条rule或指定的rule--forceall, -F 重新运行所有的rule，不管是否已经有输出结果--forcerun, -R 重新执行Snakefile，当更新了rule时候使用此命令#一些可视化命令$ snakemake --dag | dot -Tpdf &gt; dag.pdf#集群投递snakemake --cluster "qsub -V -cwd -q 节点队列" -j 10# --cluster: 集群运行指令# qusb -V -cwd -q， 表示输出当前环境变量(-V),在当前目录下运行(-cwd), 投递到指定的队列(-q), 如果不指定则使用任何可用队列# --local-cores N: 在每个集群中最多并行N核# --cluster-config/-u FILE: 集群配置文件 snakemake 实践123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187# 2019-2-19 10:28:52 part# GATK snakemake# qizhengyangfrom os.path import joinGENOME = 'genome/HWB.chromosome.fa'GTF = 'genes/HWB.gene.models.gtf'(SAMPLES,) = glob_wildcards('pairedDIR/&#123;sample&#125;_1P.fq.gz')PATTERN_R1 = join('pairedDIR', '&#123;sample&#125;_1P.fq.gz')PATTERN_R2 = join('pairedDIR', '&#123;sample&#125;_2P.fq.gz')rule all: input: 'star_index_2pass/', expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES), 'star_index_2pass/', expand('star_2pass/&#123;sample&#125;Aligned.out.sam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_rg_added_sorted.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup_split.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;.vcf', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_filtered.vcf', sample=SAMPLES)rule star_index: input: genome = GENOME, gtf = GTF output: # 最后加上 directory()，不然在集群上运行会报错 star_index = directory('star_index/') log: 'star_index.log' threads: 8 run: # star 1-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.star_index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbGTFfile &#123;input.gtf&#125;' ' 2&gt; &#123;log&#125;')rule star_1pass_align: input: index = 'star_index/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: index = 'star_1pass/&#123;sample&#125;SJ.out.tab' threads: 8 params: prefix = './star_1pass/&#123;sample&#125;' # 在使用params之前是报错的，NameError,The name 'sample' is unknown in this context run: # star 1-pass align, OK shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule star_2pass_index: input: genome = GENOME, # 这里必需加expand，不然会报错：Wildcards in input files cannot be determined from output files: # 'sample'。 # 报错信息说通配的信息不能从output里推断出来，因为我的output是文件夹。input应该是所有样品的信息，可以用expand函数，这样通配的问题就没有了。 # 然后用--sjdbFileChrStartEnd参数将所有样品的SJ.out.tab文件作为输入的annotated junction进行第二次建index # http://www.bioinfo-scrounger.com/archives/288 # 不能加三个引号（"""或'''注释）进行段落注释 splice_site = expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES) output: index = directory('star_index_2pass/') threads: 8 run: # star 2-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbFileChrStartEnd &#123;input.splice_site&#125;')rule star_2pass_align: input: index = 'star_index_2pass/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' threads: 8 params: prefix = 'star_2pass/&#123;sample&#125;' run: # star 2-pass align shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule picard: input: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' output: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' run: # RGID和RGSM的sample必须是&#123;wildcards.sample&#125;，不然 # The name 'sample' is unknown in this context. Please make sure that you defined that variable. # Also note that braces not used for variable access have to be escaped by repeating them, i.e. &#123;&#123;print $1&#125;&#125; shell('picard AddOrReplaceReadGroups' ' I=&#123;input.sam&#125;' ' O=&#123;output.bam&#125;' ' SO=coordinate' ' RGID=&#123;wildcards.sample&#125;' ' RGLB=rna' ' RGPL=illumina' ' RGPU=hiseq' ' RGSM=&#123;wildcards.sample&#125;')rule picard_markduplicates: input: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' output: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam' params: dedup_metrices = 'star_2pass/&#123;sample&#125;_dedup.metrics' run: shell('picard MarkDuplicates' ' I=&#123;input.bam&#125;' ' O=&#123;output.dedup_bam&#125;' ' CREATE_INDEX=true' ' VALIDATION_STRINGENCY=SILENT' ' M=&#123;params.dedup_metrices&#125;')rule gatk_split: input: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam', genome = GENOME output: split_bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam' run: shell('java -jar $GATK -T SplitNCigarReads' ' -R &#123;input.genome&#125;' ' -I &#123;input.dedup_bam&#125;' ' -o &#123;output.split_bam&#125;' ' -rf ReassignOneMappingQuality' ' -RMQF 255' ' -RMQT 60' ' -U ALLOW_N_CIGAR_READS')rule gatk: input: bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam', genome = GENOME output: vcf = 'star_2pass/&#123;sample&#125;.vcf' run: shell('java -jar $GATK -T HaplotypeCaller' ' -R &#123;input.genome&#125;' ' -I &#123;input.bam&#125;' ' -dontUseSoftClippedBases' ' -stand_call_conf 20.0' ' -o &#123;output.vcf&#125;')rule gatk_filter: input: genome = GENOME, vcf = 'star_2pass/&#123;sample&#125;.vcf' output: 'star_2pass/&#123;sample&#125;_filtered.vcf' run: shell('java -jar $GATK' ' -T VariantFiltration' ' -R &#123;input.genome&#125;' ' -V &#123;input.vcf&#125;' ' -window 35' ' -cluster 3' ' -filterName FS -filter "FS &gt; 30.0"' ' -filterName QD -filter "QD &lt; 2.0"' ' -o &#123;output&#125;') 实验室服务器试运行 By default snakemake executes the first rule in the snakefile. output中会自动创建没有的文件夹 12# cores设置核心数snakemake -s part.py --cores 8 跑samples中的所有样品 存储空间不够，停止 重新运行之后，是从NH-12-1开始 之前的结果只有NH-12-1Log.out，时间是2019/2/20 3:35 现在是： 说明这个样本从头开始跑了。 集群上运行12345678910111213141516# 查看节点状态pestat# 测试 dry runsnakemake -n# 检测某条rulesnakemake -n -r star_index# 可视化# snakemake --dag | dot -Tpdf &gt; dag.pdfsnakemake --forceall --dag | dot -Tpdf &gt; dag.pdf# # 投递任务 -cwd不能用 [qsub: illegal -c value]# snakemake --cluster "qsub -V -cwd -q high" -j 20snakemake --cluster "qsub -V -q high" -j 20# 报错# Complete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-20T152054.823542.snakemake.log 12# 查看错误信息 输出目录要用 directory() ，在实验室服务器上没有遇上这种情况less snakejob.star_index.254.sh.e980069 ImproperOutputException in line 30 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Outputs of incorrect type (directories when expecting files or vice versa). Output directories must be flagged with directory(). for rule star_index:star_index/Removing output files of failed job star_index since they might be corrupted:star_index/Skipped removing non-empty directory star_index/Shutting down, this might take some time. 12345snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20# -j 必须指定，否则Error: you need to specify the maximum number of jobs to be queued or executed at the same time with --jobs.# Here, -j denotes the number of jobs submitted being submitted to the cluster at the same time# -j 代表可以同时提交的任务数# 没有重新运行star_index这个任务 2019-2-20 20:22:01 错误： 1less snakejob.star_1pass_align.28.sh.e980101 Possible cause 1: not enough RAM. Check if you have enough RAM 6975932175 bytes 123# 剩下的三个重新跑 snakemake -nsnakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 没有安装picard。。这种低级错误千万不能再犯了 12conda install -c bioconda picard snakemake -n 终端意外退出，输出到屏幕上的信息没有了 123456789ls -acd .snakemake/log# 按修改时间查看文件# l长格式显示，t按时间排序（-r升序）ls -lth# snakemake程序停止# 已经提交的任务会运行完snakemake -n --quiet 1snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 Error: Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory 123# 解决办法 https://zhuanlan.zhihu.com/p/47575136snakemake --unlocksnakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 报错 1less snakejob.star_2pass_align.68.sh.e980306 libgcc_s.so.1 must be installed for pthread_cancel to work 123less snakejob.star_2pass_align.68.sh.e980306 # 会自动删除失败任务的输出文件# Removing output files of failed job star_2pass_align since they might be corrupted: 短暂的断网，与服务器连接中断 12snakemake -n# 报错，因为已经提交的任务没有运行完 nohup将程序放入后台12whatis nohupman 1 nohup If standard input is a terminal, redirect it from /dev/null. If standard output is a terminal, append output to ‘nohup.out’ if possible, ‘$HOME/nohup.out’ otherwise. If standard error is a terminal, redirect it to standard output. To save output to FILE, use ‘nohup COMMAND &gt; FILE’. 重定向与后台运行的知识 12345678nohup snakemake -n &amp;# [qizhengyang@node1 gatk_rna]$ nohup: 忽略输入并把输出追加到"nohup.out"nohup snakemake -n &gt; test_nohup.out 2&gt;&amp;1 &amp;# 标准输出和标准错误输出都重定向到文件snakemake -n &gt; test_nohup.out 2&gt;&amp;1 &amp;# 会产生进程号 12# nohup方式后台运行(&amp;) 忽略所有发送给子命令的挂断（SIGHUP）信号 重定向子命令的标准输出(stdout)和标准错误(stderr)nohup snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 &gt; snakemake.out 2&gt;&amp;1 &amp; IP 意外变化2019-2-21 16:45:01 实验室的服务器连不上，我查看了ssh server是否开启，查了IP，发现是IP变了。 重新开始2019-2-21 21:33:58查看了实验室服务器生的Index文件夹，比较了大小，之前第一步需要重做。。很悲惨 gatk加了 -Xmx10g -jar # Decrease Java heap size (-Xmx/-Xms) 123snakemake -n -s Snakefile2 --quiet# 调了最大并行数，因为会有内存不够的错误nohup snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 10 &amp;&gt; snakemake.out &amp; 1snakemake --unlock 之后再运行命令 12345678nohup snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 10 &amp;&gt; snakemake.out &amp;# snakemake.out内容重新写入# 查看任务jobs# 或ps aux|grep Snakefile2|grep -v grep# https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/ps.html 2019-2-22 14:56:03 查看任务 1ps aux|grep Snakefile2|grep -v grep 这个任务在内存中，但 qstat没有任务在运行 12345less snakemake.outcd .snakemake/log/ls -thlless 2019-02-21T224633.856137.snakemake.log # 与snakemake.out里的信息一样，没有错误提示，snakemake -n -s Snakefile2 --quiet 这个进程kill不掉 kill 7550 但是，好像起作用了，log文件时间改了，似乎多了一句话（下次log文件应该copy一份下来，或者将最后的内容粘贴下来） Will exit after finishing currently running jobs. 重新运行 123snakemake --unlock# 不放入后台，-j 20，实时监测snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 20 kill -9 7550 有效果 两个进程信息比较 院里集群上的时间快18分钟 报错先查看snakemake.log，然后查看job的std err star 2pass align 出错 2019-2-22 20:06:58 12# 查看错误信息less snakejob.star_2pass_align.52.sh.e980754 Building DAG of jobs…Using shell: /bin/bashProvided cores: 20Rules claiming more threads will be scaled down.Job counts: count jobs 1 star_2pass_align 1 [Fri Feb 22 16:44:54 2019]rule star_2pass_align: input: star_index_2pass/, pairedDIR/OV-9-2_1P.fq.gz, pairedDIR/OV-9-2_2P.fq.gz output: star_2pass/OV-9-2Aligned.out.sam jobid: 0 wildcards: sample=OV-9-2 threads: 20 libgcc_s.so.1 must be installed for pthread_cancel to workterminate called after throwing an instance of ‘St9bad_alloc’libgcc_s.so.1 must be installed for pthread_cancel to work what(): std::bad_alloc[Fri Feb 22 16:46:26 2019]Error in rule star_2pass_align: jobid: 0 output: star_2pass/OV-9-2Aligned.out.sam RuleException:CalledProcessError in line 101 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile2:Command ‘set -euo pipefail; STAR –runThreadN 20 –genomeDir star_index_2pass/ –readFilesIn pairedDIR/OV-9-2_1P.fq.gz pairedDIR/OV-9-2_2P.fq.gz –readFilesCommand zcat –outFileNamePrefix star_2pass/OV-9-2’ returned non-zero exit status 141.snakejob.star_2pass_align.52.sh.e980754 123# 查看任务qstatpestat 12345pbsnodes -a | lesssnakemake --cluster --rerun-incomplete qsub --jobs 10# qstat -a (for summary)# qstat -n (You will see where the nodes the job lands) [qizhengyang@node1 gatk_rna]$ snakemake -nBuilding DAG of jobs…IncompleteFilesException:The files below seem to be incomplete. If you are sure that certain files are not incomplete, mark them as complete with snakemake --cleanup-metadata &lt;filenames&gt; To re-generate the files rerun your command with the –rerun-incomplete flag.Incomplete files:star_2pass/OV-10-1Aligned.out.samstar_2pass/NH-10-3Aligned.out.samstar_2pass/OV-10-2Aligned.out.samstar_2pass/NH-11-2Aligned.out.samstar_2pass/HB-9-3Aligned.out.samstar_2pass/HB-10-3Aligned.out.sam 这样是在当前节点运行的 [qizhengyang@node1 gatk_rna]$ snakemake --rerun-incompleteBuilding DAG of jobs…Using shell: /bin/bashProvided cores: 1Rules claiming more threads will be scaled down.Job counts: count jobs 1 all 36 gatk 36 gatk_filter 36 gatk_split 36 picard 36 picard_markduplicates 25 star_2pass_align 206 [Fri Feb 22 23:06:04 2019]rule picard: input: star_2pass/OV-12-3Aligned.out.sam output: star_2pass/OV-12-3_rg_added_sorted.bam jobid: 101 wildcards: sample=OV-12-3 Job counts: count jobs 1 picard 1INFO 2019-02-22 23:06:07 AddOrReplaceReadGroups 12# 只有胖节点有用，将任务提交到胖节点上，比需指定 -q low。默认是 high，会处于排队状态snakemake --rerun-incomplete --cluster "qsub -q low" --jobs 10 [qizhengyang@node1 gatk_rna]$ snakemake –rerun-incomplete –cluster “qsub -q low” –jobs 10Building DAG of jobs…Using shell: /bin/bashProvided cluster nodes: 10Job counts: count jobs 1 all 36 gatk 36 gatk_filter 36 gatk_split 36 picard 36 picard_markduplicates 25 star_2pass_align 206 [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/HB-12-1Aligned.out.sam output: star_2pass/HB-12-1_rg_added_sorted.bam jobid: 106 wildcards: sample=HB-12-1 Submitted job 106 with external jobid ‘980770.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/NH-9-2Aligned.out.sam output: star_2pass/NH-9-2_rg_added_sorted.bam jobid: 108 wildcards: sample=NH-9-2 Submitted job 108 with external jobid ‘980771.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/OV-12-3Aligned.out.sam output: star_2pass/OV-12-3_rg_added_sorted.bam jobid: 101 wildcards: sample=OV-12-3 Submitted job 101 with external jobid ‘980772.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/HB-10-1Aligned.out.sam output: star_2pass/HB-10-1_rg_added_sorted.bam jobid: 85 wildcards: sample=HB-10-1 Submitted job 85 with external jobid ‘980773.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/NH-9-3Aligned.out.sam output: star_2pass/NH-9-3_rg_added_sorted.bam jobid: 74 wildcards: sample=NH-9-3 Submitted job 74 with external jobid ‘980774.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/OV-10-3Aligned.out.sam output: star_2pass/OV-10-3_rg_added_sorted.bam jobid: 98 wildcards: sample=OV-10-3 Submitted job 98 with external jobid ‘980775.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/HB-11-2Aligned.out.sam output: star_2pass/HB-11-2_rg_added_sorted.bam jobid: 77 wildcards: sample=HB-11-2 Submitted job 77 with external jobid ‘980776.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/OV-11-2Aligned.out.sam output: star_2pass/OV-11-2_rg_added_sorted.bam jobid: 90 wildcards: sample=OV-11-2 Submitted job 90 with external jobid ‘980777.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/NH-12-1Aligned.out.sam output: star_2pass/NH-12-1_rg_added_sorted.bam jobid: 107 wildcards: sample=NH-12-1 Submitted job 107 with external jobid ‘980778.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/OV-12-1Aligned.out.sam output: star_2pass/OV-12-1_rg_added_sorted.bam jobid: 102 wildcards: sample=OV-12-1 Submitted job 102 with external jobid ‘980779.node1’. 运行时间 picard 半小时，CPU时间是4小时。但是picard我并没有设置使用多少线程。 qstat qstat -a qstat -n 2019-2-23 11:02:59 Shutting down, this might take some time.Exiting because a job execution failed. Look above for error messageComplete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-22T232454.376651.snakemake.log 出错的任务 [Sat Feb 23 01:21:32 2019]Error in rule picard: jobid: 88 output: star_2pass/OV-9-2_rg_added_sorted.bam cluster_jobid: 980797.node1 1less snakejob.picard.88.sh.e980797 Building DAG of jobs…Using shell: /bin/bashProvided cores: 96Rules claiming more threads will be scaled down.Job counts: count jobs 1 picard 1 [Sat Feb 23 00:58:20 2019]rule picard: input: star_2pass/OV-9-2Aligned.out.sam output: star_2pass/OV-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=OV-9-2 INFO 2019-02-23 00:58:23 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/OV-9-2Aligned.out.sam -O star_2pass/OV-9-2_rg_added_sorted.bam -SO coordinate -RGID OV-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM OV-9-2 00:58:23.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 00:58:23 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/OV-9-2Aligned.out.sam OUTPUT=star_2pass/OV-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 00:58:23 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 00:58:27 AddOrReplaceReadGroups Created read-group ID=OV-9-2 PL=illumina LB=rna SM=OV-9-2 INFO 2019-02-23 00:59:00 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 33s. Last read position: chr9:26,425,698INFO 2019-02-23 00:59:34 AddOrReplaceReadGroups Processed 2,000,000 records. Elapsed time: 00:01:07s. Time for last 1,000,000: 33s. Last read position: chr5:38,200,263INFO 2019-02-23 01:00:29 AddOrReplaceReadGroups Processed 3,000,000 records. Elapsed time: 00:02:02s. Time for last 1,000,000: 55s. Last read position: chr4:1,886,332INFO 2019-02-23 01:00:56 AddOrReplaceReadGroups Processed 4,000,000 records. Elapsed time: 00:02:28s. Time for last 1,000,000: 26s. Last read position: chr5:6,379,073INFO 2019-02-23 01:01:34 AddOrReplaceReadGroups Processed 5,000,000 records. Elapsed time: 00:03:07s. Time for last 1,000,000: 38s. Last read position: chr6:18,567,116INFO 2019-02-23 01:02:13 AddOrReplaceReadGroups Processed 6,000,000 records. Elapsed time: 00:03:46s. Time for last 1,000,000: 38s. Last read position: chr9:39,573,973INFO 2019-02-23 01:02:40 AddOrReplaceReadGroups Processed 7,000,000 records. Elapsed time: 00:04:13s. Time for last 1,000,000: 27s. Last read position: chr5:22,938,417INFO 2019-02-23 01:03:19 AddOrReplaceReadGroups Processed 8,000,000 records. Elapsed time: 00:04:51s. Time for last 1,000,000: 38s. Last read position: chr4:23,545,960INFO 2019-02-23 01:03:55 AddOrReplaceReadGroups Processed 9,000,000 records. Elapsed time: 00:05:28s. Time for last 1,000,000: 36s. Last read position: chr4:17,586,047INFO 2019-02-23 01:04:24 AddOrReplaceReadGroups Processed 10,000,000 records. Elapsed time: 00:05:57s. Time for last 1,000,000: 28s. Last read position: chr7:21,260,263INFO 2019-02-23 01:04:58 AddOrReplaceReadGroups Processed 11,000,000 records. Elapsed time: 00:06:31s. Time for last 1,000,000: 33s. Last read position: chr4:25,569,847INFO 2019-02-23 01:05:36 AddOrReplaceReadGroups Processed 12,000,000 records. Elapsed time: 00:07:08s. Time for last 1,000,000: 37s. Last read position: chr4:25,075,124INFO 2019-02-23 01:06:04 AddOrReplaceReadGroups Processed 13,000,000 records. Elapsed time: 00:07:36s. Time for last 1,000,000: 28s. Last read position: chr1:5,055,820INFO 2019-02-23 01:06:38 AddOrReplaceReadGroups Processed 14,000,000 records. Elapsed time: 00:08:11s. Time for last 1,000,000: 34s. Last read position: chr5:40,818,516INFO 2019-02-23 01:07:13 AddOrReplaceReadGroups Processed 15,000,000 records. Elapsed time: 00:08:46s. Time for last 1,000,000: 34s. Last read position: chr3:28,597,426INFO 2019-02-23 01:07:40 AddOrReplaceReadGroups Processed 16,000,000 records. Elapsed time: 00:09:12s. Time for last 1,000,000: 26s. Last read position: chr2:52,066,083INFO 2019-02-23 01:08:24 AddOrReplaceReadGroups Processed 17,000,000 records. Elapsed time: 00:09:54s. Time for last 1,000,000: 41s. Last read position: chr5:8,214,376INFO 2019-02-23 01:08:53 AddOrReplaceReadGroups Processed 18,000,000 records. Elapsed time: 00:10:25s. Time for last 1,000,000: 31s. Last read position: chr2:42,040,926INFO 2019-02-23 01:09:19 AddOrReplaceReadGroups Processed 19,000,000 records. Elapsed time: 00:10:52s. Time for last 1,000,000: 26s. Last read position: chr8:3,160,150INFO 2019-02-23 01:09:59 AddOrReplaceReadGroups Processed 20,000,000 records. Elapsed time: 00:11:32s. Time for last 1,000,000: 39s. Last read position: chr9:2,765,258INFO 2019-02-23 01:10:36 AddOrReplaceReadGroups Processed 21,000,000 records. Elapsed time: 00:12:08s. Time for last 1,000,000: 36s. Last read position: chr7:872,502INFO 2019-02-23 01:11:13 AddOrReplaceReadGroups Processed 22,000,000 records. Elapsed time: 00:12:46s. Time for last 1,000,000: 37s. Last read position: chr5:38,922,782INFO 2019-02-23 01:11:43 AddOrReplaceReadGroups Processed 23,000,000 records. Elapsed time: 00:13:15s. Time for last 1,000,000: 29s. Last read position: chr2:43,156,703INFO 2019-02-23 01:12:10 AddOrReplaceReadGroups Processed 24,000,000 records. Elapsed time: 00:13:42s. Time for last 1,000,000: 26s. Last read position: chr2:42,168,429INFO 2019-02-23 01:12:37 AddOrReplaceReadGroups Processed 25,000,000 records. Elapsed time: 00:14:10s. Time for last 1,000,000: 27s. Last read position: chr5:47,894,574INFO 2019-02-23 01:13:05 AddOrReplaceReadGroups Processed 26,000,000 records. Elapsed time: 00:14:37s. Time for last 1,000,000: 27s. Last read position: chr5:47,836,828INFO 2019-02-23 01:13:33 AddOrReplaceReadGroups Processed 27,000,000 records. Elapsed time: 00:15:05s. Time for last 1,000,000: 27s. Last read position: chr8:19,290,720INFO 2019-02-23 01:14:01 AddOrReplaceReadGroups Processed 28,000,000 records. Elapsed time: 00:15:33s. Time for last 1,000,000: 27s. Last read position: chr5:17,045,282INFO 2019-02-23 01:14:29 AddOrReplaceReadGroups Processed 29,000,000 records. Elapsed time: 00:16:02s. Time for last 1,000,000: 28s. Last read position: chr4:16,374,034INFO 2019-02-23 01:14:56 AddOrReplaceReadGroups Processed 30,000,000 records. Elapsed time: 00:16:28s. Time for last 1,000,000: 26s. Last read position: chr2:51,915,818INFO 2019-02-23 01:15:24 AddOrReplaceReadGroups Processed 31,000,000 records. Elapsed time: 00:16:57s. Time for last 1,000,000: 28s. Last read position: chr3:24,910,383INFO 2019-02-23 01:15:55 AddOrReplaceReadGroups Processed 32,000,000 records. Elapsed time: 00:17:27s. Time for last 1,000,000: 30s. Last read position: chr7:12,459,410INFO 2019-02-23 01:16:22 AddOrReplaceReadGroups Processed 33,000,000 records. Elapsed time: 00:17:54s. Time for last 1,000,000: 26s. Last read position: chr9:39,689,303[Sat Feb 23 01:20:04 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 21.69 minutes.Runtime.totalMemory()=1004011520To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.lang.String.substring(String.java:1969) at htsjdk.samtools.util.StringUtil.split(StringUtil.java:89) at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:229) at htsjdk.samtools.SAMTextReader$RecordIterator.parseLine(SAMTextReader.java:268) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:255) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113) 1 picard 1 [Sat Feb 23 00:58:20 2019]rule picard: input: star_2pass/OV-9-2Aligned.out.sam output: star_2pass/OV-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=OV-9-2 INFO 2019-02-23 00:58:23 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/OV-9-2Aligned.out.sam -O star_2pass/OV-9-2_rg_added_sorted.bam -SO coordinate -RGID OV-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM OV-9-2 00:58:23.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 00:58:23 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/OV-9-2Aligned.out.sam OUTPUT=star_2pass/OV-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 00:58:23 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 00:58:27 AddOrReplaceReadGroups Created read-group ID=OV-9-2 PL=illumina LB=rna SM=OV-9-2 INFO 2019-02-23 00:59:00 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 33s. Last read position: chr9:26,425,698INFO 2019-02-23 00:59:34 AddOrReplaceReadGroups Processed 2,000,000 records. Elapsed time: 00:01:07s. Time for last 1,000,000: 33s. Last read position: chr5:38,200,263INFO 2019-02-23 01:00:29 AddOrReplaceReadGroups Processed 3,000,000 records. Elapsed time: 00:02:02s. Time for last 1,000,000: 55s. Last read position: chr4:1,886,332INFO 2019-02-23 01:00:56 AddOrReplaceReadGroups Processed 4,000,000 records. Elapsed time: 00:02:28s. Time for last 1,000,000: 26s. Last read position: chr5:6,379,073INFO 2019-02-23 01:01:34 AddOrReplaceReadGroups Processed 5,000,000 records. Elapsed time: 00:03:07s. Time for last 1,000,000: 38s. Last read position: chr6:18,567,116INFO 2019-02-23 01:02:13 AddOrReplaceReadGroups Processed 6,000,000 records. Elapsed time: 00:03:46s. Time for last 1,000,000: 38s. Last read position: chr9:39,573,973INFO 2019-02-23 01:02:40 AddOrReplaceReadGroups Processed 7,000,000 records. Elapsed time: 00:04:13s. Time for last 1,000,000: 27s. Last read position: chr5:22,938,417INFO 2019-02-23 01:03:19 AddOrReplaceReadGroups Processed 8,000,000 records. Elapsed time: 00:04:51s. Time for last 1,000,000: 38s. Last read position: chr4:23,545,960INFO 2019-02-23 01:03:55 AddOrReplaceReadGroups Processed 9,000,000 records. Elapsed time: 00:05:28s. Time for last 1,000,000: 36s. Last read position: chr4:17,586,047INFO 2019-02-23 01:04:24 AddOrReplaceReadGroups Processed 10,000,000 records. Elapsed time: 00:05:57s. Time for last 1,000,000: 28s. Last read position: chr7:21,260,263INFO 2019-02-23 01:04:58 AddOrReplaceReadGroups Processed 11,000,000 records. Elapsed time: 00:06:31s. Time for last 1,000,000: 33s. Last read position: chr4:25,569,847INFO 2019-02-23 01:05:36 AddOrReplaceReadGroups Processed 12,000,000 records. Elapsed time: 00:07:08s. Time for last 1,000,000: 37s. Last read position: chr4:25,075,124INFO 2019-02-23 01:06:04 AddOrReplaceReadGroups Processed 13,000,000 records. Elapsed time: 00:07:36s. Time for last 1,000,000: 28s. Last read position: chr1:5,055,820INFO 2019-02-23 01:06:38 AddOrReplaceReadGroups Processed 14,000,000 records. Elapsed time: 00:08:11s. Time for last 1,000,000: 34s. Last read position: chr5:40,818,516INFO 2019-02-23 01:07:13 AddOrReplaceReadGroups Processed 15,000,000 records. Elapsed time: 00:08:46s. Time for last 1,000,000: 34s. Last read position: chr3:28,597,426INFO 2019-02-23 01:07:40 AddOrReplaceReadGroups Processed 16,000,000 records. Elapsed time: 00:09:12s. Time for last 1,000,000: 26s. Last read position: chr2:52,066,083INFO 2019-02-23 01:08:24 AddOrReplaceReadGroups Processed 17,000,000 records. Elapsed time: 00:09:54s. Time for last 1,000,000: 41s. Last read position: chr5:8,214,376INFO 2019-02-23 01:08:53 AddOrReplaceReadGroups Processed 18,000,000 records. Elapsed time: 00:10:25s. Time for last 1,000,000: 31s. Last read position: chr2:42,040,926INFO 2019-02-23 01:09:19 AddOrReplaceReadGroups Processed 19,000,000 records. Elapsed time: 00:10:52s. Time for last 1,000,000: 26s. Last read position: chr8:3,160,150INFO 2019-02-23 01:09:59 AddOrReplaceReadGroups Processed 20,000,000 records. Elapsed time: 00:11:32s. Time for last 1,000,000: 39s. Last read position: chr9:2,765,258INFO 2019-02-23 01:10:36 AddOrReplaceReadGroups Processed 21,000,000 records. Elapsed time: 00:12:08s. Time for last 1,000,000: 36s. Last read position: chr7:872,502INFO 2019-02-23 01:11:13 AddOrReplaceReadGroups Processed 22,000,000 records. Elapsed time: 00:12:46s. Time for last 1,000,000: 37s. Last read position: chr5:38,922,782INFO 2019-02-23 01:11:43 AddOrReplaceReadGroups Processed 23,000,000 records. Elapsed time: 00:13:15s. Time for last 1,000,000: 29s. Last read position: chr2:43,156,703INFO 2019-02-23 01:12:10 AddOrReplaceReadGroups Processed 24,000,000 records. Elapsed time: 00:13:42s. Time for last 1,000,000: 26s. Last read position: chr2:42,168,429INFO 2019-02-23 01:12:37 AddOrReplaceReadGroups Processed 25,000,000 records. Elapsed time: 00:14:10s. Time for last 1,000,000: 27s. Last read position: chr5:47,894,574INFO 2019-02-23 01:13:05 AddOrReplaceReadGroups Processed 26,000,000 records. Elapsed time: 00:14:37s. Time for last 1,000,000: 27s. Last read position: chr5:47,836,828INFO 2019-02-23 01:13:33 AddOrReplaceReadGroups Processed 27,000,000 records. Elapsed time: 00:15:05s. Time for last 1,000,000: 27s. Last read position: chr8:19,290,720INFO 2019-02-23 01:14:01 AddOrReplaceReadGroups Processed 28,000,000 records. Elapsed time: 00:15:33s. Time for last 1,000,000: 27s. Last read position: chr5:17,045,282INFO 2019-02-23 01:14:29 AddOrReplaceReadGroups Processed 29,000,000 records. Elapsed time: 00:16:02s. Time for last 1,000,000: 28s. Last read position: chr4:16,374,034INFO 2019-02-23 01:14:56 AddOrReplaceReadGroups Processed 30,000,000 records. Elapsed time: 00:16:28s. Time for last 1,000,000: 26s. Last read position: chr2:51,915,818INFO 2019-02-23 01:15:24 AddOrReplaceReadGroups Processed 31,000,000 records. Elapsed time: 00:16:57s. Time for last 1,000,000: 28s. Last read position: chr3:24,910,383INFO 2019-02-23 01:15:55 AddOrReplaceReadGroups Processed 32,000,000 records. Elapsed time: 00:17:27s. Time for last 1,000,000: 30s. Last read position: chr7:12,459,410INFO 2019-02-23 01:16:22 AddOrReplaceReadGroups Processed 33,000,000 records. Elapsed time: 00:17:54s. Time for last 1,000,000: 26s. Last read position: chr9:39,689,303[Sat Feb 23 01:20:04 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 21.69 minutes.Runtime.totalMemory()=1004011520To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.lang.String.substring(String.java:1969) at htsjdk.samtools.util.StringUtil.split(StringUtil.java:89) at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:229) at htsjdk.samtools.SAMTextReader$RecordIterator.parseLine(SAMTextReader.java:268) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:255) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)Exception in thread “Thread-0” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Hashtable.(Hashtable.java:190) at java.util.Hashtable.(Hashtable.java:211) at java.util.Properties.(Properties.java:148) at java.util.Properties.(Properties.java:140) at java.util.logging.LogManager.reset(LogManager.java:1321) at java.util.logging.LogManager$Cleaner.run(LogManager.java:239)Exception in thread “Thread-1” java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:68) at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:227) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at htsjdk.samtools.util.nio.DeleteOnExitPathHook.runHooks(DeleteOnExitPathHook.java:57) at htsjdk.samtools.util.nio.DeleteOnExitPathHook$$Lambda$34/780934299.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)[Sat Feb 23 01:20:54 2019]Error in rule picard: jobid: 0 output: star_2pass/OV-9-2_rg_added_sorted.bam RuleException:CalledProcessError in line 115 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Command ‘set -euo pipefail; picard AddOrReplaceReadGroups I=star_2pass/OV-9-2Aligned.out.sam O=star_2pass/OV-9-2_rg_added_sorted.bam SO=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2’ returned non-zero exit status 1. File “/home02/qizhengyang/qizhengyang/gatk_rna/Snakefile”, line 115, in __rule_picard File “/home02/qizhengyang/anaconda3/lib/python3.6/concurrent/futures/thread.py”, line 56, in runRemoving output files of failed job picard since they might be corrupted:star_2pass/OV-9-2_rg_added_sorted.bamShutting down, this might take some time.Exiting because a job execution failed. Look above for error message(END) 123456snakemake -n --quietsnakemake --cluster "qsub -q low" --jobs 100pestat# 36 jobs, busyqstat -f | less 一个终端与服务器连接终端 Socket error Event: 32 Error: 10053.Connection closing…Socket close. Connection closed by foreign host. Disconnected from remote host(qizhengyang) at 11:29:56. Type `help’ to learn how to use Xshell prompt.[C:~]$ last 显示用户最近登录信息。top用于实时显示 process 的动态。 123456789snakemake -n -s Snakefile2 --quietmv Snakefile Snakefile_temp_1mv Snakefile1 Snakefile_temp_2mv Snakefile2 Snakefilesnakemake --unlocknohup snakemake --cluster "qsub -q low" --jobs 100 &amp;# 输出追加到"nohup.out"，nohup.out文件保留上次的信息 [qizhengyang@node1 gatk_rna]$ jobs[1]+ Running nohup snakemake –cluster “qsub -q low” –jobs 100 &amp;[qizhengyang@node1 gatk_rna]$ ps aux | grep snakemake | grep -v grep528 39349 0.5 0.0 289360 26440 pts/5 Sl 14:00 0:01 /home02/qizhengyang/anaconda3/bin/python3.6 /home02/qizhengyang/anaconda3/bin/snakemake –cluster qsub -q low –jobs 100 1ll -h nohup.out 一个 picard job 出现错误 Exception in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded Moreover, now i am running command like /usr/bin/java -Xmx40g -jar /usr/local/picard-tools-1.129/picard.jar …., and it’s working. Regards, Ravi. ava刚刚出现的年代，有一个相比于其他语言的优势就是，内存回收机制。不需要明确的调用释放内存的API，java就自动完成，这个过程就是Garbage Collection，简称GC。 其实还是有一个终极方法的，而且是治标治本的方法，就是找到占用内存大的地方，把代码优化了，就不会出现这个问题了。 Building DAG of jobs…Using shell: /bin/bashProvided cores: 96Rules claiming more threads will be scaled down.Job counts: count jobs 1 picard 1 [Sat Feb 23 14:00:10 2019]rule picard: input: star_2pass/HB-9-2Aligned.out.sam output: star_2pass/HB-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=HB-9-2 INFO 2019-02-23 14:00:12 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/HB-9-2Aligned.out.sam -O star_2pass/HB-9-2_rg_added_sorted.bam -SO coordinate -RGID HB-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM HB-9-2 14:00:12.937 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 14:00:13 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/HB-9-2Aligned.out.sam OUTPUT=star_2pass/HB-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=HB-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=HB-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 14:00:13 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 14:00:20 AddOrReplaceReadGroups Created read-group ID=HB-9-2 PL=illumina LB=rna SM=HB-9-2 INFO 2019-02-23 14:00:58 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:37s. Time for last 1,000,000: 37s. Last read position: chr1:10,583,000[Sat Feb 23 14:07:47 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 7.58 minutes.Runtime.totalMemory()=999292928To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.io.BufferedReader.readLine(BufferedReader.java:356) at java.io.LineNumberReader.readLine(LineNumberReader.java:201) at htsjdk.samtools.util.BufferedLineReader.readLine(BufferedLineReader.java:68) at htsjdk.samtools.SAMTextReader.advanceLine(SAMTextReader.java:221) at htsjdk.samtools.SAMTextReader.access$800(SAMTextReader.java:37) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:257) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)Exception in thread “Thread-1” java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:68) at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:227) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at htsjdk.samtools.util.nio.DeleteOnExitPathHook.runHooks(DeleteOnExitPathHook.java:57) at htsjdk.samtools.util.nio.DeleteOnExitPathHook$$Lambda$34/780934299.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)[Sat Feb 23 14:09:07 2019]Error in rule picard: jobid: 0 output: star_2pass/HB-9-2_rg_added_sorted.bam RuleException:CalledProcessError in line 115 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Command ‘set -euo pipefail; picard AddOrReplaceReadGroups I=star_2pass/HB-9-2Aligned.out.sam O=star_2pass/HB-9-2_rg_added_sorted.bam SO=coordinate RGID=HB-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=HB-9-2’ returned non-zero exit status 1. File “/home02/qizhengyang/qizhengyang/gatk_rna/Snakefile”, line 115, in __rule_picard File “/home02/qizhengyang/anaconda3/lib/python3.6/concurrent/futures/thread.py”, line 56, in runRemoving output files of failed job picard since they might be corrupted:star_2pass/HB-9-2_rg_added_sorted.bamShutting down, this might take some time.Exiting because a job execution failed. Look above for error message(END) [qizhengyang@node1 gatk_rna]$ snakemake -n –quietJob counts:count jobs1 all36 gatk36 gatk_filter16 gatk_split1 picard13 picard_markduplicat 12snakemake --unlocknohup snakemake --cluster &quot;qsub -q low&quot; --jobs 100 &amp; 解决办法 查看 picard.jar 的路径 设置环境变量 export picard=/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar， 写入 ~/.bash_profile，source 设置JVM最大可用内存为40g，java -Xmx40g -jar $picard。默认是1G [qizhengyang@node1 gatk_rna]$ find ~ -name picard.jar/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar/home02/qizhengyang/anaconda3/pkgs/picard-2.18.26-0/share/picard-2.18.26-0/picard.jar 查看当前环境变量 export | less 980879.node1 …b.gatk.202.sh qizhengyang 01:31:52 R low980880.node1 …b.gatk.182.sh qizhengyang 01:31:40 R low980881.node1 …b.gatk.204.sh qizhengyang 01:31:11 R low980882.node1 …b.gatk.209.sh qizhengyang 01:31:20 R low980884.node1 …b.gatk.214.sh qizhengyang 01:31:59 R low980888.node1 …b.gatk.187.sh qizhengyang 01:30:53 R low980889.node1 …b.gatk.210.sh qizhengyang 01:30:29 R low980891.node1 …b.gatk.186.sh qizhengyang 01:31:41 R low980892.node1 …b.gatk.216.sh qizhengyang 01:30:55 R low980893.node1 …b.gatk.206.sh qizhengyang 01:31:36 R low980894.node1 …b.gatk.192.sh qizhengyang 01:30:59 R low980896.node1 …b.gatk.188.sh qizhengyang 01:31:45 R low980898.node1 …b.gatk.215.sh qizhengyang 01:31:05 R low980899.node1 …_split.167.sh qizhengyang 06:39:52 R low980900.node1 …b.gatk.200.sh qizhengyang 01:31:05 R low980901.node1 …_split.153.sh qizhengyang 05:35:11 R low980902.node1 …b.gatk.201.sh qizhengyang 01:31:57 R low980903.node1 …b.gatk.195.sh qizhengyang 01:29:26 R low980904.node1 …b.gatk.197.sh qizhengyang 01:29:35 R low980905.node1 …b.gatk.185.sh qizhengyang 01:30:31 R low980909.node1 …_split.160.sh qizhengyang 05:36:38 R low980910.node1 …b.gatk.198.sh qizhengyang 01:29:47 R low980912.node1 …b.gatk.193.sh qizhengyang 01:30:22 R low980913.node1 …_split.155.sh qizhengyang 04:15:22 R low980914.node1 …_split.175.sh qizhengyang 04:04:21 R low980915.node1 …_split.172.sh qizhengyang 03:43:49 R low980916.node1 …_split.177.sh qizhengyang 03:22:15 R low980917.node1 …_split.147.sh qizhengyang 05:08:37 R low980918.node1 …_split.158.sh qizhengyang 05:14:32 R low980919.node1 …_split.176.sh qizhengyang 04:49:57 R low980920.node1 …_split.163.sh qizhengyang 04:37:11 R low980921.node1 …_split.171.sh qizhengyang 04:17:19 R low980922.node1 …_split.154.sh qizhengyang 04:05:50 R low980923.node1 …_split.148.sh qizhengyang 03:46:37 R low980925.node1 …_split.181.sh qizhengyang 03:25:10 R low980926.node1 …_split.169.sh qizhengyang 02:20:52 R low 任务完成 [Sun Feb 24 23:55:12 2019]Finished job 0.103 of 103 steps (100%) doneComplete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-23T214006.459737.snakemake.log 最终的Snakefile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186# 2019-2-19 10:28:52 part# GATK snakemake# qizhengyangfrom os.path import joinGENOME = 'genome/HWB.chromosome.fa'GTF = 'genes/HWB.gene.models.gtf'(SAMPLES,) = glob_wildcards('pairedDIR/&#123;sample&#125;_1P.fq.gz')PATTERN_R1 = join('pairedDIR', '&#123;sample&#125;_1P.fq.gz')PATTERN_R2 = join('pairedDIR', '&#123;sample&#125;_2P.fq.gz')rule all: input: 'star_index_2pass/', expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES), 'star_index_2pass/', expand('star_2pass/&#123;sample&#125;Aligned.out.sam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_rg_added_sorted.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup_split.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;.vcf', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_filtered.vcf', sample=SAMPLES)rule star_index: input: genome = GENOME, gtf = GTF output: star_index = directory('star_index/') log: 'star_index.log' threads: 20 run: # star 1-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.star_index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbGTFfile &#123;input.gtf&#125;' ' 2&gt; &#123;log&#125;')rule star_1pass_align: input: index = 'star_index/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: index = 'star_1pass/&#123;sample&#125;SJ.out.tab' threads: 20 params: prefix = './star_1pass/&#123;sample&#125;' # 在使用params之前是报错的，NameError,The name 'sample' is unknown in this context run: # star 1-pass align, OK shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule star_2pass_index: input: genome = GENOME, # 这里必需加expand，不然会报错：Wildcards in input files cannot be determined from output files: # 'sample'。 # 很奇怪，确实是需要所有样本的剪接位点信息，我之前没有注意到。。感谢报错 # 然后用--sjdbFileChrStartEnd参数将所有样品的SJ.out.tab文件作为输入的annotated junction进行第二次建index # http://www.bioinfo-scrounger.com/archives/288 # 这里不能加三个引号（"""或'''注释） splice_site = expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES) output: index = directory('star_index_2pass/') threads: 20 run: # star 2-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbFileChrStartEnd &#123;input.splice_site&#125;')rule star_2pass_align: input: index = 'star_index_2pass/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' threads: 20 params: prefix = 'star_2pass/&#123;sample&#125;' run: # star 2-pass align shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule picard: input: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' output: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' run: # RGID和RGSM的sample必须是&#123;wildcards.sample&#125;，不然 # The name 'sample' is unknown in this context. Please make sure that you defined that variable. # Also note that braces not used for variable access have to be escaped by repeating them, i.e. &#123;&#123;print $1&#125;&#125; shell('java -Xmx40g -jar $picard AddOrReplaceReadGroups' ' I=&#123;input.sam&#125;' ' O=&#123;output.bam&#125;' ' SO=coordinate' ' RGID=&#123;wildcards.sample&#125;' ' RGLB=rna' ' RGPL=illumina' ' RGPU=hiseq' ' RGSM=&#123;wildcards.sample&#125;')rule picard_markduplicates: input: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' output: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam' params: dedup_metrices = 'star_2pass/&#123;sample&#125;_dedup.metrics' run: shell('java -Xmx40g -jar $picard MarkDuplicates' ' I=&#123;input.bam&#125;' ' O=&#123;output.dedup_bam&#125;' ' CREATE_INDEX=true' ' VALIDATION_STRINGENCY=SILENT' ' M=&#123;params.dedup_metrices&#125;')rule gatk_split: input: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam', genome = GENOME output: split_bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam' run: shell('java -Xmx10g -jar $GATK -T SplitNCigarReads' ' -R &#123;input.genome&#125;' ' -I &#123;input.dedup_bam&#125;' ' -o &#123;output.split_bam&#125;' ' -rf ReassignOneMappingQuality' ' -RMQF 255' ' -RMQT 60' ' -U ALLOW_N_CIGAR_READS')rule gatk: input: bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam', genome = GENOME output: vcf = 'star_2pass/&#123;sample&#125;.vcf' run: shell('java -Xmx10g -jar $GATK -T HaplotypeCaller' ' -R &#123;input.genome&#125;' ' -I &#123;input.bam&#125;' ' -dontUseSoftClippedBases' ' -stand_call_conf 20.0' ' -o &#123;output.vcf&#125;')rule gatk_filter: input: genome = GENOME, vcf = 'star_2pass/&#123;sample&#125;.vcf' output: 'star_2pass/&#123;sample&#125;_filtered.vcf' run: shell('java -Xmx10g -jar $GATK ' ' -T VariantFiltration' ' -R &#123;input.genome&#125;' ' -V &#123;input.vcf&#125;' ' -window 35' ' -cluster 3' ' -filterName FS -filter "FS &gt; 30.0"' ' -filterName QD -filter "QD &lt; 2.0"' ' -o &#123;output&#125;') 参考资料 利用snakemake搭建流程简明教程 snakemake docs RNAseq variant calling pipeline Build bioinformatics pipelines with Snakemake kallisto-snakefiles Writing a RNA-Seq workflow with snakemake snakemake初步 RNA-seq 检测变异之 GATK 最佳实践流程 snakemake-example 我最喜欢的流程管理工具]]></content>
      <tags>
        <tag>生物信息</tag>
        <tag>snakemake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[the machine stops]]></title>
    <url>%2F2019%2F02%2F11%2Fthe-machine-stops%2F</url>
    <content type="text"><![CDATA[从工业革命开始，城市化，信息化，智能化，人们的生活发生了不可逆转的剧变。有些东西消失后就不会再出现，比如蒸汽机车，有些东西即使外界再变也不会消失，比如文明。 they have given up, to a great extent, the amenities and achievements of civilization: solitude and leisure, the sanction to be oneself, truly absorbed, whether in contemplating a work of art, a scientific theory, a sunset, or the face of one’s beloved.]]></content>
  </entry>
  <entry>
    <title><![CDATA[甲基化数据分析]]></title>
    <url>%2F2019%2F01%2F30%2F%E7%94%B2%E5%9F%BA%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[DNA甲基化数据分析流程 用Trimmomatic去除低质量序列（q&lt;20），接头。 用BSMAP比对，允许0.8的错配率。 用methratio.py提取甲基化比例，选项-r去除PCR重复。 获得DMR 为了得到可靠的DMR区域，合并两个生物学重复，仅考虑所有文库中深度至少为4的胞嘧啶。 使用200bp窗口（50bp步长）识别DMR。 对每个窗口内的甲基化和未甲基化胞嘧啶进行Fisher精确检验。使用Benjamini-Hochberg对p值进行调整，估计错误发生率（FDR）。 FDR&lt;0.01，甲基化水平变化大于1.5倍且至少含有5个差异甲基化胞嘧啶（DMCs：Fisher精确检验中p&lt;0.01）的窗口用于进一步分析，窗口在100bp内合并为更大的区域。 RNA-seq 数据分析 Trimmomatic去除低质量序列和接头 用STAR进行比对，–sjdbGTFfile 用于提供基因组注释文件 htseq-count计算每个基因map上的片段数 DESeq2计算差异表达基因 DMR-associated基因分析 DMR相关基因定义为2kb启动子区域内具有DMR的基因 仅用DMR-associated genes进行基因聚类 差异基因定义，FPKM &gt;= 1, FDR &lt;= 0.01, fc &gt;= 1.5 GO注释 使用拟南芥（TAIR10）、番茄（ITAG3）、草莓（PhytozomeV12）的蛋白序列和GO注释文件 blast 用GOATOOLs进行GO富集 小RNA分析 使用BWA比对 计算DMR的24 nt 小RNA丰度（normalized to per million per one hundred base pair）]]></content>
      <tags>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql使用过程中遇到的一些问题]]></title>
    <url>%2F2019%2F01%2F29%2FMysql%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%2F</url>
    <content type="text"><![CDATA[blast2go本地化需要用到mysql，操作系统Centos 7。 1. 卸载之前的mysql之前的版本是mysql5.6，想要更新到mysql5.7。 查看mysql安装了哪些东西。 1rpm -qa |grep -i mysql 开始卸载 1yum remove XXX 查看卸载是否完成 查找mysql相关目录 1find / -name mysql 删除相关目录 123rm -rf XXXrm -rf /etc/my.cnfrm -rf rm -rf /var/log/mysqld.log 2. 安装Mysql5.7 安装mysql源 1234wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpmyum localinstall mysql57-community-release-el7-11.noarch.rpm# 检查是否安装成功yum repolist enabled | grep &quot;mysql.*-community.*&quot; 启动mysql服务 123systemctl start mysqld# 查看状态systemctl status mysqld 这里可能会遇到 “Another mysqld server running on port 3306 error”，可以采用netstat -lp | grep 3306 查找占用这个端口号的进程，kill 这个PID。（我发现占用这个3306的是mysql??）。 修改root密码 我用root身份导入数据的时候，提示我要修改密码。生成的默认密码在 /var/log/mysqld.log 文件中。使用 grep 命令找到日志中的密码。 1grep &apos;temporary password&apos; /var/log/mysqld.log 1mysql&gt;ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123,c%vPl9ek&apos;; 或者： 123mysql&gt; use mysql;mysql&gt; update mysql.user set authentication_string=password(&apos;123,c%vPl9ek&apos;) where user=&apos;root&apos;;mysql&gt; flush privileges; 3. 导入数据1mysql -u root -p &lt; b2gdb.sql 会出现莫名其妙的错误，“ERROR 1819 (HY000) at line 4: Your password does not satisfy the current policy requirements”，但是我的密码是符合它的规则的。最后采取的办法是把检验密码的插件删了。 1mysql&gt;uninstall plugin validate_password; 有试着导入数据，这次的错误是： “ERROR 1101 (42000) at line 9: BLOB, TEXT, GEOMETRY or JSON column ‘description’ can’t have a default value” 先查看了sql_mode 12mysql&gt; select @@session.sql_mode;mysql&gt; select @@global.sql_mode; 之后重新设置sql_mode 12set sql_mode=&apos;&apos;;set global sql_mode=&apos;&apos;; 重新打开一个终端，进入mysql，查看sql_mode 1select @@global.sql_mode; 在尝试导入数据，成功。 4. blast2go123456cd data1/data/blast2go/mysql -u root -p &lt; b2gdb.sqlmysql -u root -p -e &quot;GRANT ALL ON b2gdb.* TO &apos;blast2go&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;blast4it&apos;;&quot;mysql -u root -p -e &quot;FLUSH PRIVILEGES;&quot;# 这一步很耗时nohup time mysql -s -u root -p b2gdb &lt; go_monthly-assocdb-data &gt; mysql.out 2&gt;&amp;1 &amp; 5. 参考 MySQL sql_mode 说明 blast2go本地化教程 your password does not satisfy the current policy requirements another mysqld server running on the port 3306 Centos7 完全卸载mysql CentOS 7 下 MySQL 5.7 的安装与配置]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建立本地nr数据库]]></title>
    <url>%2F2019%2F01%2F28%2Fblast2go%2F</url>
    <content type="text"><![CDATA[1234567891011# 下载nr数据库，并解压# 安装nt/nr库需要进行环境变量配置，在家目录下新建一个.ncbirc配置文件nohup time update_blastdb.pl --decompress nr &gt; out.log 2&gt;&amp;1 &amp;# 同一目录下建立nr_plant子库blastdb_aliastool -seqidlist sequence.seq -db nr -out nr_plant -title nr_plant# 运行时间相当长blastp -query AT-ARR.fa -db nr -outfmt 11 -out ARR.blastp@nr.asn -num_threads 8# 测试时间time blastp -query ~/qizhengyang/AT-ARR.fa -db nr_plant -outfmt 11 -out ARR.blastp@nr.asn -num_threads 8]]></content>
      <tags>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器安装 CentOS 7 并开启ssh服务之后]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85-CentOS-7-%E5%B9%B6%E5%BC%80%E5%90%AFssh%E6%9C%8D%E5%8A%A1%E4%B9%8B%E5%90%8E%2F</url>
    <content type="text"><![CDATA[挂载服务器里的另外两块硬盘硬盘 查看没有挂载的硬盘 fdisk -l 格式化硬盘，mkfs.ext4 /dev/sdb.报错，/dev/sdb is apparently in use by the system; will not make a 文件系统 here!。用这个命令：dmsetup remove_all mkfs.ext4 /dev/sdb 挂载 mount /dev/sdb /data mount挂载ntfs 格式的移动硬盘 1234成功执行mount -t ntfs-3g /dev/sdd1 /run/media/qi/Zhangmiao_15623277907未成功执行mount -t ntfs-3g /dev/sdb5 /run/media/qi/软件 自动挂载设置 Ps:在使用gnome桌面挂载U盘的时候发现，系统可以识别NTFS 分区的存在，但是通过桌面无法自动挂载，系统会提示： Error mounting /dev/sdb1 at /run/media/lenovo/v220w: Command-line `mount -t “ntfs” -o “uhelper=udisks2,nodev,nosuid,uid=1000,gid=1000,dmask=0077,fmask=0177” “/dev/sdb1” “/run/media/lenovo/v220w”‘ exited with non-zero exit status 32: mount: unknown filesystem type ‘ntfs’ mount 提示未知的文件系统类型 ‘ntfs’ 解决办法： $ mount[Tab][Tab] #连续按两次 Tab 键作命令补齐 mount mount.glusterfs mount.nfs4 mountstatsmount.cifs mount.lowntfs-3g mount.ntfs-3gmount.fuse mount.nfs mountpoint 可以看到只有mount.ntfs-3g，在使用 mount -t 挂载ntfs 时 mount 会调用 mount.ntfs-3g 而非默认的 mount.ntfs $ locate mount.ntfs-3g #查找有关文件所在位置/usr/local/share/man/man8/mount.ntfs-3g.8/usr/sbin/mount.ntfs-3g$ sudo ln -s /usr/sbin/mount.ntfs-3g /usr/sbin/mount.ntfs #创建软链接 之后就可以自动识别U 盘而不会出现如上报错了。。。 centOS7 将“桌面”、“图片”等替换成英文 export LANG=en_US xdg-user-dirs-gtk-update 弹出配置界面，选择替换 export LANG=zh_CN.UTF-8 重启，不替换。 IP变化 wlp0s26f7u3:inet 192.168.1.102/24发现只有连同一个wifi(Magic-home)的电脑才能登入。 原来的IP：4: wlp0s26f7u3:inet 10.164.11.166]]></content>
      <tags>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R输出图形]]></title>
    <url>%2F2018%2F12%2F12%2Fexporting-nice-plots-in-r%2F</url>
    <content type="text"><![CDATA[这篇博文写的很好 https://www.r-bloggers.com/exporting-nice-plots-in-r/介绍的 Inkscape 也很好用，解决了之前IGV输出SVG图片的问题。导入PPT只显示一半。用Ai cc 打开，报错。“往返Tiny时剪贴将丢失”，只有框架没有图 There are two main problems when exporting graphics from R: Anti-aliasing is not activated in Windows R (this does not apply to Linux or Mac) – windows下没有反锯齿设置，解决：library(Cairo) When increasing the resolution the labels automatically decrease and become unreadable –分辨率升高，标签会自动缩小 If we want to increase the resolution of the plot we can’t just change the resolution parameter:We also have to change the point size, the formula is size * new resolution DPI / 72 DPI: If we double the image size we also need to double the point size: 123456789101112131415161718192021222324# https://www.r-graph-gallery.com/265-grouped-boxplot-with-ggplot2/# exporting nice plot https://www.r-bloggers.com/exporting-nice-plots-in-r/# librarylibrary(ggplot2) # create a data framevariety=rep(LETTERS[1:7], each=40)treatment=rep(c("high","low"),each=20)note=seq(1:280)+sample(1:150, 280, replace=T)data=data.frame(variety, treatment , note) # grouped boxplotggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot()# One box per treatmentggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot() + facet_wrap(~treatment)# one box per varietyggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot() + facet_wrap(~variety, scale="free")]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google news "epigenetics plants"]]></title>
    <url>%2F2018%2F12%2F10%2FGoogle-News%2F</url>
    <content type="text"><![CDATA[University of Nebraska-Lincoln researchers have found revolutionary evidence that an evolutionary phenomenon at work in complex organisms is at play in their single-celled, extreme-loving counterparts, too.]]></content>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群上安装DESeq2遇到的困难及解决办法]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85DESeq2%E9%81%87%E5%88%B0%E7%9A%84%E5%9B%B0%E9%9A%BE%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[BiocManager::install(“DESeq2”, version = “3.8”)报错，错误内容：configure: WARNING: Only g++ version 4.7.2 or greater can be used with RcppArmadillo.configure: error: Please use a different compiler.ERROR: configuration failed for package ‘RcppArmadillo’ 源码安装gcc参考../configure –prefix=/home02/qizhengyang/packages -enable-checking=release -enable-languages=c,c++ -disable-multilib报错，错误内容：/usr/bin/ld: cannot find -lgfortran 重新安装gcc，增加fortran../configure –prefix=/home02/qizhengyang/packages -enable-checking=release -enable-languages=c,c++,fortran -disable-multilib &amp;&amp; make -j4 &amp;&amp; make install 添加两个libexport LD_LIBRARY_PATH=$HOME/packages/lib64:$LD_LIBRARY_PATHexport LD_LIBRARY_PATH=$HOME/packages/lib:$LD_LIBRARY_PATH BiocManager::install(“DESeq2”, version = “3.8”)成功 备注：configure时间超级长。过程艰辛。还尝试过手动安装RcppArmadillo，失败。.libPaths()“/home02/qizhengyang/packages/R/lib64/R/library”R CMD INSTALL -l /home02/qizhengyang/packages/R/lib64/R/library RcppArmadillo_0.3.6.3.tar.gz]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群上安装的R包]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85%E7%9A%84R%E5%8C%85%2F</url>
    <content type="text"><![CDATA[写了一个R script批量安装。1234567install.packages("devtools",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")install.packages("dplyr",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")install.packages("BiocManager",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")BiocManager::install("ballgown", version = "3.8")BiocManager::install(genefilter", version = "3.8")devtools::install_github('alyssafrazee/RSkittleBrewer')BiocManager::install("methylKit", version = "3.8")]]></content>
  </entry>
  <entry>
    <title><![CDATA[集群上更新R并安装methylKit]]></title>
    <url>%2F2018%2F10%2F31%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85R%2F</url>
    <content type="text"><![CDATA[普通用户在集群上安装R主要的教程里面有一些需要注意的，安装bzip，要加入一个 -fPIC 到 CFLAG 。（在 Makefile中CFLAG=-fPIC -Wall -Winline -O2 -g…）在R安装configure之前需要注意设置环境变量123LD_LIBRARY_PATH=/home02/qizhengyang/packages/libexport CFLAGS="-I/home02/qizhengyang/packages/include"export LDFLAGS="-L/home02/qizhengyang/packages/lib" R安装完成之后123export R_HOME=$HOME/packages/Rexport R_LIBS=$HOME/packages/R/lib64/libraryexport PATH="$R_HOME/bin":$PATH 最后 source ~/.bashrc 装methyKit也是坑一堆。我下了autoconf-2.69，libxml2-2.7.2。源码安装。./configure –prefix=/home02/qizhengyang/packages]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《局外人》的主旨]]></title>
    <url>%2F2018%2F08%2F17%2F%E5%B1%80%E5%A4%96%E4%BA%BA%E4%B8%BB%E6%97%A8%2F</url>
    <content type="text"><![CDATA[1937年8月一个男人，在人们通常视为人生大事的地方（婚姻、社会地位等等）寻找人生，然后某天在翻阅一本时装目录的时候，突然了解到他对自己人生亦即时装目录上鼓吹的那种人生是何其无所谓。]]></content>
  </entry>
  <entry>
    <title><![CDATA[《非洲的青山》]]></title>
    <url>%2F2018%2F07%2F14%2F%E9%9D%9E%E6%B4%B2%E7%9A%84%E9%9D%92%E5%B1%B1%2F</url>
    <content type="text"><![CDATA[第四章，“乔伊斯中等身高，他把眼睛用坏了。在那最后一晚，喝醉了，和乔伊斯在一起，他不断引用埃德加·基内的话：‘思维清晰、生命绚丽如战争时一样。’我知道我没有把这句话彻底弄清楚。等你见到他，他会提到三年前被打断的谈话。能见到我们这个时代伟大的作家真让人高兴。”这段话越发让我觉得我看的不是一部小说。 查了维基百科 Green Hills of Africa is a 1935 work of nonfiction by American writer Ernest Hemingway.]]></content>
  </entry>
</search>
