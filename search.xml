<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[GATK calling variants in RNA-seq using snakemake framework]]></title>
    <url>%2F2019%2F02%2F20%2Fsnakemake%2F</url>
    <content type="text"><![CDATA[GATK calling variants in RNA-seq using snakemake framework变异检测流程 检查md5值12md5sum -c checksums.md5# 报错,使用dos2unix转化格式 trimmomatic1234567# 2019-2-12 17:16:35 trimfor i in *_1.clean.fq.gzdo trimmomatic PE -threads 8 -phred33 $i $&#123;i/_1/_2&#125; -baseout $&#123;i:0:7&#125;.fq.gz \ ILLUMINACLIP:/home/qi/miniconda3/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 \ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done 2019-2-13 12:55:38 查看结果，只trim了一部分，查看trim.log。HB-9-3终止。 ps -aux | grep trimmomatic 查看任务是否还在运行。没有在运行。程序不知什么原因意外中断，来的时候xshell也没有连上服务器，怀疑是服务器自动重启了。 12345678910nohup mv HB-*.fq.gz trim &amp;rm nohup.outcd trimrm HB-9-3__*fq.gzmv HB-9-3* ..cd ..# trim剩下的fq.gznohup time bash trim.sh &gt; trim.log 2&gt;&amp;1 &amp;jobsps -aux | grep trimmomatic 2019-2-13 17:06:38 查看任务，意外终止。NH-11-2 移动、删除文件，reboot，重新执行脚本。 1nohup time bash trim.sh &gt;&gt; trim.log 2&gt;&amp;1 &amp; 2019-2-13 19:35:51 查看，又意外停止 查看系统最后重启时间 1who -b 显然，服务器会自动重启。 不运行任务的时候，不会重启。是因为CPU温度过高？ 更改脚本试一试，把-threads 8 改成 4 123456for i in *_1.clean.fq.gzdo trimmomatic PE -threads 4 -phred33 $i $&#123;i/_1/_2&#125; -baseout $&#123;i:0:7&#125;.fq.gz \ ILLUMINACLIP:/home/qi/miniconda3/share/trimmomatic-0.38-1/adapters/TruSeq3-PE.fa:2:30:10 \ LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done 1nohup time bash trim.sh &gt; trim.log 2&gt;&amp;1 &amp; 没有效果 threads改成16 一小时不到又重启了。。 2019-2-14 16:16:57 质检1fastqc *.fq.gz -o qcDIR/ -t 6 -d qcDIR/ &gt;&gt;fastqc.r.log 2&gt;&gt;fastqc.e.log 1conda install -c bioconda multiqc 做multiqc之前构建一个python3的虚拟环境 1234567891011python --versionconda create --name py3.6 python=3.6source activate py3.6# You'll want to add the source activate py3.6 line to your .bashrc file so that the environment is loaded every time you load the terminal.# conda deactivate# conda activate py3.6# Windows: activate py3.6# 需要重新安装conda install -c bioconda multiqc 123#利用multiqc整合结果，方便批量查看mkdir qcDIR_multiqcmultiqc qcDIR/ -o qcDIR_multiqc/ 用 snakemake 编写任务流程snakemake是一个用来编写任务流程的工具，用python写的，因此其执行的流程脚本也比较通俗易懂，易于理解，可以看做用户友好版的make。（make在安装软件的时候会用到，没有研究过makefile文件，对它的用处不是太了解。） 其实流程控制是复杂任务（在生信领域很常见）必需的关注点。只是snakemake对于代码功力不够的人来说，在写好代码与重复流程的花销的trade off上，还不如一遍遍重复流程。。但是真的是一个写好了就很好用的东西。 snakemake能够使用文件名通配的方式对一类文件进行处理 12# installingconda install -c bioconda snakemake 简单的例子12345678910111213cd $HOME# Create a folder where we will run our commands:mkdir snakemake-examplecd snakemake-example# Make a fake genome:touch genome.fa# Make some fake data:mkdir fastqtouch fastq/Sample1.R1.fastq.gz fastq/Sample1.R2.fastq.gztouch fastq/Sample2.R1.fastq.gz fastq/Sample2.R2.fastq.gz snakemake 脚本 123456789101112131415SAMPLES = ['Sample1', 'Sample2']rule all: input: expand('&#123;sample&#125;.txt', sample=SAMPLES)rule quantify_genes: input: genome = 'genome.fa', r1 = 'fastq/&#123;sample&#125;.R1.fastq.gz', r2 = 'fastq/&#123;sample&#125;.R2.fastq.gz' output: '&#123;sample&#125;.txt' shell: 'echo &#123;input.genome&#125; &#123;input.r1&#125; &#123;input.r2&#125; &gt; &#123;output&#125;' 学院集群上运行 1snakemake --snakefile Snakefile 12# 可视化snakemake --forceall --dag | dot -Tpng &gt; dag1.png snakemake 规则 Snakemake基于规则执行命令，规则一般由input, output,shell三部分组成。除了rule all，其余必须有output Snakemake可以自动确定不同规则的输入输出的依赖关系，根据时间戳来判断文件是否需要重新生成 Snakemake以{sample}.fa形式进行文件名通配，用{wildcards.sample}获取sample的实际文件名 Snakemake用expand()生成多个文件名，本质是Python的列表推导式 Snakemake可以在规则外直接写Python代码，在规则内的run里也可以写Python代码。 Snakefile的第一个规则通常是rule all，根据all里的文件决定执行哪些rule。如上面的例子，注释掉all里的input则不执行第二条rule，（推断未尝试：rule all里定义最终的输出文件，程序也能执行，那么rule里的输出文件在什么时候会被删除，是在所有rule运行完之后，还是在判断出该输出文件不会被用到的时候？） 在output中的结果文件可以是未存在目录中的文件,这时会自动创建不存在的目录（不需要事先建文件夹，这个功能实在是方便） snakemake 命令 wildcards: 用来获取通配符匹配到的部分，例如对于通配符”{dataset}/file.{group}.txt”匹配到文件101/file.A.txt，则{wildcards.dataset}就是101，{wildcards.group}就是A。 temp: 通过temp方法可以在所有rule运行完后删除指定的中间文件，eg.output: temp(“f1.bam”)。 protected: 用来指定某些中间文件是需要保留的，eg.output: protected(“f1.bam”)。 snakemake 执行一般讲所有的参数配置写入Snakefile后直接在Snakefile所在路径执行snakemake命令即可开始执行流程任务，如果只有一个snakefile的话，连文件都不用写。一些常用的参数： 123456789101112131415161718--snakefile, -s 指定Snakefile，否则是当前目录下的Snakefile--dryrun, -n 不真正执行，一般用来查看Snakefile是否有错--printshellcmds, -p 输出要执行的shell命令--reason, -r 输出每条rule执行的原因,默认FALSE--cores, --jobs, -j 指定运行的核数，若不指定，则使用最大的核数--force, -f 重新运行第一条rule或指定的rule--forceall, -F 重新运行所有的rule，不管是否已经有输出结果--forcerun, -R 重新执行Snakefile，当更新了rule时候使用此命令#一些可视化命令$ snakemake --dag | dot -Tpdf &gt; dag.pdf#集群投递snakemake --cluster "qsub -V -cwd -q 节点队列" -j 10# --cluster: 集群运行指令# qusb -V -cwd -q， 表示输出当前环境变量(-V),在当前目录下运行(-cwd), 投递到指定的队列(-q), 如果不指定则使用任何可用队列# --local-cores N: 在每个集群中最多并行N核# --cluster-config/-u FILE: 集群配置文件 snakemake 实践123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187# 2019-2-19 10:28:52 part# GATK snakemake# qizhengyangfrom os.path import joinGENOME = 'genome/HWB.chromosome.fa'GTF = 'genes/HWB.gene.models.gtf'(SAMPLES,) = glob_wildcards('pairedDIR/&#123;sample&#125;_1P.fq.gz')PATTERN_R1 = join('pairedDIR', '&#123;sample&#125;_1P.fq.gz')PATTERN_R2 = join('pairedDIR', '&#123;sample&#125;_2P.fq.gz')rule all: input: 'star_index_2pass/', expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES), 'star_index_2pass/', expand('star_2pass/&#123;sample&#125;Aligned.out.sam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_rg_added_sorted.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup_split.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;.vcf', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_filtered.vcf', sample=SAMPLES)rule star_index: input: genome = GENOME, gtf = GTF output: # 最后加上 directory()，不然在集群上运行会报错 star_index = directory('star_index/') log: 'star_index.log' threads: 8 run: # star 1-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.star_index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbGTFfile &#123;input.gtf&#125;' ' 2&gt; &#123;log&#125;')rule star_1pass_align: input: index = 'star_index/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: index = 'star_1pass/&#123;sample&#125;SJ.out.tab' threads: 8 params: prefix = './star_1pass/&#123;sample&#125;' # 在使用params之前是报错的，NameError,The name 'sample' is unknown in this context run: # star 1-pass align, OK shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule star_2pass_index: input: genome = GENOME, # 这里必需加expand，不然会报错：Wildcards in input files cannot be determined from output files: # 'sample'。 # 报错信息说通配的信息不能从output里推断出来，因为我的output是文件夹。input应该是所有样品的信息，可以用expand函数，这样通配的问题就没有了。 # 然后用--sjdbFileChrStartEnd参数将所有样品的SJ.out.tab文件作为输入的annotated junction进行第二次建index # http://www.bioinfo-scrounger.com/archives/288 # 不能加三个引号（"""或'''注释）进行段落注释 splice_site = expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES) output: index = directory('star_index_2pass/') threads: 8 run: # star 2-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbFileChrStartEnd &#123;input.splice_site&#125;')rule star_2pass_align: input: index = 'star_index_2pass/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' threads: 8 params: prefix = 'star_2pass/&#123;sample&#125;' run: # star 2-pass align shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule picard: input: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' output: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' run: # RGID和RGSM的sample必须是&#123;wildcards.sample&#125;，不然 # The name 'sample' is unknown in this context. Please make sure that you defined that variable. # Also note that braces not used for variable access have to be escaped by repeating them, i.e. &#123;&#123;print $1&#125;&#125; shell('picard AddOrReplaceReadGroups' ' I=&#123;input.sam&#125;' ' O=&#123;output.bam&#125;' ' SO=coordinate' ' RGID=&#123;wildcards.sample&#125;' ' RGLB=rna' ' RGPL=illumina' ' RGPU=hiseq' ' RGSM=&#123;wildcards.sample&#125;')rule picard_markduplicates: input: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' output: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam' params: dedup_metrices = 'star_2pass/&#123;sample&#125;_dedup.metrics' run: shell('picard MarkDuplicates' ' I=&#123;input.bam&#125;' ' O=&#123;output.dedup_bam&#125;' ' CREATE_INDEX=true' ' VALIDATION_STRINGENCY=SILENT' ' M=&#123;params.dedup_metrices&#125;')rule gatk_split: input: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam', genome = GENOME output: split_bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam' run: shell('java -jar $GATK -T SplitNCigarReads' ' -R &#123;input.genome&#125;' ' -I &#123;input.dedup_bam&#125;' ' -o &#123;output.split_bam&#125;' ' -rf ReassignOneMappingQuality' ' -RMQF 255' ' -RMQT 60' ' -U ALLOW_N_CIGAR_READS')rule gatk: input: bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam', genome = GENOME output: vcf = 'star_2pass/&#123;sample&#125;.vcf' run: shell('java -jar $GATK -T HaplotypeCaller' ' -R &#123;input.genome&#125;' ' -I &#123;input.bam&#125;' ' -dontUseSoftClippedBases' ' -stand_call_conf 20.0' ' -o &#123;output.vcf&#125;')rule gatk_filter: input: genome = GENOME, vcf = 'star_2pass/&#123;sample&#125;.vcf' output: 'star_2pass/&#123;sample&#125;_filtered.vcf' run: shell('java -jar $GATK' ' -T VariantFiltration' ' -R &#123;input.genome&#125;' ' -V &#123;input.vcf&#125;' ' -window 35' ' -cluster 3' ' -filterName FS -filter "FS &gt; 30.0"' ' -filterName QD -filter "QD &lt; 2.0"' ' -o &#123;output&#125;') 实验室服务器试运行 By default snakemake executes the first rule in the snakefile. output中会自动创建没有的文件夹 12# cores设置核心数snakemake -s part.py --cores 8 跑samples中的所有样品 存储空间不够，停止 重新运行之后，是从NH-12-1开始 之前的结果只有NH-12-1Log.out，时间是2019/2/20 3:35 现在是： 说明这个样本从头开始跑了。 集群上运行12345678910111213141516# 查看节点状态pestat# 测试 dry runsnakemake -n# 检测某条rulesnakemake -n -r star_index# 可视化# snakemake --dag | dot -Tpdf &gt; dag.pdfsnakemake --forceall --dag | dot -Tpdf &gt; dag.pdf# # 投递任务 -cwd不能用 [qsub: illegal -c value]# snakemake --cluster "qsub -V -cwd -q high" -j 20snakemake --cluster "qsub -V -q high" -j 20# 报错# Complete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-20T152054.823542.snakemake.log 12# 查看错误信息 输出目录要用 directory() ，在实验室服务器上没有遇上这种情况less snakejob.star_index.254.sh.e980069 ImproperOutputException in line 30 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Outputs of incorrect type (directories when expecting files or vice versa). Output directories must be flagged with directory(). for rule star_index:star_index/Removing output files of failed job star_index since they might be corrupted:star_index/Skipped removing non-empty directory star_index/Shutting down, this might take some time. 12345snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20# -j 必须指定，否则Error: you need to specify the maximum number of jobs to be queued or executed at the same time with --jobs.# Here, -j denotes the number of jobs submitted being submitted to the cluster at the same time# -j 代表可以同时提交的任务数# 没有重新运行star_index这个任务 2019-2-20 20:22:01 错误： 1less snakejob.star_1pass_align.28.sh.e980101 Possible cause 1: not enough RAM. Check if you have enough RAM 6975932175 bytes 123# 剩下的三个重新跑 snakemake -nsnakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 没有安装picard。。这种低级错误千万不能再犯了 12conda install -c bioconda picard snakemake -n 终端意外退出，输出到屏幕上的信息没有了 123456789ls -acd .snakemake/log# 按修改时间查看文件# l长格式显示，t按时间排序（-r升序）ls -lth# snakemake程序停止# 已经提交的任务会运行完snakemake -n --quiet 1snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 Error: Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory 123# 解决办法 https://zhuanlan.zhihu.com/p/47575136snakemake --unlocksnakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 报错 1less snakejob.star_2pass_align.68.sh.e980306 libgcc_s.so.1 must be installed for pthread_cancel to work 123less snakejob.star_2pass_align.68.sh.e980306 # 会自动删除失败任务的输出文件# Removing output files of failed job star_2pass_align since they might be corrupted: 短暂的断网，与服务器连接中断 12snakemake -n# 报错，因为已经提交的任务没有运行完 nohup将程序放入后台12whatis nohupman 1 nohup If standard input is a terminal, redirect it from /dev/null. If standard output is a terminal, append output to ‘nohup.out’ if possible, ‘$HOME/nohup.out’ otherwise. If standard error is a terminal, redirect it to standard output. To save output to FILE, use ‘nohup COMMAND &gt; FILE’. 重定向与后台运行的知识 12345678nohup snakemake -n &amp;# [qizhengyang@node1 gatk_rna]$ nohup: 忽略输入并把输出追加到"nohup.out"nohup snakemake -n &gt; test_nohup.out 2&gt;&amp;1 &amp;# 标准输出和标准错误输出都重定向到文件snakemake -n &gt; test_nohup.out 2&gt;&amp;1 &amp;# 会产生进程号 12# nohup方式后台运行(&amp;) 忽略所有发送给子命令的挂断（SIGHUP）信号 重定向子命令的标准输出(stdout)和标准错误(stderr)nohup snakemake -s Snakefile1 --cluster "qsub -V -q high" -j 20 &gt; snakemake.out 2&gt;&amp;1 &amp; IP 意外变化2019-2-21 16:45:01 实验室的服务器连不上，我查看了ssh server是否开启，查了IP，发现是IP变了。 重新开始2019-2-21 21:33:58查看了实验室服务器生的Index文件夹，比较了大小，之前第一步需要重做。。很悲惨 gatk加了 -Xmx10g -jar # Decrease Java heap size (-Xmx/-Xms) 123snakemake -n -s Snakefile2 --quiet# 调了最大并行数，因为会有内存不够的错误nohup snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 10 &amp;&gt; snakemake.out &amp; 1snakemake --unlock 之后再运行命令 12345678nohup snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 10 &amp;&gt; snakemake.out &amp;# snakemake.out内容重新写入# 查看任务jobs# 或ps aux|grep Snakefile2|grep -v grep# https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/ps.html 2019-2-22 14:56:03 查看任务 1ps aux|grep Snakefile2|grep -v grep 这个任务在内存中，但 qstat没有任务在运行 12345less snakemake.outcd .snakemake/log/ls -thlless 2019-02-21T224633.856137.snakemake.log # 与snakemake.out里的信息一样，没有错误提示，snakemake -n -s Snakefile2 --quiet 这个进程kill不掉 kill 7550 但是，好像起作用了，log文件时间改了，似乎多了一句话（下次log文件应该copy一份下来，或者将最后的内容粘贴下来） Will exit after finishing currently running jobs. 重新运行 123snakemake --unlock# 不放入后台，-j 20，实时监测snakemake -s Snakefile2 --cluster "qsub -V -q high" -j 20 kill -9 7550 有效果 两个进程信息比较 院里集群上的时间快18分钟 报错先查看snakemake.log，然后查看job的std err star 2pass align 出错 2019-2-22 20:06:58 12# 查看错误信息less snakejob.star_2pass_align.52.sh.e980754 Building DAG of jobs…Using shell: /bin/bashProvided cores: 20Rules claiming more threads will be scaled down.Job counts: count jobs 1 star_2pass_align 1 [Fri Feb 22 16:44:54 2019]rule star_2pass_align: input: star_index_2pass/, pairedDIR/OV-9-2_1P.fq.gz, pairedDIR/OV-9-2_2P.fq.gz output: star_2pass/OV-9-2Aligned.out.sam jobid: 0 wildcards: sample=OV-9-2 threads: 20 libgcc_s.so.1 must be installed for pthread_cancel to workterminate called after throwing an instance of ‘St9bad_alloc’libgcc_s.so.1 must be installed for pthread_cancel to work what(): std::bad_alloc[Fri Feb 22 16:46:26 2019]Error in rule star_2pass_align: jobid: 0 output: star_2pass/OV-9-2Aligned.out.sam RuleException:CalledProcessError in line 101 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile2:Command ‘set -euo pipefail; STAR –runThreadN 20 –genomeDir star_index_2pass/ –readFilesIn pairedDIR/OV-9-2_1P.fq.gz pairedDIR/OV-9-2_2P.fq.gz –readFilesCommand zcat –outFileNamePrefix star_2pass/OV-9-2’ returned non-zero exit status 141.snakejob.star_2pass_align.52.sh.e980754 123# 查看任务qstatpestat 12345pbsnodes -a | lesssnakemake --cluster --rerun-incomplete qsub --jobs 10# qstat -a (for summary)# qstat -n (You will see where the nodes the job lands) [qizhengyang@node1 gatk_rna]$ snakemake -nBuilding DAG of jobs…IncompleteFilesException:The files below seem to be incomplete. If you are sure that certain files are not incomplete, mark them as complete with snakemake --cleanup-metadata &lt;filenames&gt; To re-generate the files rerun your command with the –rerun-incomplete flag.Incomplete files:star_2pass/OV-10-1Aligned.out.samstar_2pass/NH-10-3Aligned.out.samstar_2pass/OV-10-2Aligned.out.samstar_2pass/NH-11-2Aligned.out.samstar_2pass/HB-9-3Aligned.out.samstar_2pass/HB-10-3Aligned.out.sam 这样是在当前节点运行的 [qizhengyang@node1 gatk_rna]$ snakemake --rerun-incompleteBuilding DAG of jobs…Using shell: /bin/bashProvided cores: 1Rules claiming more threads will be scaled down.Job counts: count jobs 1 all 36 gatk 36 gatk_filter 36 gatk_split 36 picard 36 picard_markduplicates 25 star_2pass_align 206 [Fri Feb 22 23:06:04 2019]rule picard: input: star_2pass/OV-12-3Aligned.out.sam output: star_2pass/OV-12-3_rg_added_sorted.bam jobid: 101 wildcards: sample=OV-12-3 Job counts: count jobs 1 picard 1INFO 2019-02-22 23:06:07 AddOrReplaceReadGroups 12# 只有胖节点有用，将任务提交到胖节点上，比需指定 -q low。默认是 high，会处于排队状态snakemake --rerun-incomplete --cluster "qsub -q low" --jobs 10 [qizhengyang@node1 gatk_rna]$ snakemake –rerun-incomplete –cluster “qsub -q low” –jobs 10Building DAG of jobs…Using shell: /bin/bashProvided cluster nodes: 10Job counts: count jobs 1 all 36 gatk 36 gatk_filter 36 gatk_split 36 picard 36 picard_markduplicates 25 star_2pass_align 206 [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/HB-12-1Aligned.out.sam output: star_2pass/HB-12-1_rg_added_sorted.bam jobid: 106 wildcards: sample=HB-12-1 Submitted job 106 with external jobid ‘980770.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/NH-9-2Aligned.out.sam output: star_2pass/NH-9-2_rg_added_sorted.bam jobid: 108 wildcards: sample=NH-9-2 Submitted job 108 with external jobid ‘980771.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/OV-12-3Aligned.out.sam output: star_2pass/OV-12-3_rg_added_sorted.bam jobid: 101 wildcards: sample=OV-12-3 Submitted job 101 with external jobid ‘980772.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/HB-10-1Aligned.out.sam output: star_2pass/HB-10-1_rg_added_sorted.bam jobid: 85 wildcards: sample=HB-10-1 Submitted job 85 with external jobid ‘980773.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/NH-9-3Aligned.out.sam output: star_2pass/NH-9-3_rg_added_sorted.bam jobid: 74 wildcards: sample=NH-9-3 Submitted job 74 with external jobid ‘980774.node1’. [Fri Feb 22 23:24:56 2019]rule picard: input: star_2pass/OV-10-3Aligned.out.sam output: star_2pass/OV-10-3_rg_added_sorted.bam jobid: 98 wildcards: sample=OV-10-3 Submitted job 98 with external jobid ‘980775.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/HB-11-2Aligned.out.sam output: star_2pass/HB-11-2_rg_added_sorted.bam jobid: 77 wildcards: sample=HB-11-2 Submitted job 77 with external jobid ‘980776.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/OV-11-2Aligned.out.sam output: star_2pass/OV-11-2_rg_added_sorted.bam jobid: 90 wildcards: sample=OV-11-2 Submitted job 90 with external jobid ‘980777.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/NH-12-1Aligned.out.sam output: star_2pass/NH-12-1_rg_added_sorted.bam jobid: 107 wildcards: sample=NH-12-1 Submitted job 107 with external jobid ‘980778.node1’. [Fri Feb 22 23:24:57 2019]rule picard: input: star_2pass/OV-12-1Aligned.out.sam output: star_2pass/OV-12-1_rg_added_sorted.bam jobid: 102 wildcards: sample=OV-12-1 Submitted job 102 with external jobid ‘980779.node1’. 运行时间 picard 半小时，CPU时间是4小时。但是picard我并没有设置使用多少线程。 qstat qstat -a qstat -n 2019-2-23 11:02:59 Shutting down, this might take some time.Exiting because a job execution failed. Look above for error messageComplete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-22T232454.376651.snakemake.log 出错的任务 [Sat Feb 23 01:21:32 2019]Error in rule picard: jobid: 88 output: star_2pass/OV-9-2_rg_added_sorted.bam cluster_jobid: 980797.node1 1less snakejob.picard.88.sh.e980797 Building DAG of jobs…Using shell: /bin/bashProvided cores: 96Rules claiming more threads will be scaled down.Job counts: count jobs 1 picard 1 [Sat Feb 23 00:58:20 2019]rule picard: input: star_2pass/OV-9-2Aligned.out.sam output: star_2pass/OV-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=OV-9-2 INFO 2019-02-23 00:58:23 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/OV-9-2Aligned.out.sam -O star_2pass/OV-9-2_rg_added_sorted.bam -SO coordinate -RGID OV-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM OV-9-2 00:58:23.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 00:58:23 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/OV-9-2Aligned.out.sam OUTPUT=star_2pass/OV-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 00:58:23 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 00:58:27 AddOrReplaceReadGroups Created read-group ID=OV-9-2 PL=illumina LB=rna SM=OV-9-2 INFO 2019-02-23 00:59:00 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 33s. Last read position: chr9:26,425,698INFO 2019-02-23 00:59:34 AddOrReplaceReadGroups Processed 2,000,000 records. Elapsed time: 00:01:07s. Time for last 1,000,000: 33s. Last read position: chr5:38,200,263INFO 2019-02-23 01:00:29 AddOrReplaceReadGroups Processed 3,000,000 records. Elapsed time: 00:02:02s. Time for last 1,000,000: 55s. Last read position: chr4:1,886,332INFO 2019-02-23 01:00:56 AddOrReplaceReadGroups Processed 4,000,000 records. Elapsed time: 00:02:28s. Time for last 1,000,000: 26s. Last read position: chr5:6,379,073INFO 2019-02-23 01:01:34 AddOrReplaceReadGroups Processed 5,000,000 records. Elapsed time: 00:03:07s. Time for last 1,000,000: 38s. Last read position: chr6:18,567,116INFO 2019-02-23 01:02:13 AddOrReplaceReadGroups Processed 6,000,000 records. Elapsed time: 00:03:46s. Time for last 1,000,000: 38s. Last read position: chr9:39,573,973INFO 2019-02-23 01:02:40 AddOrReplaceReadGroups Processed 7,000,000 records. Elapsed time: 00:04:13s. Time for last 1,000,000: 27s. Last read position: chr5:22,938,417INFO 2019-02-23 01:03:19 AddOrReplaceReadGroups Processed 8,000,000 records. Elapsed time: 00:04:51s. Time for last 1,000,000: 38s. Last read position: chr4:23,545,960INFO 2019-02-23 01:03:55 AddOrReplaceReadGroups Processed 9,000,000 records. Elapsed time: 00:05:28s. Time for last 1,000,000: 36s. Last read position: chr4:17,586,047INFO 2019-02-23 01:04:24 AddOrReplaceReadGroups Processed 10,000,000 records. Elapsed time: 00:05:57s. Time for last 1,000,000: 28s. Last read position: chr7:21,260,263INFO 2019-02-23 01:04:58 AddOrReplaceReadGroups Processed 11,000,000 records. Elapsed time: 00:06:31s. Time for last 1,000,000: 33s. Last read position: chr4:25,569,847INFO 2019-02-23 01:05:36 AddOrReplaceReadGroups Processed 12,000,000 records. Elapsed time: 00:07:08s. Time for last 1,000,000: 37s. Last read position: chr4:25,075,124INFO 2019-02-23 01:06:04 AddOrReplaceReadGroups Processed 13,000,000 records. Elapsed time: 00:07:36s. Time for last 1,000,000: 28s. Last read position: chr1:5,055,820INFO 2019-02-23 01:06:38 AddOrReplaceReadGroups Processed 14,000,000 records. Elapsed time: 00:08:11s. Time for last 1,000,000: 34s. Last read position: chr5:40,818,516INFO 2019-02-23 01:07:13 AddOrReplaceReadGroups Processed 15,000,000 records. Elapsed time: 00:08:46s. Time for last 1,000,000: 34s. Last read position: chr3:28,597,426INFO 2019-02-23 01:07:40 AddOrReplaceReadGroups Processed 16,000,000 records. Elapsed time: 00:09:12s. Time for last 1,000,000: 26s. Last read position: chr2:52,066,083INFO 2019-02-23 01:08:24 AddOrReplaceReadGroups Processed 17,000,000 records. Elapsed time: 00:09:54s. Time for last 1,000,000: 41s. Last read position: chr5:8,214,376INFO 2019-02-23 01:08:53 AddOrReplaceReadGroups Processed 18,000,000 records. Elapsed time: 00:10:25s. Time for last 1,000,000: 31s. Last read position: chr2:42,040,926INFO 2019-02-23 01:09:19 AddOrReplaceReadGroups Processed 19,000,000 records. Elapsed time: 00:10:52s. Time for last 1,000,000: 26s. Last read position: chr8:3,160,150INFO 2019-02-23 01:09:59 AddOrReplaceReadGroups Processed 20,000,000 records. Elapsed time: 00:11:32s. Time for last 1,000,000: 39s. Last read position: chr9:2,765,258INFO 2019-02-23 01:10:36 AddOrReplaceReadGroups Processed 21,000,000 records. Elapsed time: 00:12:08s. Time for last 1,000,000: 36s. Last read position: chr7:872,502INFO 2019-02-23 01:11:13 AddOrReplaceReadGroups Processed 22,000,000 records. Elapsed time: 00:12:46s. Time for last 1,000,000: 37s. Last read position: chr5:38,922,782INFO 2019-02-23 01:11:43 AddOrReplaceReadGroups Processed 23,000,000 records. Elapsed time: 00:13:15s. Time for last 1,000,000: 29s. Last read position: chr2:43,156,703INFO 2019-02-23 01:12:10 AddOrReplaceReadGroups Processed 24,000,000 records. Elapsed time: 00:13:42s. Time for last 1,000,000: 26s. Last read position: chr2:42,168,429INFO 2019-02-23 01:12:37 AddOrReplaceReadGroups Processed 25,000,000 records. Elapsed time: 00:14:10s. Time for last 1,000,000: 27s. Last read position: chr5:47,894,574INFO 2019-02-23 01:13:05 AddOrReplaceReadGroups Processed 26,000,000 records. Elapsed time: 00:14:37s. Time for last 1,000,000: 27s. Last read position: chr5:47,836,828INFO 2019-02-23 01:13:33 AddOrReplaceReadGroups Processed 27,000,000 records. Elapsed time: 00:15:05s. Time for last 1,000,000: 27s. Last read position: chr8:19,290,720INFO 2019-02-23 01:14:01 AddOrReplaceReadGroups Processed 28,000,000 records. Elapsed time: 00:15:33s. Time for last 1,000,000: 27s. Last read position: chr5:17,045,282INFO 2019-02-23 01:14:29 AddOrReplaceReadGroups Processed 29,000,000 records. Elapsed time: 00:16:02s. Time for last 1,000,000: 28s. Last read position: chr4:16,374,034INFO 2019-02-23 01:14:56 AddOrReplaceReadGroups Processed 30,000,000 records. Elapsed time: 00:16:28s. Time for last 1,000,000: 26s. Last read position: chr2:51,915,818INFO 2019-02-23 01:15:24 AddOrReplaceReadGroups Processed 31,000,000 records. Elapsed time: 00:16:57s. Time for last 1,000,000: 28s. Last read position: chr3:24,910,383INFO 2019-02-23 01:15:55 AddOrReplaceReadGroups Processed 32,000,000 records. Elapsed time: 00:17:27s. Time for last 1,000,000: 30s. Last read position: chr7:12,459,410INFO 2019-02-23 01:16:22 AddOrReplaceReadGroups Processed 33,000,000 records. Elapsed time: 00:17:54s. Time for last 1,000,000: 26s. Last read position: chr9:39,689,303[Sat Feb 23 01:20:04 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 21.69 minutes.Runtime.totalMemory()=1004011520To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.lang.String.substring(String.java:1969) at htsjdk.samtools.util.StringUtil.split(StringUtil.java:89) at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:229) at htsjdk.samtools.SAMTextReader$RecordIterator.parseLine(SAMTextReader.java:268) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:255) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113) 1 picard 1 [Sat Feb 23 00:58:20 2019]rule picard: input: star_2pass/OV-9-2Aligned.out.sam output: star_2pass/OV-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=OV-9-2 INFO 2019-02-23 00:58:23 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/OV-9-2Aligned.out.sam -O star_2pass/OV-9-2_rg_added_sorted.bam -SO coordinate -RGID OV-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM OV-9-2 00:58:23.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 00:58:23 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/OV-9-2Aligned.out.sam OUTPUT=star_2pass/OV-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 00:58:23 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 00:58:27 AddOrReplaceReadGroups Created read-group ID=OV-9-2 PL=illumina LB=rna SM=OV-9-2 INFO 2019-02-23 00:59:00 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:33s. Time for last 1,000,000: 33s. Last read position: chr9:26,425,698INFO 2019-02-23 00:59:34 AddOrReplaceReadGroups Processed 2,000,000 records. Elapsed time: 00:01:07s. Time for last 1,000,000: 33s. Last read position: chr5:38,200,263INFO 2019-02-23 01:00:29 AddOrReplaceReadGroups Processed 3,000,000 records. Elapsed time: 00:02:02s. Time for last 1,000,000: 55s. Last read position: chr4:1,886,332INFO 2019-02-23 01:00:56 AddOrReplaceReadGroups Processed 4,000,000 records. Elapsed time: 00:02:28s. Time for last 1,000,000: 26s. Last read position: chr5:6,379,073INFO 2019-02-23 01:01:34 AddOrReplaceReadGroups Processed 5,000,000 records. Elapsed time: 00:03:07s. Time for last 1,000,000: 38s. Last read position: chr6:18,567,116INFO 2019-02-23 01:02:13 AddOrReplaceReadGroups Processed 6,000,000 records. Elapsed time: 00:03:46s. Time for last 1,000,000: 38s. Last read position: chr9:39,573,973INFO 2019-02-23 01:02:40 AddOrReplaceReadGroups Processed 7,000,000 records. Elapsed time: 00:04:13s. Time for last 1,000,000: 27s. Last read position: chr5:22,938,417INFO 2019-02-23 01:03:19 AddOrReplaceReadGroups Processed 8,000,000 records. Elapsed time: 00:04:51s. Time for last 1,000,000: 38s. Last read position: chr4:23,545,960INFO 2019-02-23 01:03:55 AddOrReplaceReadGroups Processed 9,000,000 records. Elapsed time: 00:05:28s. Time for last 1,000,000: 36s. Last read position: chr4:17,586,047INFO 2019-02-23 01:04:24 AddOrReplaceReadGroups Processed 10,000,000 records. Elapsed time: 00:05:57s. Time for last 1,000,000: 28s. Last read position: chr7:21,260,263INFO 2019-02-23 01:04:58 AddOrReplaceReadGroups Processed 11,000,000 records. Elapsed time: 00:06:31s. Time for last 1,000,000: 33s. Last read position: chr4:25,569,847INFO 2019-02-23 01:05:36 AddOrReplaceReadGroups Processed 12,000,000 records. Elapsed time: 00:07:08s. Time for last 1,000,000: 37s. Last read position: chr4:25,075,124INFO 2019-02-23 01:06:04 AddOrReplaceReadGroups Processed 13,000,000 records. Elapsed time: 00:07:36s. Time for last 1,000,000: 28s. Last read position: chr1:5,055,820INFO 2019-02-23 01:06:38 AddOrReplaceReadGroups Processed 14,000,000 records. Elapsed time: 00:08:11s. Time for last 1,000,000: 34s. Last read position: chr5:40,818,516INFO 2019-02-23 01:07:13 AddOrReplaceReadGroups Processed 15,000,000 records. Elapsed time: 00:08:46s. Time for last 1,000,000: 34s. Last read position: chr3:28,597,426INFO 2019-02-23 01:07:40 AddOrReplaceReadGroups Processed 16,000,000 records. Elapsed time: 00:09:12s. Time for last 1,000,000: 26s. Last read position: chr2:52,066,083INFO 2019-02-23 01:08:24 AddOrReplaceReadGroups Processed 17,000,000 records. Elapsed time: 00:09:54s. Time for last 1,000,000: 41s. Last read position: chr5:8,214,376INFO 2019-02-23 01:08:53 AddOrReplaceReadGroups Processed 18,000,000 records. Elapsed time: 00:10:25s. Time for last 1,000,000: 31s. Last read position: chr2:42,040,926INFO 2019-02-23 01:09:19 AddOrReplaceReadGroups Processed 19,000,000 records. Elapsed time: 00:10:52s. Time for last 1,000,000: 26s. Last read position: chr8:3,160,150INFO 2019-02-23 01:09:59 AddOrReplaceReadGroups Processed 20,000,000 records. Elapsed time: 00:11:32s. Time for last 1,000,000: 39s. Last read position: chr9:2,765,258INFO 2019-02-23 01:10:36 AddOrReplaceReadGroups Processed 21,000,000 records. Elapsed time: 00:12:08s. Time for last 1,000,000: 36s. Last read position: chr7:872,502INFO 2019-02-23 01:11:13 AddOrReplaceReadGroups Processed 22,000,000 records. Elapsed time: 00:12:46s. Time for last 1,000,000: 37s. Last read position: chr5:38,922,782INFO 2019-02-23 01:11:43 AddOrReplaceReadGroups Processed 23,000,000 records. Elapsed time: 00:13:15s. Time for last 1,000,000: 29s. Last read position: chr2:43,156,703INFO 2019-02-23 01:12:10 AddOrReplaceReadGroups Processed 24,000,000 records. Elapsed time: 00:13:42s. Time for last 1,000,000: 26s. Last read position: chr2:42,168,429INFO 2019-02-23 01:12:37 AddOrReplaceReadGroups Processed 25,000,000 records. Elapsed time: 00:14:10s. Time for last 1,000,000: 27s. Last read position: chr5:47,894,574INFO 2019-02-23 01:13:05 AddOrReplaceReadGroups Processed 26,000,000 records. Elapsed time: 00:14:37s. Time for last 1,000,000: 27s. Last read position: chr5:47,836,828INFO 2019-02-23 01:13:33 AddOrReplaceReadGroups Processed 27,000,000 records. Elapsed time: 00:15:05s. Time for last 1,000,000: 27s. Last read position: chr8:19,290,720INFO 2019-02-23 01:14:01 AddOrReplaceReadGroups Processed 28,000,000 records. Elapsed time: 00:15:33s. Time for last 1,000,000: 27s. Last read position: chr5:17,045,282INFO 2019-02-23 01:14:29 AddOrReplaceReadGroups Processed 29,000,000 records. Elapsed time: 00:16:02s. Time for last 1,000,000: 28s. Last read position: chr4:16,374,034INFO 2019-02-23 01:14:56 AddOrReplaceReadGroups Processed 30,000,000 records. Elapsed time: 00:16:28s. Time for last 1,000,000: 26s. Last read position: chr2:51,915,818INFO 2019-02-23 01:15:24 AddOrReplaceReadGroups Processed 31,000,000 records. Elapsed time: 00:16:57s. Time for last 1,000,000: 28s. Last read position: chr3:24,910,383INFO 2019-02-23 01:15:55 AddOrReplaceReadGroups Processed 32,000,000 records. Elapsed time: 00:17:27s. Time for last 1,000,000: 30s. Last read position: chr7:12,459,410INFO 2019-02-23 01:16:22 AddOrReplaceReadGroups Processed 33,000,000 records. Elapsed time: 00:17:54s. Time for last 1,000,000: 26s. Last read position: chr9:39,689,303[Sat Feb 23 01:20:04 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 21.69 minutes.Runtime.totalMemory()=1004011520To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.lang.String.substring(String.java:1969) at htsjdk.samtools.util.StringUtil.split(StringUtil.java:89) at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:229) at htsjdk.samtools.SAMTextReader$RecordIterator.parseLine(SAMTextReader.java:268) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:255) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)Exception in thread “Thread-0” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Hashtable.(Hashtable.java:190) at java.util.Hashtable.(Hashtable.java:211) at java.util.Properties.(Properties.java:148) at java.util.Properties.(Properties.java:140) at java.util.logging.LogManager.reset(LogManager.java:1321) at java.util.logging.LogManager$Cleaner.run(LogManager.java:239)Exception in thread “Thread-1” java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:68) at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:227) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at htsjdk.samtools.util.nio.DeleteOnExitPathHook.runHooks(DeleteOnExitPathHook.java:57) at htsjdk.samtools.util.nio.DeleteOnExitPathHook$$Lambda$34/780934299.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)[Sat Feb 23 01:20:54 2019]Error in rule picard: jobid: 0 output: star_2pass/OV-9-2_rg_added_sorted.bam RuleException:CalledProcessError in line 115 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Command ‘set -euo pipefail; picard AddOrReplaceReadGroups I=star_2pass/OV-9-2Aligned.out.sam O=star_2pass/OV-9-2_rg_added_sorted.bam SO=coordinate RGID=OV-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=OV-9-2’ returned non-zero exit status 1. File “/home02/qizhengyang/qizhengyang/gatk_rna/Snakefile”, line 115, in __rule_picard File “/home02/qizhengyang/anaconda3/lib/python3.6/concurrent/futures/thread.py”, line 56, in runRemoving output files of failed job picard since they might be corrupted:star_2pass/OV-9-2_rg_added_sorted.bamShutting down, this might take some time.Exiting because a job execution failed. Look above for error message(END) 123456snakemake -n --quietsnakemake --cluster "qsub -q low" --jobs 100pestat# 36 jobs, busyqstat -f | less 一个终端与服务器连接终端 Socket error Event: 32 Error: 10053.Connection closing…Socket close. Connection closed by foreign host. Disconnected from remote host(qizhengyang) at 11:29:56. Type `help’ to learn how to use Xshell prompt.[C:~]$ last 显示用户最近登录信息。top用于实时显示 process 的动态。 123456789snakemake -n -s Snakefile2 --quietmv Snakefile Snakefile_temp_1mv Snakefile1 Snakefile_temp_2mv Snakefile2 Snakefilesnakemake --unlocknohup snakemake --cluster "qsub -q low" --jobs 100 &amp;# 输出追加到"nohup.out"，nohup.out文件保留上次的信息 [qizhengyang@node1 gatk_rna]$ jobs[1]+ Running nohup snakemake –cluster “qsub -q low” –jobs 100 &amp;[qizhengyang@node1 gatk_rna]$ ps aux | grep snakemake | grep -v grep528 39349 0.5 0.0 289360 26440 pts/5 Sl 14:00 0:01 /home02/qizhengyang/anaconda3/bin/python3.6 /home02/qizhengyang/anaconda3/bin/snakemake –cluster qsub -q low –jobs 100 1ll -h nohup.out 一个 picard job 出现错误 Exception in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded Moreover, now i am running command like /usr/bin/java -Xmx40g -jar /usr/local/picard-tools-1.129/picard.jar …., and it’s working. Regards, Ravi. ava刚刚出现的年代，有一个相比于其他语言的优势就是，内存回收机制。不需要明确的调用释放内存的API，java就自动完成，这个过程就是Garbage Collection，简称GC。 其实还是有一个终极方法的，而且是治标治本的方法，就是找到占用内存大的地方，把代码优化了，就不会出现这个问题了。 Building DAG of jobs…Using shell: /bin/bashProvided cores: 96Rules claiming more threads will be scaled down.Job counts: count jobs 1 picard 1 [Sat Feb 23 14:00:10 2019]rule picard: input: star_2pass/HB-9-2Aligned.out.sam output: star_2pass/HB-9-2_rg_added_sorted.bam jobid: 0 wildcards: sample=HB-9-2 INFO 2019-02-23 14:00:12 AddOrReplaceReadGroups ** NOTE: Picard’s command line syntax is changing. ** For more information, please see:** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition) ** The command line looks like this in the new syntax: ** AddOrReplaceReadGroups -I star_2pass/HB-9-2Aligned.out.sam -O star_2pass/HB-9-2_rg_added_sorted.bam -SO coordinate -RGID HB-9-2 -RGLB rna -RGPL illumina -RGPU hiseq -RGSM HB-9-2 14:00:12.937 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so[Sat Feb 23 14:00:13 CST 2019] AddOrReplaceReadGroups INPUT=star_2pass/HB-9-2Aligned.out.sam OUTPUT=star_2pass/HB-9-2_rg_added_sorted.bam SORT_ORDER=coordinate RGID=HB-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=HB-9-2 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false[Sat Feb 23 14:00:13 CST 2019] Executing as qizhengyang@node21 on Linux 2.6.32-431.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.18.26-SNAPSHOTINFO 2019-02-23 14:00:20 AddOrReplaceReadGroups Created read-group ID=HB-9-2 PL=illumina LB=rna SM=HB-9-2 INFO 2019-02-23 14:00:58 AddOrReplaceReadGroups Processed 1,000,000 records. Elapsed time: 00:00:37s. Time for last 1,000,000: 37s. Last read position: chr1:10,583,000[Sat Feb 23 14:07:47 CST 2019] picard.sam.AddOrReplaceReadGroups done. Elapsed time: 7.58 minutes.Runtime.totalMemory()=999292928To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelpException in thread “main” java.lang.OutOfMemoryError: GC overhead limit exceeded at java.util.Arrays.copyOfRange(Arrays.java:3664) at java.lang.String.(String.java:207) at java.io.BufferedReader.readLine(BufferedReader.java:356) at java.io.LineNumberReader.readLine(LineNumberReader.java:201) at htsjdk.samtools.util.BufferedLineReader.readLine(BufferedLineReader.java:68) at htsjdk.samtools.SAMTextReader.advanceLine(SAMTextReader.java:221) at htsjdk.samtools.SAMTextReader.access$800(SAMTextReader.java:37) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:257) at htsjdk.samtools.SAMTextReader$RecordIterator.next(SAMTextReader.java:228) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569) at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548) at picard.sam.AddOrReplaceReadGroups.doWork(AddOrReplaceReadGroups.java:182) at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295) at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:103) at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:113)Exception in thread “Thread-1” java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.nio.fs.UnixFileAttributes.get(UnixFileAttributes.java:68) at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:227) at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) at java.nio.file.Files.delete(Files.java:1126) at htsjdk.samtools.util.nio.DeleteOnExitPathHook.runHooks(DeleteOnExitPathHook.java:57) at htsjdk.samtools.util.nio.DeleteOnExitPathHook$$Lambda$34/780934299.run(Unknown Source) at java.lang.Thread.run(Thread.java:745)[Sat Feb 23 14:09:07 2019]Error in rule picard: jobid: 0 output: star_2pass/HB-9-2_rg_added_sorted.bam RuleException:CalledProcessError in line 115 of /home02/qizhengyang/qizhengyang/gatk_rna/Snakefile:Command ‘set -euo pipefail; picard AddOrReplaceReadGroups I=star_2pass/HB-9-2Aligned.out.sam O=star_2pass/HB-9-2_rg_added_sorted.bam SO=coordinate RGID=HB-9-2 RGLB=rna RGPL=illumina RGPU=hiseq RGSM=HB-9-2’ returned non-zero exit status 1. File “/home02/qizhengyang/qizhengyang/gatk_rna/Snakefile”, line 115, in __rule_picard File “/home02/qizhengyang/anaconda3/lib/python3.6/concurrent/futures/thread.py”, line 56, in runRemoving output files of failed job picard since they might be corrupted:star_2pass/HB-9-2_rg_added_sorted.bamShutting down, this might take some time.Exiting because a job execution failed. Look above for error message(END) [qizhengyang@node1 gatk_rna]$ snakemake -n –quietJob counts:count jobs1 all36 gatk36 gatk_filter16 gatk_split1 picard13 picard_markduplicat 12snakemake --unlocknohup snakemake --cluster &quot;qsub -q low&quot; --jobs 100 &amp; 解决办法 查看 picard.jar 的路径 设置环境变量 export picard=/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar， 写入 ~/.bash_profile，source 设置JVM最大可用内存为40g，java -Xmx40g -jar $picard。默认是1G [qizhengyang@node1 gatk_rna]$ find ~ -name picard.jar/home02/qizhengyang/anaconda3/share/picard-2.18.26-0/picard.jar/home02/qizhengyang/anaconda3/pkgs/picard-2.18.26-0/share/picard-2.18.26-0/picard.jar 查看当前环境变量 export | less 980879.node1 …b.gatk.202.sh qizhengyang 01:31:52 R low980880.node1 …b.gatk.182.sh qizhengyang 01:31:40 R low980881.node1 …b.gatk.204.sh qizhengyang 01:31:11 R low980882.node1 …b.gatk.209.sh qizhengyang 01:31:20 R low980884.node1 …b.gatk.214.sh qizhengyang 01:31:59 R low980888.node1 …b.gatk.187.sh qizhengyang 01:30:53 R low980889.node1 …b.gatk.210.sh qizhengyang 01:30:29 R low980891.node1 …b.gatk.186.sh qizhengyang 01:31:41 R low980892.node1 …b.gatk.216.sh qizhengyang 01:30:55 R low980893.node1 …b.gatk.206.sh qizhengyang 01:31:36 R low980894.node1 …b.gatk.192.sh qizhengyang 01:30:59 R low980896.node1 …b.gatk.188.sh qizhengyang 01:31:45 R low980898.node1 …b.gatk.215.sh qizhengyang 01:31:05 R low980899.node1 …_split.167.sh qizhengyang 06:39:52 R low980900.node1 …b.gatk.200.sh qizhengyang 01:31:05 R low980901.node1 …_split.153.sh qizhengyang 05:35:11 R low980902.node1 …b.gatk.201.sh qizhengyang 01:31:57 R low980903.node1 …b.gatk.195.sh qizhengyang 01:29:26 R low980904.node1 …b.gatk.197.sh qizhengyang 01:29:35 R low980905.node1 …b.gatk.185.sh qizhengyang 01:30:31 R low980909.node1 …_split.160.sh qizhengyang 05:36:38 R low980910.node1 …b.gatk.198.sh qizhengyang 01:29:47 R low980912.node1 …b.gatk.193.sh qizhengyang 01:30:22 R low980913.node1 …_split.155.sh qizhengyang 04:15:22 R low980914.node1 …_split.175.sh qizhengyang 04:04:21 R low980915.node1 …_split.172.sh qizhengyang 03:43:49 R low980916.node1 …_split.177.sh qizhengyang 03:22:15 R low980917.node1 …_split.147.sh qizhengyang 05:08:37 R low980918.node1 …_split.158.sh qizhengyang 05:14:32 R low980919.node1 …_split.176.sh qizhengyang 04:49:57 R low980920.node1 …_split.163.sh qizhengyang 04:37:11 R low980921.node1 …_split.171.sh qizhengyang 04:17:19 R low980922.node1 …_split.154.sh qizhengyang 04:05:50 R low980923.node1 …_split.148.sh qizhengyang 03:46:37 R low980925.node1 …_split.181.sh qizhengyang 03:25:10 R low980926.node1 …_split.169.sh qizhengyang 02:20:52 R low 任务完成 [Sun Feb 24 23:55:12 2019]Finished job 0.103 of 103 steps (100%) doneComplete log: /home02/qizhengyang/qizhengyang/gatk_rna/.snakemake/log/2019-02-23T214006.459737.snakemake.log 最终的Snakefile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186# 2019-2-19 10:28:52 part# GATK snakemake# qizhengyangfrom os.path import joinGENOME = 'genome/HWB.chromosome.fa'GTF = 'genes/HWB.gene.models.gtf'(SAMPLES,) = glob_wildcards('pairedDIR/&#123;sample&#125;_1P.fq.gz')PATTERN_R1 = join('pairedDIR', '&#123;sample&#125;_1P.fq.gz')PATTERN_R2 = join('pairedDIR', '&#123;sample&#125;_2P.fq.gz')rule all: input: 'star_index_2pass/', expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES), 'star_index_2pass/', expand('star_2pass/&#123;sample&#125;Aligned.out.sam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_rg_added_sorted.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_dedup_split.bam', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;.vcf', sample=SAMPLES), expand('star_2pass/&#123;sample&#125;_filtered.vcf', sample=SAMPLES)rule star_index: input: genome = GENOME, gtf = GTF output: star_index = directory('star_index/') log: 'star_index.log' threads: 20 run: # star 1-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.star_index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbGTFfile &#123;input.gtf&#125;' ' 2&gt; &#123;log&#125;')rule star_1pass_align: input: index = 'star_index/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: index = 'star_1pass/&#123;sample&#125;SJ.out.tab' threads: 20 params: prefix = './star_1pass/&#123;sample&#125;' # 在使用params之前是报错的，NameError,The name 'sample' is unknown in this context run: # star 1-pass align, OK shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule star_2pass_index: input: genome = GENOME, # 这里必需加expand，不然会报错：Wildcards in input files cannot be determined from output files: # 'sample'。 # 很奇怪，确实是需要所有样本的剪接位点信息，我之前没有注意到。。感谢报错 # 然后用--sjdbFileChrStartEnd参数将所有样品的SJ.out.tab文件作为输入的annotated junction进行第二次建index # http://www.bioinfo-scrounger.com/archives/288 # 这里不能加三个引号（"""或'''注释） splice_site = expand('star_1pass/&#123;sample&#125;SJ.out.tab', sample=SAMPLES) output: index = directory('star_index_2pass/') threads: 20 run: # star 2-pass index, OK shell('STAR --runThreadN &#123;threads&#125; --runMode genomeGenerate' ' --genomeDir &#123;output.index&#125;' ' --genomeFastaFiles &#123;input.genome&#125;' ' --sjdbFileChrStartEnd &#123;input.splice_site&#125;')rule star_2pass_align: input: index = 'star_index_2pass/', r1 = PATTERN_R1, r2 = PATTERN_R2 output: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' threads: 20 params: prefix = 'star_2pass/&#123;sample&#125;' run: # star 2-pass align shell('STAR --runThreadN &#123;threads&#125; --genomeDir &#123;input.index&#125;' ' --readFilesIn &#123;input.r1&#125; &#123;input.r2&#125;' ' --readFilesCommand zcat' ' --outFileNamePrefix &#123;params.prefix&#125;')rule picard: input: sam = 'star_2pass/&#123;sample&#125;Aligned.out.sam' output: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' run: # RGID和RGSM的sample必须是&#123;wildcards.sample&#125;，不然 # The name 'sample' is unknown in this context. Please make sure that you defined that variable. # Also note that braces not used for variable access have to be escaped by repeating them, i.e. &#123;&#123;print $1&#125;&#125; shell('java -Xmx40g -jar $picard AddOrReplaceReadGroups' ' I=&#123;input.sam&#125;' ' O=&#123;output.bam&#125;' ' SO=coordinate' ' RGID=&#123;wildcards.sample&#125;' ' RGLB=rna' ' RGPL=illumina' ' RGPU=hiseq' ' RGSM=&#123;wildcards.sample&#125;')rule picard_markduplicates: input: bam = 'star_2pass/&#123;sample&#125;_rg_added_sorted.bam' output: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam' params: dedup_metrices = 'star_2pass/&#123;sample&#125;_dedup.metrics' run: shell('java -Xmx40g -jar $picard MarkDuplicates' ' I=&#123;input.bam&#125;' ' O=&#123;output.dedup_bam&#125;' ' CREATE_INDEX=true' ' VALIDATION_STRINGENCY=SILENT' ' M=&#123;params.dedup_metrices&#125;')rule gatk_split: input: dedup_bam = 'star_2pass/&#123;sample&#125;_dedup.bam', genome = GENOME output: split_bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam' run: shell('java -Xmx10g -jar $GATK -T SplitNCigarReads' ' -R &#123;input.genome&#125;' ' -I &#123;input.dedup_bam&#125;' ' -o &#123;output.split_bam&#125;' ' -rf ReassignOneMappingQuality' ' -RMQF 255' ' -RMQT 60' ' -U ALLOW_N_CIGAR_READS')rule gatk: input: bam = 'star_2pass/&#123;sample&#125;_dedup_split.bam', genome = GENOME output: vcf = 'star_2pass/&#123;sample&#125;.vcf' run: shell('java -Xmx10g -jar $GATK -T HaplotypeCaller' ' -R &#123;input.genome&#125;' ' -I &#123;input.bam&#125;' ' -dontUseSoftClippedBases' ' -stand_call_conf 20.0' ' -o &#123;output.vcf&#125;')rule gatk_filter: input: genome = GENOME, vcf = 'star_2pass/&#123;sample&#125;.vcf' output: 'star_2pass/&#123;sample&#125;_filtered.vcf' run: shell('java -Xmx10g -jar $GATK ' ' -T VariantFiltration' ' -R &#123;input.genome&#125;' ' -V &#123;input.vcf&#125;' ' -window 35' ' -cluster 3' ' -filterName FS -filter "FS &gt; 30.0"' ' -filterName QD -filter "QD &lt; 2.0"' ' -o &#123;output&#125;') 参考资料 利用snakemake搭建流程简明教程 snakemake docs RNAseq variant calling pipeline Build bioinformatics pipelines with Snakemake kallisto-snakefiles Writing a RNA-Seq workflow with snakemake snakemake初步 RNA-seq 检测变异之 GATK 最佳实践流程 snakemake-example 我最喜欢的流程管理工具]]></content>
      <tags>
        <tag>生物信息</tag>
        <tag>snakemake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[the machine stops]]></title>
    <url>%2F2019%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%81%9C%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[从工业革命开始，城市化，信息化，智能化，人们的生活发生了不可逆转的剧变。有些东西消失后就不会再出现，比如蒸汽机车，有些东西即使外界再变也不会消失，比如文明。 they have given up, to a great extent, the amenities and achievements of civilization: solitude and leisure, the sanction to be oneself, truly absorbed, whether in contemplating a work of art, a scientific theory, a sunset, or the face of one’s beloved.]]></content>
  </entry>
  <entry>
    <title><![CDATA[甲基化数据分析]]></title>
    <url>%2F2019%2F01%2F30%2F%E7%94%B2%E5%9F%BA%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[DNA甲基化数据分析流程 用Trimmomatic去除低质量序列（q&lt;20），接头。 用BSMAP比对，允许0.8的错配率。 用methratio.py提取甲基化比例，选项-r去除PCR重复。 获得DMR 为了得到可靠的DMR区域，合并两个生物学重复，仅考虑所有文库中深度至少为4的胞嘧啶。 使用200bp窗口（50bp步长）识别DMR。 对每个窗口内的甲基化和未甲基化胞嘧啶进行Fisher精确检验。使用Benjamini-Hochberg对p值进行调整，估计错误发生率（FDR）。 FDR&lt;0.01，甲基化水平变化大于1.5倍且至少含有5个差异甲基化胞嘧啶（DMCs：Fisher精确检验中p&lt;0.01）的窗口用于进一步分析，窗口在100bp内合并为更大的区域。 RNA-seq 数据分析 Trimmomatic去除低质量序列和接头 用STAR进行比对，–sjdbGTFfile 用于提供基因组注释文件 htseq-count计算每个基因map上的片段数 DESeq2计算差异表达基因 DMR-associated基因分析 DMR相关基因定义为2kb启动子区域内具有DMR的基因 仅用DMR-associated genes进行基因聚类 差异基因定义，FPKM &gt;= 1, FDR &lt;= 0.01, fc &gt;= 1.5 GO注释 使用拟南芥（TAIR10）、番茄（ITAG3）、草莓（PhytozomeV12）的蛋白序列和GO注释文件 blast 用GOATOOLs进行GO富集 小RNA分析 使用BWA比对 计算DMR的24 nt 小RNA丰度（normalized to per million per one hundred base pair）]]></content>
      <tags>
        <tag>生物信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql使用过程中遇到的一些问题]]></title>
    <url>%2F2019%2F01%2F29%2FMysql%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%2F</url>
    <content type="text"><![CDATA[blast2go本地化需要用到mysql，操作系统Centos 7。 1. 卸载之前的mysql之前的版本是mysql5.6，想要更新到mysql5.7。 查看mysql安装了哪些东西。 1rpm -qa |grep -i mysql 开始卸载 1yum remove XXX 查看卸载是否完成 查找mysql相关目录 1find / -name mysql 删除相关目录 123rm -rf XXXrm -rf /etc/my.cnfrm -rf rm -rf /var/log/mysqld.log 2. 安装Mysql5.7 安装mysql源 1234wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpmyum localinstall mysql57-community-release-el7-11.noarch.rpm# 检查是否安装成功yum repolist enabled | grep &quot;mysql.*-community.*&quot; 启动mysql服务 123systemctl start mysqld# 查看状态systemctl status mysqld 这里可能会遇到 “Another mysqld server running on port 3306 error”，可以采用netstat -lp | grep 3306 查找占用这个端口号的进程，kill 这个PID。（我发现占用这个3306的是mysql??）。 修改root密码 我用root身份导入数据的时候，提示我要修改密码。生成的默认密码在 /var/log/mysqld.log 文件中。使用 grep 命令找到日志中的密码。 1grep &apos;temporary password&apos; /var/log/mysqld.log 1mysql&gt;ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123,c%vPl9ek&apos;; 或者： 123mysql&gt; use mysql;mysql&gt; update mysql.user set authentication_string=password(&apos;123,c%vPl9ek&apos;) where user=&apos;root&apos;;mysql&gt; flush privileges; 3. 导入数据1mysql -u root -p &lt; b2gdb.sql 会出现莫名其妙的错误，“ERROR 1819 (HY000) at line 4: Your password does not satisfy the current policy requirements”，但是我的密码是符合它的规则的。最后采取的办法是把检验密码的插件删了。 1mysql&gt;uninstall plugin validate_password; 有试着导入数据，这次的错误是： “ERROR 1101 (42000) at line 9: BLOB, TEXT, GEOMETRY or JSON column ‘description’ can’t have a default value” 先查看了sql_mode 12mysql&gt; select @@session.sql_mode;mysql&gt; select @@global.sql_mode; 之后重新设置sql_mode 12set sql_mode=&apos;&apos;;set global sql_mode=&apos;&apos;; 重新打开一个终端，进入mysql，查看sql_mode 1select @@global.sql_mode; 在尝试导入数据，成功。 4. blast2go123456cd data1/data/blast2go/mysql -u root -p &lt; b2gdb.sqlmysql -u root -p -e &quot;GRANT ALL ON b2gdb.* TO &apos;blast2go&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;blast4it&apos;;&quot;mysql -u root -p -e &quot;FLUSH PRIVILEGES;&quot;# 这一步很耗时nohup time mysql -s -u root -p b2gdb &lt; go_monthly-assocdb-data &gt; mysql.out 2&gt;&amp;1 &amp; 5. 参考 MySQL sql_mode 说明 blast2go本地化教程 your password does not satisfy the current policy requirements another mysqld server running on the port 3306 Centos7 完全卸载mysql CentOS 7 下 MySQL 5.7 的安装与配置]]></content>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tair breaking news]]></title>
    <url>%2F2019%2F01%2F17%2Ftair-breaking-news%2F</url>
    <content type="text"><![CDATA[tair breaking news 必须fq设置全局代理）才能查看 https://zenodo.org/record/2530282#.XECIyvln18E 这里可以下载到最新的tair注释文件。下载后存”F:\Project\org.At.tair.db”。创建物种注释包https://www.jianshu.com/p/8963f396bfed 《狂人日记》，那个时代的知识分子]]></content>
  </entry>
  <entry>
    <title><![CDATA[SF felt like the Mecca of tech, but also the center of capitalism]]></title>
    <url>%2F2019%2F01%2F05%2FSF%2F</url>
    <content type="text"><![CDATA[There is a lot of money, but not a lot of wealth？ 该怎么翻译？]]></content>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R输出图形]]></title>
    <url>%2F2018%2F12%2F12%2Fexporting-nice-plots-in-r%2F</url>
    <content type="text"><![CDATA[这篇博文写的很好 https://www.r-bloggers.com/exporting-nice-plots-in-r/介绍的 Inkscape 也很好用，解决了之前IGV输出SVG图片的问题。导入PPT只显示一半。用Ai cc 打开，报错。“往返Tiny时剪贴将丢失”，只有框架没有图 There are two main problems when exporting graphics from R: Anti-aliasing is not activated in Windows R (this does not apply to Linux or Mac) – windows下没有反锯齿设置，解决：library(Cairo) When increasing the resolution the labels automatically decrease and become unreadable –分辨率升高，标签会自动缩小 If we want to increase the resolution of the plot we can’t just change the resolution parameter:We also have to change the point size, the formula is size * new resolution DPI / 72 DPI: If we double the image size we also need to double the point size: 123456789101112131415161718192021222324# https://www.r-graph-gallery.com/265-grouped-boxplot-with-ggplot2/# exporting nice plot https://www.r-bloggers.com/exporting-nice-plots-in-r/# librarylibrary(ggplot2) # create a data framevariety=rep(LETTERS[1:7], each=40)treatment=rep(c("high","low"),each=20)note=seq(1:280)+sample(1:150, 280, replace=T)data=data.frame(variety, treatment , note) # grouped boxplotggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot()# One box per treatmentggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot() + facet_wrap(~treatment)# one box per varietyggplot(data, aes(x=variety, y=note, fill=treatment)) + geom_boxplot() + facet_wrap(~variety, scale="free")]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google news "epigenetics plants"]]></title>
    <url>%2F2018%2F12%2F10%2FGoogle-News%2F</url>
    <content type="text"><![CDATA[University of Nebraska-Lincoln researchers have found revolutionary evidence that an evolutionary phenomenon at work in complex organisms is at play in their single-celled, extreme-loving counterparts, too.]]></content>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好像windows下circos不缺GD模块]]></title>
    <url>%2F2018%2F12%2F07%2FGD-module%2F</url>
    <content type="text"><![CDATA[windows下好像并没有缺失GD模块？版本：This is perl 5, version 24, subversion 2 (v5.24.2) built for MSWin32-x64-multi-thread missing Config::General missing Font::TTF::Fontmissing Math::Bezier missing Math::Roundmissing Math::VecStatmissing Regexp::Commonmissing SVGmissing Set::IntSpanmissing Statistics::Basicmissing Text::Format GD模块在C:\Perl64\lib我自己安装的在C:\Perl64\site\lib perl circos -moduleppm install Math::Round Math::VecStat Regexp::Common SVG Set::IntSpan Statistics::Basic Text::Format]]></content>
  </entry>
  <entry>
    <title><![CDATA[bed和bedGraph区别]]></title>
    <url>%2F2018%2F11%2F14%2Fbed%E5%92%8CbedGraph%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[BedGraph has 4 fields (chr start stop score), and the simplest BED file has 3 fields (chr start stop), so one easy way to convert from BED to bedGraph would be to use awk: cat file.bedGraph | awk ‘{print $1 “\t” $2 “\t” $3}’ &gt; file.bedIf your bedGraph file contains a header line, you can get rid of it with an inverse grep: grep -v track file.bedGraph | awk ‘{print $1 “\t” $2 “\t” $3}’ &gt; file.bedOr you can use tail to return everything but the first line (if your track line is the first line). If you want to add more fields to the bed file, you can insert them in the print statement (I don’t remember the minimum requirement for ChIPpeakAnno).]]></content>
  </entry>
  <entry>
    <title><![CDATA[ggplot2 绘图]]></title>
    <url>%2F2018%2F11%2F12%2Fggplot2%E7%BB%98%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[可以尝试设置一个bestfit图层参数12bestfit &lt;- geom_smooth(method = "lm", se = F, colour = alpha("steelblue", 0.5), size = 2)]]></content>
  </entry>
  <entry>
    <title><![CDATA[集群上安装DESeq2遇到的困难及解决办法]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85DESeq2%E9%81%87%E5%88%B0%E7%9A%84%E5%9B%B0%E9%9A%BE%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[BiocManager::install(“DESeq2”, version = “3.8”)报错，错误内容：configure: WARNING: Only g++ version 4.7.2 or greater can be used with RcppArmadillo.configure: error: Please use a different compiler.ERROR: configuration failed for package ‘RcppArmadillo’ 源码安装gcc参考../configure –prefix=/home02/qizhengyang/packages -enable-checking=release -enable-languages=c,c++ -disable-multilib报错，错误内容：/usr/bin/ld: cannot find -lgfortran 重新安装gcc，增加fortran../configure –prefix=/home02/qizhengyang/packages -enable-checking=release -enable-languages=c,c++,fortran -disable-multilib &amp;&amp; make -j4 &amp;&amp; make install 添加两个libexport LD_LIBRARY_PATH=$HOME/packages/lib64:$LD_LIBRARY_PATHexport LD_LIBRARY_PATH=$HOME/packages/lib:$LD_LIBRARY_PATH BiocManager::install(“DESeq2”, version = “3.8”)成功 备注：configure时间超级长。过程艰辛。还尝试过手动安装RcppArmadillo，失败。.libPaths()“/home02/qizhengyang/packages/R/lib64/R/library”R CMD INSTALL -l /home02/qizhengyang/packages/R/lib64/R/library RcppArmadillo_0.3.6.3.tar.gz]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群上安装的R包]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85%E7%9A%84R%E5%8C%85%2F</url>
    <content type="text"><![CDATA[写了一个R script批量安装。1234567install.packages("devtools",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")install.packages("dplyr",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")install.packages("BiocManager",repos="https://mirrors.tuna.tsinghua.edu.cn/CRAN")BiocManager::install("ballgown", version = "3.8")BiocManager::install(genefilter", version = "3.8")devtools::install_github('alyssafrazee/RSkittleBrewer')BiocManager::install("methylKit", version = "3.8")]]></content>
  </entry>
  <entry>
    <title><![CDATA[集群上更新R并安装methylKit]]></title>
    <url>%2F2018%2F10%2F31%2F%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85R%2F</url>
    <content type="text"><![CDATA[普通用户在集群上安装R主要的教程里面有一些需要注意的，安装bzip，要加入一个 -fPIC 到 CFLAG 。（在 Makefile中CFLAG=-fPIC -Wall -Winline -O2 -g…）在R安装configure之前需要注意设置环境变量123LD_LIBRARY_PATH=/home02/qizhengyang/packages/libexport CFLAGS="-I/home02/qizhengyang/packages/include"export LDFLAGS="-L/home02/qizhengyang/packages/lib" R安装完成之后123export R_HOME=$HOME/packages/Rexport R_LIBS=$HOME/packages/R/lib64/libraryexport PATH="$R_HOME/bin":$PATH 最后 source ~/.bashrc 装methyKit也是坑一堆。我下了autoconf-2.69，libxml2-2.7.2。源码安装。./configure –prefix=/home02/qizhengyang/packages]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[trimmatic使用]]></title>
    <url>%2F2018%2F10%2F30%2Ftrimmatic%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[更改了一下之前的trimmatic脚本 12345678910111213#PBS -N trim#PBS -l nodes=2:ppn=10#PBS -q highho=/home02/qizhengyang/rna_work_place/rna_datacd $hodir=$ho/trim_resultfor i in *R1.fq.gzdo java -jar ~/softwares/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 20 -phred33 \ $i $&#123;i/R1/R2&#125; -baseout $&#123;i/_R1/&#125; ILLUMINACLIP:/home02/qizhengyang/softwares/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 \ TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36done [qizhengyang@node1 rna_data]$ i=”abd”[qizhengyang@node1 rna_data]$ echo $icde [qizhengyang@node1 rna_data]$ echo ${i}cdeabdcde[qizhengyang@node1 rna_data]$ echo $i”cde”abdcde]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows blast+]]></title>
    <url>%2F2018%2F09%2F04%2Fwindows-blast%2B%2F</url>
    <content type="text"><![CDATA[下载网址ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/双击exe文件即可，安装程序会自动设置环境变量。blastn -version 查看版本信息]]></content>
  </entry>
  <entry>
    <title><![CDATA[Sep-3]]></title>
    <url>%2F2018%2F09%2F03%2FSep-3%2F</url>
    <content type="text"><![CDATA[“当您使用〖发送至Kindle〗电子邮箱发送个人文件时，您的文件会自动保存到Kindle图书馆中。”如果禁用，则我发送至Kindle电子邮箱的文件不会出现在Kindle图书馆中。 我删除了某条记录之后，发现这条记录里有我想了解的信息。 我打算将smartypants改回true。改之前再试一试，我能在editplus上打出中文破折号（一个全角字符）。 实验结果，会渲染成占两个全角字符的破折号，不能加空格。 我不打算将smartypants改为true了。 我打算删掉Runaway Baby。理由：太长。 紧接着数据，即使在中文输入状态下，点还是半角的点，不是句号。]]></content>
  </entry>
  <entry>
    <title><![CDATA[DNA甲基化与自闭症]]></title>
    <url>%2F2018%2F08%2F26%2FDNA%E7%94%B2%E5%9F%BA%E5%8C%96%E4%B8%8E%E8%87%AA%E9%97%AD%E7%97%87%2F</url>
    <content type="text"><![CDATA[摘要：越来越多的证据表明自闭症谱系障碍（ASD）中基因与环境之间存在复杂的相互作用，包括染色质基因中罕见的从头突变，例如Rett综合征中的methyl-CpG结合蛋白2（MECP2)。基因组序列、基因途径和DNA甲基化的全基因组研究为ASD提供了有价值的机制见解。 词汇： autism spectrum disorder:自闭症谱系障碍 加入spectrum代表自闭症本身的多元性。1 CpG islands: CpG密度极高的区域(&lt; 1 kb)。 Gene body: 基因组上从转录起始位点到终止位点的一段区域。]]></content>
  </entry>
  <entry>
    <title><![CDATA[she - from Ed Sheeran]]></title>
    <url>%2F2018%2F08%2F25%2Fshe-from-Ed-Sheeran%2F</url>
    <content type="text"><![CDATA[一首歌的歌词，来自Ed Sheeran Patience, my enemyAnd loving’s my friendIt’s harder to leaveWith my heart on my sleeveThan to stay and just pretendOh, she knows me so wellOh, she knows me like I know myself]]></content>
  </entry>
  <entry>
    <title><![CDATA[《局外人》的主旨]]></title>
    <url>%2F2018%2F08%2F17%2F%E5%B1%80%E5%A4%96%E4%BA%BA%E4%B8%BB%E6%97%A8%2F</url>
    <content type="text"><![CDATA[1937年8月一个男人，在人们通常视为人生大事的地方（婚姻、社会地位等等）寻找人生，然后某天在翻阅一本时装目录的时候，突然了解到他对自己人生亦即时装目录上鼓吹的那种人生是何其无所谓。]]></content>
  </entry>
  <entry>
    <title><![CDATA[anaconda创建不同的运行环境]]></title>
    <url>%2F2018%2F07%2F21%2Fanaconda%E5%88%9B%E5%BB%BA%E4%B8%8D%E5%90%8C%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[CPC依赖biopython。 服务器上的python版本太低。运行 pip install biopython 报错。这种情况下安装Anaconda，并使用conda创建了一个python=2.7.9的虚拟环境。123conda create -n py2 python=2.7.9 #createsource activate py2 #activatesource deactivate #deactivate 预测lncRNA:1perl LncRNA_Finder_2.0.pl -i test.fa -k housekeeping.fa -s smallRNA.fa -o jc_test -t 1]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《非洲的青山》]]></title>
    <url>%2F2018%2F07%2F14%2F%E9%9D%9E%E6%B4%B2%E7%9A%84%E9%9D%92%E5%B1%B1%2F</url>
    <content type="text"><![CDATA[第四章，“乔伊斯中等身高，他把眼睛用坏了。在那最后一晚，喝醉了，和乔伊斯在一起，他不断引用埃德加·基内的话：‘思维清晰、生命绚丽如战争时一样。’我知道我没有把这句话彻底弄清楚。等你见到他，他会提到三年前被打断的谈话。能见到我们这个时代伟大的作家真让人高兴。”这段话越发让我觉得我看的不是一部小说。 查了维基百科 Green Hills of Africa is a 1935 work of nonfiction by American writer Ernest Hemingway.]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo]]></title>
    <url>%2F2018%2F07%2F13%2Fhexo%2F</url>
    <content type="text"><![CDATA[NexT教程12345hexo clean hexo g hexo s hexo d # 后面两步可合并 hexo d -g 连接Github与本地]]></content>
  </entry>
  <entry>
    <title><![CDATA[《非洲的青山》摘录]]></title>
    <url>%2F2018%2F07%2F13%2F%E3%80%8A%E9%9D%9E%E6%B4%B2%E7%9A%84%E9%9D%92%E5%B1%B1%E3%80%8B%E6%91%98%E5%BD%95%2F</url>
    <content type="text"><![CDATA[《非洲的青山》第三章，关于贝尔蒙特的注释信息中写道，“他是海明威的好朋友，曾出现在海明威的小说《死在午后》、《太阳照常升起》中。晚年受病痛折磨，和海明威一样，饮弹自尽。” 小说的第三章的后半部是我和妻子的对话 “你不觉得老爹很英俊吗？”“不觉得。”我说，“挂眼皮才英俊。”“挂眼皮那是漂亮。但你真的不认为老爹英俊吗？”……“但你真的不认为他英俊吗？”“不认为。” 第四章 P.O.M从一个行囊里拿出几本书，和老爹一起看了起来， 我想起托尔斯泰，想到战争的经历对一个作家而言是多么有益。战争是重大的主题之一，当然也是最难进行真实描写的主题之一。 雨天交通拥堵时在湿滑的沥青路和鹅卵石路上骑车的感觉。]]></content>
  </entry>
</search>
